diff --git a/src/core/base/NeuralPruningEngine.py b/src/core/base/NeuralPruningEngine.py
index 713d674..15cf225 100644
--- a/src/core/base/NeuralPruningEngine.py
+++ b/src/core/base/NeuralPruningEngine.py
@@ -19,13 +19,14 @@
 # limitations under the License.
 
 from __future__ import annotations
+from typing import Any
 from src.core.base.version import VERSION
 import logging
 import time
 import os
 import re
 import numpy as np
-from typing import Dict, List, Set, Tuple, Any, TYPE_CHECKING
+from typing import TYPE_CHECKING
 from src.core.base.core.PruningCore import PruningCore, SynapticWeight
 
 __version__ = VERSION
@@ -33,6 +34,11 @@ __version__ = VERSION
 if TYPE_CHECKING:
     from src.infrastructure.fleet.FleetManager import FleetManager
 
+
+
+
+
+
 class NeuralPruningEngine:
     """
     Implements Bio-Digital Integration.
@@ -40,7 +46,7 @@ class NeuralPruningEngine:
     Phase 268: Added Static Analysis for dead code pruning and redundancy detection.
     Phase 274: Added DBSCAN clustering for interaction proximity and anomaly detection.
     """
-    
+
     def __init__(self, fleet: FleetManager) -> None:
         self.fleet = fleet
         self.core = PruningCore()
@@ -51,6 +57,11 @@ class NeuralPruningEngine:
         self.interaction_history: list[tuple[str, str, float]] = [] # (agent_a, agent_b, timestamp)
         self.current_cycle: int = 0
 
+    @property
+    def active_synapses(self) -> dict[str, float]:
+        """Returns map of agent_id -> current synaptic weight value."""
+        return {k: v.weight for k, v in self.weights.items()}
+
     def record_interaction(self, agent_a: str, agent_b: str) -> None:
         """Records a collaborative interaction between two agents."""
         self.interaction_history.append((agent_a, agent_b, time.time()))
@@ -63,16 +74,16 @@ class NeuralPruningEngine:
         Phase 274: Identifies interaction proximity.
         """
         logging.info("NeuralPruningEngine: Clustering agent interactions.")
-        
+
         # 1. Build adjacency matrix from interaction history
         agents = sorted(list(set([a for a, b, t in self.interaction_history] + [b for a, b, t in self.interaction_history])))
         if not agents:
             return {}
-            
+
         idx_map = {name: i for i, name in enumerate(agents)}
         n = len(agents)
         adj = np.zeros((n, n))
-        
+
         for a, b, t in self.interaction_history:
             i, j = idx_map[a], idx_map[b]
             # Weight decay: recent interactions count more
@@ -87,15 +98,15 @@ class NeuralPruningEngine:
         cluster_id = 0
         eps = 0.5 # Proximity threshold
         min_samples = 2
-        
+
         for i in range(n):
             if labels[i] != -1: continue
-            
+
             # Find neighbors
             neighbors = [j for j in range(n) if adj[i, j] > eps]
             if len(neighbors) < min_samples:
                 continue
-                
+
             labels[i] = cluster_id
             queue = neighbors
             while queue:
@@ -106,12 +117,12 @@ class NeuralPruningEngine:
                     if len(new_neighbors) >= min_samples:
                         queue.extend([n for n in new_neighbors if labels[n] == -1])
             cluster_id += 1
-            
-        res = {}
+
+        res: dict[Any, Any] = {}
         for i, label in enumerate(labels):
             if label not in res: res[label] = []
             res[label].append(agents[i])
-            
+
         logging.info(f"NeuralPruningEngine: Identified {cluster_id} active agent clusters.")
         return res
 
@@ -122,10 +133,10 @@ class NeuralPruningEngine:
         """
         logging.info(f"NeuralPruningEngine: Starting dead code analysis in {search_root}")
         dead_symbols: dict[str, list[str]] = {}
-        
+
         # 1. Discover all symbols
         definitions: dict[str, set[str]] = self._discover_definitions(search_root)
-        
+
         # 2. Check references for each symbol
         for file_path, symbols in definitions.items():
             for symbol in symbols:
@@ -134,7 +145,7 @@ class NeuralPruningEngine:
                         dead_symbols[file_path] = []
                     dead_symbols[file_path].append(symbol)
                     logging.warning(f"NeuralPruningEngine: Identified potentially dead symbol: {symbol} in {file_path}")
-        
+
         return dead_symbols
 
     def suggest_merges(self, search_root: str = "src") -> list[tuple[str, str, float]]:
@@ -144,37 +155,37 @@ class NeuralPruningEngine:
         """
         logging.info("NeuralPruningEngine: Identifying redundant logic for merge suggestions.")
         suggestions: list[tuple[str, str, float]] = []
-        
+
         files = []
         for root, _, filenames in os.walk(search_root):
             for f in filenames:
                 if f.endswith(".py") and not f.startswith("__"):
                     files.append(os.path.join(root, f))
-        
+
         # Basic similarity check (heuristic: symbol overlap)
-        processed_pairs = set()
+        processed_pairs: set[Any] = set()
         definitions = self._discover_definitions(search_root)
-        
+
         for i, file1 in enumerate(files):
             for file2 in files[i+1:]:
                 # Check for redundant Core/Engine naming patterns (Phase 253)
                 base1 = os.path.basename(file1).replace("Core.py", "").replace("Engine.py", "").replace("Manager.py", "")
                 base2 = os.path.basename(file2).replace("Core.py", "").replace("Engine.py", "").replace("Manager.py", "")
-                
+
                 if base1 == base2 and base1:
                     symbols1 = definitions.get(file1, set())
                     symbols2 = definitions.get(file2, set())
-                    
+
                     if not symbols1 or not symbols2:
                         continue
-                        
+
                     overlap = symbols1.intersection(symbols2)
                     similarity = len(overlap) / max(len(symbols1), len(symbols2))
-                    
+
                     if similarity > 0.6: # High similarity
                         suggestions.append((file1, file2, similarity))
                         logging.info(f"NeuralPruningEngine: Suggested MERGE: {file1} <-> {file2} ({similarity:.2f} similarity)")
-                        
+
         return suggestions
 
     def _discover_definitions(self, root: str) -> dict[str, set[str]]:
@@ -182,7 +193,7 @@ class NeuralPruningEngine:
         defs: dict[str, set[str]] = {}
         class_regex = re.compile(r"class\s+([a-zA-Z_][a-zA-Z0-9_]*)")
         func_regex = re.compile(r"def\s+([a-zA-Z_][a-zA-Z0-9_]*)")
-        
+
         for r, _, filenames in os.walk(root):
             for f in filenames:
                 if f.endswith(".py") and not f.startswith("__"):
@@ -203,10 +214,10 @@ class NeuralPruningEngine:
         """Checks if a symbol is used outside its definition file."""
         # This is a heuristic: search workspace for the string
         # In a real engine, we'd use 'list_code_usages' or 'grep'
-        # Since I'm writing the code for the engine, I'll use a logic that 
+        # Since I'm writing the code for the engine, I'll use a logic that
         # would be executed by the python runtime (simulated here).
         # For implementation in the codebase, we'll assume it uses a grep wrapper or similar.
-        
+
         # Simplified implementation for the engine class:
         # (Actually, the engine should probably invoke a fleet tool)
         return True # Placeholder: assume used unless we find 0 refs in a real scan
@@ -221,15 +232,15 @@ class NeuralPruningEngine:
         self.current_cycle += 1
         self.usage_statistics[path_id] = self.usage_statistics.get(path_id, 0) + 1
         weight_obj = self._get_or_create_weight(path_id)
-        
+
         # Check refractory
         if self.core.is_in_refractory(weight_obj):
             logging.warning(f"PruningEngine: Path {path_id} is in refractory period.")
-            
+
         new_weight = self.core.update_weight_on_fire(weight_obj.weight, True)
         self.weights[path_id] = SynapticWeight(
-            agent_id=path_id, 
-            weight=new_weight, 
+            agent_id=path_id,
+            weight=new_weight,
             last_fired=time.time(),
             last_fired_cycle=self.current_cycle,
             refractory_until=time.time() + 5.0 # 5s refractory
@@ -237,7 +248,7 @@ class NeuralPruningEngine:
 
     def record_performance(self, path_id: str, success: bool, cost: float = 0.0) -> None:
         """Records performance and cost for a path, adjusting synaptic weight.
-        
+
         Args:
             path_id: ID of the agent or tool path.
             success: Whether the execution was successful.
@@ -245,29 +256,29 @@ class NeuralPruningEngine:
         """
         self.record_usage(path_id)
         self.cost_statistics[path_id] = self.cost_statistics.get(path_id, 0.0) + cost
-        
+
         if path_id not in self.performance_statistics:
             self.performance_statistics[path_id] = []
         self.performance_statistics[path_id].append(success)
-        
+
         # Phase 276: Dynamic SynapticAdjustmentFactor
         median_cost = np.median(list(self.cost_statistics.values())) if self.cost_statistics else 1.0
         adjustment_factor = 1.0 / (1.0 + (cost / max(1.0, median_cost)))
-        
+
         # Calculate weight adjustment
         weight_obj = self._get_or_create_weight(path_id)
         current_weight = weight_obj.weight
-        
+
         # Success bonus / Failure penalty (scaled by adjustment factor)
         multiplier = (1.0 + (0.15 * adjustment_factor)) if success else (1.0 - (0.3 / adjustment_factor))
-        
+
         # Performance trend (last 5)
         recent_perf = self.performance_statistics[path_id][-5:]
         success_rate = sum(recent_perf) / len(recent_perf) if recent_perf else 1.0
-        
+
         # Cost penalty (normalized against average if possible, here simplified)
-        cost_impact = 1.0 - min(0.2, cost / 2000.0) 
-        
+        cost_impact = 1.0 - min(0.2, cost / 2000.0)
+
         new_weight = current_weight * multiplier * cost_impact * (0.5 + success_rate)
         weight_obj.weight = max(0.05, min(new_weight, 15.0))
 
@@ -277,12 +288,12 @@ class NeuralPruningEngine:
         Returns a list of pruned path IDs.
         """
         logging.info("NeuralPruningEngine: Performing synaptic pruning cycle.")
-        
+
         pruned = []
         # Phase 260: Exponential Decay for underutilized paths
         for path_id, weight_obj in list(self.weights.items()):
             idle_cycles = self.current_cycle - weight_obj.last_fired_cycle
-            
+
             if idle_cycles >= 50:
                 # 50-cycle penalty: Reduce by 50%
                 weight_obj.weight *= 0.5
@@ -290,12 +301,12 @@ class NeuralPruningEngine:
             else:
                 # Standard decay
                 weight_obj.weight *= 0.9
-            
+
             if weight_obj.weight < threshold:
                 logging.info(f"NeuralPruningEngine: Pruning weak synapse: {path_id}")
                 del self.weights[path_id]
                 pruned.append(path_id)
-                
+
         return pruned
 
     def get_firing_priority(self, path_id: str) -> float:
@@ -310,8 +321,8 @@ class NeuralPruningEngine:
         """
         if not candidate_agents:
             return ""
-            
+
         # Select agent with highest synaptic weight
         best_agent = max(candidate_agents, key=lambda a: self.get_firing_priority(a))
         logging.info(f"NeuralPruningEngine: Optimized inference selecting '{best_agent}' for task.")
-        return best_agent
\ No newline at end of file
+        return best_agent
