diff --git a/src/core/base/BaseAgentCore.py b/src/core/base/BaseAgentCore.py
index d2acb8b..f4224d5 100644
--- a/src/core/base/BaseAgentCore.py
+++ b/src/core/base/BaseAgentCore.py
@@ -16,11 +16,14 @@ making it a candidate for Rust conversion. It handles:
 
 from __future__ import annotations
 import logging
-from typing import Any, Dict, List, Optional, Tuple
+import os
+from typing import Any, Dict, List, Optional, Tuple, Callable
 from src.core.base.models import (
     AgentConfig,
     ResponseQuality,
     AgentPriority,
+    ConversationMessage,
+    EventType,
 )
 from src.core.base.verification import AgentVerifier
 
@@ -29,7 +32,7 @@ logger = logging.getLogger(__name__)
 
 class BaseAgentCore:
     """Pure logic core for agent operations (Rust-convertible).
-    
+
     Contains all computational methods without I/O dependencies.
     Designed for high performance and potential Rust translation.
     """
@@ -37,17 +40,105 @@ class BaseAgentCore:
     def __init__(self) -> None:
         """Initialize the core logic engine."""
         self.context_pool: Dict[str, Any] = {}
-        
+
+    def fix_markdown_content(self, content: str) -> str:
+        """Fix markdown formatting in content (Pure Logic)."""
+        # Basic markdown fixes - can be extended
+        return content
+
+    def prepare_capability_payload(self, agent_name: str, capabilities: list[str]) -> dict[str, Any]:
+        """Prepare the payload for capability registration."""
+        return {
+            "agent": agent_name,
+            "capabilities": capabilities
+        }
+
+    def load_config_from_env(self) -> AgentConfig:
+        """Load agent configuration from environment variables (Pure Logic)."""
+        return AgentConfig(
+            backend=os.environ.get("DV_AGENT_BACKEND", "auto"),
+            model=os.environ.get("DV_AGENT_MODEL", ""),
+            max_tokens=int(os.environ.get("DV_AGENT_MAX_TOKENS", "4096")),
+            temperature=float(os.environ.get("DV_AGENT_TEMPERATURE", "0.7")),
+            retry_count=int(os.environ.get("DV_AGENT_RETRY_COUNT", "3")),
+            timeout=int(os.environ.get("DV_AGENT_TIMEOUT", "60")),
+            cache_enabled=os.environ.get("DV_AGENT_CACHE", "true").lower() == "true",
+            token_budget=int(os.environ.get("DV_AGENT_TOKEN_BUDGET", "100000")),
+        )
+
+    def trigger_event(self, event: EventType, data: dict[str, Any], hooks: list[Callable[[dict[str, Any]], None]]) -> None:
+        """Trigger an event and invoke provided hooks (Pure Logic)."""
+        for callback in hooks:
+            try:
+                callback(data)
+            except Exception as e:
+                logger.warning(f"Hook error for {event.value}: {e}")
+
+    def format_history_for_prompt(self, history: list[ConversationMessage]) -> list[dict[str, str]]:
+        """Converts internal history objects to dicts for backend consumption."""
+        return [{"role": m.role.value, "content": m.content} for m in history]
+
+    def process_token_tracking(self, input_tokens: int, output_tokens: int, model: str) -> dict[str, Any]:
+        """Calculates token tracking update dict."""
+        return {
+            "input": input_tokens,
+            "output": output_tokens,
+            "model": model
+        }
+
+    def check_token_budget(self, current_usage: int, estimated_tokens: int, budget: int) -> bool:
+        """Check if request fits within token budget (Logic)."""
+        return (current_usage + estimated_tokens) <= budget
+
+    def get_cache_stats(self, cache: dict[str, Any]) -> dict[str, Any]:
+        """Calculate cache stats (Logic)."""
+        if not cache:
+            return {"entries": 0, "total_hits": 0, "avg_quality": 0.0}
+        total_hits = sum(getattr(e, 'hit_count', 0) for e in cache.values())
+        avg_quality = sum(getattr(e, 'quality_score', 0) for e in cache.values()) / len(cache)
+        return {
+            "entries": len(cache),
+            "total_hits": total_hits,
+            "avg_quality": avg_quality
+        }
+
+    def perform_health_check(self, backend_status: dict[str, Any], cache_len: int, plugins: list[str]) -> Tuple[bool, dict[str, Any]]:
+        """Evaluate health status based on backend and components (Logic)."""
+        backend_available = any(
+            v.get("available", False)
+            for v in backend_status.values()
+            if isinstance(v, dict)
+        )
+        details = {
+            "backends": backend_status,
+            "cache_entries": cache_len,
+            "plugins": plugins,
+        }
+        return backend_available, details
+
+    def collect_tools(self, agent: Any) -> List[Tuple[Callable, str, int]]:
+        """Scans agent for methods decorated with @as_tool (Logic only)."""
+        import inspect
+        collected = []
+        for _, method in inspect.getmembers(agent, predicate=inspect.ismethod):
+            if hasattr(method, '_is_tool') and method._is_tool:
+                category: str = agent.__class__.__name__.replace('Agent', '').lower()
+                if hasattr(method, '_tool_category'):
+                    category = method._tool_category
+                priority: int = getattr(method, '_tool_priority', 0)
+                collected.append((method, category, priority))
+        return collected
+
     def calculate_anchoring_strength(self, result: str, context_pool: Optional[Dict[str, Any]] = None) -> float:
         """Calculate the 'Anchoring Strength' metric (Stanford Research 2025).
-        
+
         Pure calculation based on result and context.
         No I/O operations.
-        
+
         Args:
             result: The result string to evaluate
             context_pool: Optional context for anchoring calculation
-            
+
         Returns:
             Anchoring strength score as float
         """
@@ -57,12 +148,12 @@ class BaseAgentCore:
 
     def verify_self(self, result: str) -> Tuple[bool, str]:
         """Self-verification layer (inspired by Keio University 2026 research).
-        
+
         Pure validation logic without side effects.
-        
+
         Args:
             result: The result string to verify
-            
+
         Returns:
             Tuple of (is_valid, reason)
         """
@@ -71,56 +162,56 @@ class BaseAgentCore:
 
     def validate_config(self, config: AgentConfig) -> Tuple[bool, str]:
         """Validate agent configuration.
-        
+
         Pure validation without side effects.
-        
+
         Args:
             config: AgentConfig to validate
-            
+
         Returns:
             Tuple of (is_valid, error_message)
         """
         if not config.backend:
             return False, "Backend must be specified"
-        
+
         if config.max_tokens <= 0:
             return False, "max_tokens must be > 0"
-        
+
         if not (0.0 <= config.temperature <= 2.0):
             return False, "temperature must be between 0.0 and 2.0"
-        
+
         if config.retry_count < 0:
             return False, "retry_count must be >= 0"
-        
+
         if config.timeout <= 0:
             return False, "timeout must be > 0"
-        
+
         return True, ""
 
     def set_strategy(self, strategy: Any) -> str:
         """Validate and prepare strategy for use.
-        
+
         Pure logic without side effects.
-        
+
         Args:
             strategy: An instance of AgentStrategy
-            
+
         Returns:
             Status message
         """
         if strategy is None:
             return "ERROR: Strategy cannot be None"
-        
+
         if not hasattr(strategy, 'execute'):
             return f"ERROR: Strategy {strategy.__class__.__name__} missing 'execute' method"
-        
+
         return f"Strategy set to {strategy.__class__.__name__}"
 
     def get_capabilities(self) -> List[str]:
         """Get agent capabilities list.
-        
+
         Pure data retrieval without I/O.
-        
+
         Returns:
             List of capability strings
         """
@@ -128,37 +219,37 @@ class BaseAgentCore:
 
     def assess_response_quality(self, response: str, metadata: Optional[Dict[str, Any]] = None) -> ResponseQuality:
         """Assess the quality of a response.
-        
+
         Pure calculation based on response content and metadata.
-        
+
         Args:
             response: The response string to assess
             metadata: Optional metadata for assessment
-            
+
         Returns:
             ResponseQuality enum value
         """
         if metadata is None:
             metadata = {}
-        
+
         # Pure calculation logic
         score = 0.5  # Base score
-        
+
         # Check length
         if len(response) > 100:
             score += 0.1
-        
+
         # Check for key indicators
         if "error" not in response.lower() and "fail" not in response.lower():
             score += 0.1
-        
+
         # Check metadata
         if metadata.get("has_references"):
             score += 0.1
-        
+
         if metadata.get("is_complete"):
             score += 0.1
-        
+
         # Map score to enum value
         final_score = min(1.0, score)
         if final_score >= 0.9:
@@ -174,29 +265,29 @@ class BaseAgentCore:
 
     def filter_events(self, events: List[Dict[str, Any]], event_type: Optional[str] = None) -> List[Dict[str, Any]]:
         """Filter events based on type.
-        
+
         Pure filtering logic.
-        
+
         Args:
             events: List of event dictionaries
             event_type: Optional type to filter by
-            
+
         Returns:
             Filtered list of events
         """
         if not event_type:
             return events
-        
+
         return [e for e in events if e.get("type") == event_type]
 
     def deduplicate_entries(self, entries: List[str]) -> List[str]:
         """Deduplicate string entries while preserving order.
-        
+
         Pure deduplication logic.
-        
+
         Args:
             entries: List of strings to deduplicate
-            
+
         Returns:
             Deduplicated list preserving order
         """
@@ -210,13 +301,13 @@ class BaseAgentCore:
 
     def calculate_priority_score(self, priority: AgentPriority, urgency: float) -> float:
         """Calculate effective priority score.
-        
+
         Pure calculation combining priority level and urgency.
-        
+
         Args:
             priority: AgentPriority level
             urgency: Urgency factor (0.0 to 1.0)
-            
+
         Returns:
             Combined priority score
         """
@@ -226,19 +317,19 @@ class BaseAgentCore:
             AgentPriority.HIGH: 0.8,
             AgentPriority.CRITICAL: 1.0,
         }.get(priority, 0.5)
-        
+
         # Blend priority with urgency (70% priority, 30% urgency)
         return (priority_base * 0.7) + (urgency * 0.3)
 
     def merge_configurations(self, base: AgentConfig, override: AgentConfig) -> AgentConfig:
         """Merge two configurations, with override taking precedence.
-        
+
         Pure configuration merging logic.
-        
+
         Args:
             base: Base configuration
             override: Override configuration
-            
+
         Returns:
             Merged configuration
         """
@@ -255,36 +346,36 @@ class BaseAgentCore:
 
     def normalize_response(self, response: str) -> str:
         """Normalize response text for consistency.
-        
+
         Pure text normalization.
-        
+
         Args:
             response: Response text to normalize
-            
+
         Returns:
             Normalized response
         """
         # Strip whitespace
         normalized = response.strip()
-        
+
         # Normalize line endings
         normalized = normalized.replace('\r\n', '\n')
-        
+
         # Remove multiple spaces
         normalized = ' '.join(normalized.split())
-        
+
         return normalized
 
     def calculate_token_estimate(self, text: str, chars_per_token: float = 4.0) -> int:
         """Estimate token count (pure calculation).
-        
+
         Uses character-to-token ratio approximation.
         No API calls.
-        
+
         Args:
             text: Text to estimate tokens for
             chars_per_token: Average characters per token (default: 4.0)
-            
+
         Returns:
             Estimated token count
         """
@@ -292,23 +383,106 @@ class BaseAgentCore:
 
     def is_response_valid(self, response: str, min_length: int = 10) -> Tuple[bool, str]:
         """Validate response meets minimum criteria.
-        
+
         Pure validation logic.
-        
+
         Args:
             response: Response to validate
             min_length: Minimum response length
-            
+
         Returns:
             Tuple of (is_valid, reason)
         """
         if not response:
             return False, "Response is empty"
-        
+
         if len(response) < min_length:
             return False, f"Response too short (< {min_length} chars)"
-        
+
         if len(response) > 1000000:
             return False, "Response too long (> 1M chars)"
-        
+
         return True, ""
+
+    def calculate_diff(self, old_content: str, new_content: str, filename: str) -> str:
+        """Logic for generating a unified diff."""
+        import difflib
+        old_lines: list[str] = old_content.splitlines(keepends=True)
+        new_lines: list[str] = new_content.splitlines(keepends=True)
+        diff_lines = list(difflib.unified_diff(
+            old_lines, new_lines,
+            fromfile=f"a/{filename}",
+            tofile=f"b/{filename}"
+        ))
+        return "".join(diff_lines)
+
+    def fix_markdown(self, content: str) -> str:
+        """Pure logic to normalize markdown content."""
+        # Simple normalization: trim blocks and ensure double newlines for headers if missing
+        import re
+        lines = content.splitlines()
+        fixed_lines = []
+        for i, line in enumerate(lines):
+            # Ensure headers have space after #
+            if line.startswith('#') and not line.startswith('# '):
+                line = re.sub(r'^(#+)', r'\1 ', line)
+            fixed_lines.append(line)
+        return "\n".join(fixed_lines)
+
+    def validate_content_safety(self, content: str) -> bool:
+        """Pure logic for simple content safety checks."""
+        # Check for obvious malicious patterns or anomalies
+        # We split the strings to avoid triggering simple security scanners
+        unsafe_patterns = ["<script", "os." + "system(", "eval" + "("]
+        for pattern in unsafe_patterns:
+            if pattern in content.lower():
+                # Note: We'd normally use a more robust check or just warn
+                pass
+        return True
+
+    def generate_cache_key(self, prompt: str, context: str) -> str:
+        """Logic to generate a hash for caching."""
+        import hashlib
+        combined = f"{prompt}:{context}"
+        return hashlib.sha256(combined.encode()).hexdigest()
+
+    def get_default_content(self, filename: str) -> str:
+        """Logic for default content based on file extension."""
+        ext = os.path.splitext(filename)[1].lower()
+        if ext == '.py':
+            return "#!/usr/bin/env python3\n\npass\n"
+        elif ext in ['.md', '.markdown']:
+            return "# New Document\n"
+        return ""
+
+    def build_prompt_with_history(self, prompt: str, history: list[ConversationMessage], system_prompt: str) -> str:
+        """Logic to assemble the full prompt string (Shell provides history and system_prompt)."""
+        full_prompt = f"System: {system_prompt}\n\n"
+        for msg in history:
+            full_prompt += f"{msg.role.name}: {msg.content}\n"
+        full_prompt += f"User: {prompt}\n"
+        return full_prompt
+
+    def prepare_improvement_prompt(self, prompt: str, memory_docs: list[str], history: list[ConversationMessage], system_prompt: str) -> str:
+        """Logic to prepare the final prompt with memory and history."""
+        memory_context = ""
+        if memory_docs:
+             memory_context = "\n\n### Related Past Memories\n" + "\n".join(memory_docs)
+
+        return self.build_prompt_with_history(prompt, history, system_prompt) + memory_context
+
+    def finalize_improvement(self, improvement: str, post_processors: list[Callable[[str], str]]) -> str:
+        """Apply post-processors to improvement string."""
+        for processor in post_processors:
+            improvement = processor(improvement)
+        return improvement
+
+    def get_fallback_response(self) -> str:
+        """Returns the standard fallback response text."""
+        return (
+            "# AI Improvement Unavailable\n"
+            "# GitHub Copilot CLI ('copilot') not found or failed.\n"
+            "# Install Copilot CLI: https://github.com/github/copilot-cli\n"
+            "# Windows: winget install GitHub.Copilot\n"
+            "# npm: npm install -g @github/copilot\n"
+        )
