diff --git a/src/infrastructure/backend/SubagentCore.py b/src/infrastructure/backend/SubagentCore.py
index 53f1c9d..1ea6b96 100644
--- a/src/infrastructure/backend/SubagentCore.py
+++ b/src/infrastructure/backend/SubagentCore.py
@@ -25,7 +25,7 @@ from src.core.base.version import VERSION
 import logging
 import os
 import time
-from typing import Optional, TYPE_CHECKING
+from typing import TYPE_CHECKING
 from .RunnerBackends import BackendHandlers
 
 __version__ = VERSION
@@ -33,9 +33,14 @@ __version__ = VERSION
 if TYPE_CHECKING:
     from .SubagentRunner import SubagentRunner
 
+
+
+
+
+
 class SubagentCore:
     """Delegated execution core for SubagentRunner."""
-    
+
     def __init__(self, runner: SubagentRunner) -> None:
         self.runner = runner
 
@@ -43,7 +48,7 @@ class SubagentCore:
         """Run a subagent using available backends."""
         backend_env = os.environ.get("DV_AGENT_BACKEND", "auto").strip().lower()
         use_cache = os.environ.get("DV_AGENT_CACHE", "true").lower() == "true"
-        
+
         cache_model = backend_env if backend_env != "auto" else "subagent_auto"
         cache_key = self.runner._get_cache_key(f"{description}:{prompt}:{original_content}", cache_model)
 
@@ -106,14 +111,14 @@ class SubagentCore:
             res = _try_openai_api()
         else:
             # auto (default) logic
-            res = (_try_vllm() or _try_ollama() or _try_codex_cli() or 
-                   _try_copilot_cli() or _try_github_models() or 
+            res = (_try_vllm() or _try_ollama() or _try_codex_cli() or
+                   _try_copilot_cli() or _try_github_models() or
                    _try_openai_api() or _try_gh_copilot(allow_non_command=False))
 
         if res and use_cache:
             self.runner._response_cache[cache_key] = res
             self.runner.disk_cache.set(cache_key, res)
-        
+
         if self.runner.recorder:
             self.runner.recorder.record_interaction(
                 provider="SubagentRunner",
@@ -121,7 +126,7 @@ class SubagentCore:
                 prompt=prompt,
                 result=res or "FAILED"
             )
-            
+
         return res
 
     def llm_chat_via_github_models(
@@ -162,7 +167,7 @@ class SubagentCore:
                 max_retries=max_retries,
                 stream=stream
             )
-            
+
             if result:
                 if validate_content and not self.runner.validate_response_content(result):
                     logging.warning("Response validation failed")
@@ -176,4 +181,4 @@ class SubagentCore:
         except Exception as e:
             self.runner._metrics["errors"] += 1
             logging.error(f"GitHub Models call failed: {e}")
-            raise
\ No newline at end of file
+            raise
