diff --git a/src/infrastructure/backend/llm_backends/GitHubModelsBackend.py b/src/infrastructure/backend/llm_backends/GitHubModelsBackend.py
index ada2b9e..fadaf5b 100644
--- a/src/infrastructure/backend/llm_backends/GitHubModelsBackend.py
+++ b/src/infrastructure/backend/llm_backends/GitHubModelsBackend.py
@@ -27,6 +27,11 @@ from .LLMBackend import LLMBackend
 
 __version__ = VERSION
 
+
+
+
+
+
 class GitHubModelsBackend(LLMBackend):
     """GitHub Models LLM Backend."""
 
@@ -49,7 +54,7 @@ class GitHubModelsBackend(LLMBackend):
                 if path.exists():
                     try:
                         token = path.read_text(encoding="utf-8").strip()
-                        if token: 
+                        if token:
                             logging.debug(f"DEBUG: token found in file: {path}")
                             break
                     except Exception:
@@ -70,12 +75,12 @@ class GitHubModelsBackend(LLMBackend):
         if not token:
             logging.warning("GitHub Models: Missing token. Skipping.")
             return "" # Return empty instead of raising to allow fallback logic to proceed
-        
+
         logging.debug(f"DEBUG: using token: {token[:3]}...")
 
         base_url = (kwargs.get("base_url") or os.environ.get("GITHUB_MODELS_BASE_URL") or "https://models.inference.ai.azure.com").strip()
         url = base_url.rstrip("/") + "/v1/chat/completions"
-        
+
         # Multi-modal support logic
         import re
         image_match = re.search(r"\[IMAGE_DATA:(.*?)\]", prompt, re.DOTALL)
@@ -97,12 +102,12 @@ class GitHubModelsBackend(LLMBackend):
             ],
             "stream": kwargs.get("stream", False)
         }
-        
+
         headers = {
             "Authorization": f"Bearer {token}",
             "Content-Type": "application/json",
         }
-        
+
         max_retries = kwargs.get("max_retries", 2)
         timeout_s = kwargs.get("timeout_s", 60)
         import json
@@ -111,9 +116,9 @@ class GitHubModelsBackend(LLMBackend):
             try:
                 # Use current token (might have been updated in previous attempt)
                 headers["Authorization"] = f"Bearer {token}"
-                
+
                 response = self.session.post(url, headers=headers, data=json.dumps(payload), timeout=timeout_s)
-                
+
                 if response.status_code == 401:
                     logging.warning(f"GitHub Models: Unauthorized (401) on attempt {attempt+1}. Refreshing token...")
                     try:
@@ -121,12 +126,12 @@ class GitHubModelsBackend(LLMBackend):
                         # Phase 149: Hardened self-healing. Try to refresh via GH CLI.
                         res = subprocess.run(["gh", "auth", "token"], capture_output=True, text=True, check=False)
                         new_token = res.stdout.strip() if res.returncode == 0 else ""
-                        
+
                         if new_token and new_token != token:
                             logging.info("GitHub Models: New token obtained via GitHub CLI. Retrying...")
                             token = new_token
                             # Sticky token for session (Phase 141)
-                            os.environ["GITHUB_TOKEN"] = token 
+                            os.environ["GITHUB_TOKEN"] = token
                             headers["Authorization"] = f"Bearer {token}"
                             # Retry immediately with new token
                             response = self.session.post(url, headers=headers, data=json.dumps(payload), timeout=timeout_s)
@@ -136,12 +141,12 @@ class GitHubModelsBackend(LLMBackend):
                             logging.warning("GitHub Models: Token refresh returned identical token. Authorization likely revoked.")
                     except Exception as e:
                         logging.debug(f"GitHub Models token refresh error: {e}")
-                
+
                 if response.status_code == 401:
                     logging.warning("GitHub Models: Unauthorized even after token refresh.")
                     self._update_status("github_models", False)
                     return ""
-                
+
                 response.raise_for_status()
                 data = response.json()
                 content = data["choices"][0]["message"]["content"].strip()
@@ -157,4 +162,4 @@ class GitHubModelsBackend(LLMBackend):
                     logging.debug(f"GitHub Models call failed after {max_retries} retries: {e}")
                     self._update_status("github_models", False)
                     self._record("github_models", model, prompt, f"ERROR: {str(e)}", system_prompt=system_prompt)
-        return ""
\ No newline at end of file
+        return ""
