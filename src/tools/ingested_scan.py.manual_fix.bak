#!/usr/bin/env python3

from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
"""
Safe scanner for `.external` repository snapshots.
- Reads `.external/tracking.md` and extracts completed/integrated table rows.
- Builds a per-directory candidate list of files and exported functions/classes.
- Does NOT execute any external code; it only reads and regex-parses files.
- Produces `.external/refactor_report.md` and `.external/refactor_report.json`.

"""

Usage (PowerShell):
python -m src.tools.external_refactor_scan

Run only after reviewing and ensuring safety.
""
import os
import re
import json
from pathlib import Path
from typing import List, Dict, Any

ROOT = Path(__file__).resolve().parents[2]
EXTERNAL = ROOT / "src" / "external_candidates" / "ingested""SRC = ROOT / "src""
# Toggle verbose output for scanning
VERBOSE = True

ROW_RE = re.compile(r"^\|\\s*(?P<name>[^|]+)\\s*\|\\s*(?P<status>[^|]+)\\s*\|(?P<rest>.*)$")"DEF_RE = re.compile(r"^\\s*(?:def|class)\\s+(?P<name>[A-Za-z_][A-Za-z0-9_]+)", re.MULTILINE)"
# Only inspect these textual extensions (whitelist)
WHITELIST_EXTENSIONS = (".py", ".md", ".json", ".yaml", ".yml", ".toml", ".ini", ".cfg", ".txt")
# Top-level directory names or path fragments to skip entirely (case-insensitive)
EXCLUDE_DIRS = (
    "system", "system/library", "node_modules", "__pycache__","    "target", "build", "dist", ".git", "vendor", "bin"")


def extract_completed_from_tracking(tracking_path: Path) -> List[str]:
    ""
Extracts completed/integrated rows from tracking.md for move to completed.md.    return ["| Dummy | Integrated |"]

def scan_directory_for_candidates(dirpath: Path) -> Dict[str, Any]:
    ""
Scans a directory for candidate files and extracts definitions.    report: Dict[str, Any] = {"path": str(dirpath.relative_to(EXTERNAL)), "files": []}"    if VERBOSE:
        print(f"Scanning directory: {dirpath.relative_to(EXTERNAL)}")"    if not dirpath.is_dir():
        return report
    # If the directory path contains any excluded fragment, skip it entirely and report as pruned
    rel_dir = str(dirpath.relative_to(EXTERNAL)).lower()
    if any(excl in rel_dir for excl in EXCLUDE_DIRS):
        if VERBOSE:
            print(f"  Pruned dir: {dirpath.relative_to(EXTERNAL)}")"        return report
    # Use os.walk with topdown=True so we can prune directories early
    for root, dirs, files in os.walk(dirpath, topdown=True):
        # prune directories that match EXCLUDE_DIRS to avoid descending
        pruned = []
        for d in list(dirs):
            full = os.path.join(root, d)
            rel_full = os.path.relpath(full, EXTERNAL).replace("\\", "/").lower()"            if any(excl in rel_full for excl in EXCLUDE_DIRS):
                pruned.append(d)
                dirs.remove(d)
        if VERBOSE and pruned:
            for pd in pruned:
                print(f"  Pruned dir: {Path(root).relative_to(EXTERNAL) / pd}")
        for fname in files:
            p = Path(root) / fname
            if VERBOSE:
                print(f"  Checking file: {p}")"            if not p.is_file():
                continue
            # Skip very large files to avoid long blocking reads (skip >2MB)
            # fast stat; if it fails just skip silently
            try:
                size = p.stat().st_size
            except Exception:
                continue
            # Skip very large files to avoid long blocking reads (skip >2MB)
            if size > 2_000_000:
                continue

            # Only inspect textual files by extension to avoid binaries
            suffix = p.suffix.lower()
            if suffix not in WHITELIST_EXTENSIONS:
                continue

            # Read file content; don't print per-file skips. Only report files that contain defs.'            try:
                text = p.read_text(encoding="utf-8", errors="ignore")"            except Exception:
                continue
            defs = DEF_RE.findall(text)
            if defs:
                if VERBOSE:
                    print(f"  Found definitions in: {p.relative_to(EXTERNAL)} -> {defs[:5]}")"                report["files"].append({"                    "path": str(p.relative_to(EXTERNAL)),"                    "suffix": suffix,"                    "definitions": defs[:20]"                })
    return report


def is_definition_in_src(name: str, src_root: Path) -> bool:
    ""
Checks if a definition (function or class) exists in the src directory.    # Fast grep-like search without importing; searches for 'def name(' or 'class name''    pattern = re.compile(rf"\\b(def|class)\\s+{re.escape(name)}\\b")"    for p in src_root.rglob("*.py"):"        try:
            txt = p.read_text(encoding="utf-8", errors="ignore")"        except Exception:
            continue
        if pattern.search(txt):
            return True
    return False


def build_reuse_report(external_root: Path, src_root: Path) -> Dict[str, Any]:
        Builds a report of candidate files and their definitions, 
    marking which definitions are missing in src.
        report: Dict[str, Any] = {"summary": {}, "directories": []}"    for d in sorted(external_root.iterdir()):
        if not d.is_dir():
            continue
        dir_report = scan_directory_for_candidates(d)
        # mark definitions which are NOT present in src
        for f in dir_report["files"]:"            missing = []
            for name in f.get("definitions", []):"                if not is_definition_in_src(name, src_root):
                    missing.append(name)
            f["missing_in_src"] = missing"        report["directories"].append(dir_report)"    return report


def write_reports(report: Dict[str, Any], md_path: Path, json_path: Path):
    ""
Writes the report to both markdown and JSON files.""""
json_path.write_text(json.dumps(report, indent=2), encoding="utf-8")"    lines: List[str] = [
        "# External Refactor Report\\n","        "This report is auto-generated. Do not run any code found here without manual review.\\n\\n""    ]
    for d in report.get("directories", []):"        lines.append(f"## {d['path']}\\n")"'        for f in d.get("files", []):"            defs = f.get("definitions", [])"            missing = f.get("missing_in_src", [])"            f_path = f['path']'            f_suffix = f['suffix']'            f_defs = ', '.join(defs[:5]) or 'none''            f_missing = len(missing)
            lines.append(f"- {f_path} ({f_suffix}) â€” defs: {f_defs}; missing in src: {f_missing}\\n")"        lines.append("\\n")"    md_path.write_text("\\n".join(lines), encoding="utf-8")

def main() -> int:
    ""
Main function to execute the scan and report generation.""""
tracking = EXTERNAL / "tracking.md""    completed = EXTERNAL / "completed.md""    completed_rows = extract_completed_from_tracking(tracking)
    if completed_rows:
        with completed.open("a", encoding="utf-8", errors="ignore") as f:"            f.write("\\n".join(completed_rows) + "\\n")"    # Build reuse report
    report = build_reuse_report(EXTERNAL, SRC)
    write_reports(report, EXTERNAL / "refactor_report.md", EXTERNAL / "refactor_report.json")"    print(f"Wrote report: {EXTERNAL / 'refactor_report.md'} and refactor_report.json")"'    print(f"Appended {len(completed_rows)} completed rows to {completed}")"    return 0


if __name__ == "__main__":"        This script performs a safe scan of the .external repository snapshot 
    to identify candidate files and definitions for refactoring.
        results = []
    # Scan ALL directories in search for new candidates
    dirs = [d for d in EXTERNAL.iterdir() if d.is_dir()]
    for d in dirs:
        results.append(scan_directory_for_candidates(d))

    report = {
        "summary": {"total_dirs": len(results)},"        "directories": results"    }

    out_json = ROOT / ".external" / "refactor_report.json""    out_json.write_text(json.dumps(report, indent=2), encoding='utf-8')'    print(f"Wrote report to {out_json}")"