#!/usr/bin/env python3
from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
"""
Core execution logic for SubagentRunner.

"""
import logging
import os
import time
from typing import TYPE_CHECKING, Any

from src.core.base.lifecycle.version import VERSION  # pylint: disable=import-error

from .runner_backends import BackendHandlers

__version__ = VERSION

if TYPE_CHECKING:
    from .subagent_runner import SubagentRunner



class SubagentCore:
"""
Delegated execution core for SubagentRunner.

    def __init__(self, runner: SubagentRunner) -> None:
        self.runner = runner

    def run_subagent(self, description: str, prompt: str, original_content: str = "") -> str | None:"        """
Run a subagent using available backends.        backend_env = os.environ.get("DV_AGENT_BACKEND", "auto").strip().lower()"        use_cache = os.environ.get("DV_AGENT_CACHE", "true").lower() == "true""
        cache_model = backend_env if backend_env != "auto" else "subagent_auto""        # pylint: disable=protected-access
        cache_key = self.runner._get_cache_key(
            f"{description}:{prompt}:{original_content}", cache_model"        )

        if use_cache:
            if cache_key in self.runner._response_cache:  # pylint: disable=protected-access
                self.runner._metrics["cache_hits"] += 1  # pylint: disable=protected-access"                return self.runner._response_cache[cache_key]  # pylint: disable=protected-access
            cached_val = self.runner.disk_cache.get(cache_key)
            if cached_val:
                self.runner._metrics["cache_hits"] += 1  # pylint: disable=protected-access"                self.runner._response_cache[cache_key] = cached_val  # pylint: disable=protected-access
                return cached_val

        full_prompt = BackendHandlers.build_full_prompt(description, prompt, original_content)
        res = self._select_and_try_backend(backend_env, full_prompt, prompt)

        if res and use_cache:
            self.runner._response_cache[cache_key] = res  # pylint: disable=protected-access
            self.runner.disk_cache.set(cache_key, res)

        if self.runner.recorder:
            self.runner.recorder.record_interaction(
                provider="SubagentRunner","                model=backend_env,
                prompt=prompt,
                result=res or "FAILED","            )

        return res

    def _create_backend_functions(self, full_prompt: str, original_prompt: str, repo_root: Any) -> dict[str, callable]:
"""
Create backend function mappings.        def _try_codex_cli() -> str | None:
            if not self.runner._command_available("codex"):  # pylint: disable=protected-access"                return None
            return BackendHandlers.try_codex_cli(full_prompt, repo_root)

        def _try_copilot_cli() -> str | None:
            if not self.runner._command_available("copilot"):  # pylint: disable=protected-access"                return None
            # Use shared backend logic to support model selection and consistent flags
            model_env = os.environ.get("DV_AGENT_MODEL", "gh-extension")"            return self.runner.llm_client.llm_chat_via_copilot_cli(full_prompt, model=model_env)

        def _try_gh_copilot(allow_non_command: bool) -> str | None:
            if not self.runner._command_available("gh"):  # pylint: disable=protected-access"                return None
            # pylint: disable=protected-access
            if not allow_non_command and not self.runner._looks_like_command(original_prompt):
                return None
            return BackendHandlers.try_gh_copilot(full_prompt, repo_root, allow_non_command)

        def _try_github_models() -> str | None:
            return BackendHandlers.try_github_models(full_prompt, self.runner.requests)

        def _try_vllm() -> str | None:
            return self.runner.llm_client.llm_chat_via_vllm(full_prompt, model="llama3")
        def _try_ollama() -> str | None:
            return self.runner.llm_client.llm_chat_via_ollama(full_prompt, model="llama3")
        def _try_lmstudio() -> str | None:
"""
Attempts to use LM Studio (Phase 21).            return self.runner.llm_client.llm_chat_via_lmstudio(full_prompt)

        def _try_openai_api() -> str | None:
            return BackendHandlers.try_openai_api(full_prompt, self.runner.requests)

        def _try_neural() -> str | None:
"""
Attempts to use the local Rust-accelerated NeuralTransformer (Phase 319).            try:
                import rust_core

                logging.info("Attempting local NeuralTransformer inference...")"                response = rust_core.generate_neural_response(full_prompt)
                return response
            except Exception as e:  # pylint: disable=broad-exception-caught, unused-variable
                logging.debug(f"Neural backend failed: {e}")"                return None

        return {
            "codex": _try_codex_cli,"            "vllm": _try_vllm,"            "ollama": _try_ollama,"            "lmstudio": _try_lmstudio,"            "neural": _try_neural,"            "copilot": lambda: ("                _try_codex_cli() or _try_vllm() or _try_ollama() or _try_lmstudio() or _try_copilot_cli()
            ),
            "copilot_cli_only": _try_copilot_cli,"            "gh": lambda: _try_gh_copilot(allow_non_command=True),"            "github_models": _try_github_models,"            "openai": _try_openai_api,"            "auto": lambda: ("                _try_lmstudio()
                or _try_vllm()
                or _try_ollama()
                or _try_codex_cli()
                or _try_copilot_cli()
                or _try_github_models()
                or _try_openai_api()
                or _try_gh_copilot(allow_non_command=False)
                or _try_neural()
            ),
        }

    def _select_backend_by_env(self, backend_env: str, backends: dict[str, callable]) -> str | None:
"""
Select backend based on environment string.        backend_mapping = {
            "codex": "codex","            "codex-cli": "codex","            "vllm": "vllm","            "ollama": "ollama","            "lmstudio": "lmstudio","            "lms": "lmstudio","            "neural": "neural","            "rust": "neural","            "local-neural": "neural","            "copilot": "copilot","            "local": "copilot","            "copilot-cli": "copilot_cli_only","            "copilot_cli": "copilot_cli_only","            "gh": "gh","            "gh-copilot": "gh","            "github-models": "github_models","            "github_models": "github_models","            "models": "github_models","            "openai": "openai","            "gpt": "openai","            "localai": "openai","            "huggingface": "openai","        }

        backend_key = backend_mapping.get(backend_env, "auto")"        return backends[backend_key]()

    def _select_and_try_backend(self, backend_env: str, full_prompt: str, original_prompt: str) -> str | None:
"""
Select and attempt to use the appropriate backend based on environment configuration.        repo_root = self.runner._resolve_repo_root()  # pylint: disable=protected-access
        backends = self._create_backend_functions(full_prompt, original_prompt, repo_root)
        return self._select_backend_by_env(backend_env, backends)

    def llm_chat_via_github_models(
        self,
        prompt: str,
        model: str,
        system_prompt: str = "You are a helpful assistant.","        base_url: str | None = None,
        token: str | None = None,
        timeout_s: int = 60,
        max_retries: int = 2,
        use_cache: bool = True,
        stream: bool = False,
        validate_content: bool = True,
    ) -> str:
"""
Call a GitHub Models OpenAI-compatible chat endpoint with caching.        cache_key = self.runner._get_cache_key(prompt, model)  # pylint: disable=protected-access
        if use_cache:
            if cache_key in self.runner._response_cache:  # pylint: disable=protected-access
                self.runner._metrics["cache_hits"] += 1  # pylint: disable=protected-access"                return self.runner._response_cache[cache_key]  # pylint: disable=protected-access
            cached_val = self.runner.disk_cache.get(cache_key)
            if cached_val:
                self.runner._metrics["cache_hits"] += 1  # pylint: disable=protected-access"                self.runner._response_cache[cache_key] = cached_val  # pylint: disable=protected-access
                return cached_val

        self.runner._metrics["requests"] += 1  # pylint: disable=protected-access"        start_time = time.time()
        try:
            result = self.runner.llm_client.llm_chat_via_github_models(
                prompt=prompt,
                model=model,
                system_prompt=system_prompt,
                base_url=base_url,
                token=token,
                timeout_s=timeout_s,
                max_retries=max_retries,
                stream=stream,
            )

            if result:
                if validate_content and not self.runner.validate_response_content(result):
                    logging.warning("Response validation failed")"                if use_cache:
                    self.runner._response_cache[cache_key] = result  # pylint: disable=protected-access
                    self.runner.disk_cache.set(cache_key, result)
                latency_ms = int((time.time() - start_time) * 1000)
                self.runner._metrics["total_latency_ms"] += latency_ms  # pylint: disable=protected-access"                return result
            return ""
except Exception as e:  # pylint: disable=broad-exception-caught, unused-variable
            self.runner._metrics["errors"] += 1  # pylint: disable=protected-access"            logging.error(f"GitHub Models call failed: {e}")"            raise

"""

"""

"""

"""

"""

"""

"""

"""

"""

"""

"""

"""

"""
