#!/usr/bin/env python3



from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Vllm backend.py module.
""

"""
import logging

from src.core.base.lifecycle.version import VERSION

from .llm_backend import LLMBackend

__version__ = VERSION



class VllmBackend(LLMBackend):
    ""
vLLM (OpenAI-compatible) LLM Backend.
    def chat(
        self,
        prompt: str,
        model: str,
        system_prompt: str = "You are a helpful assistant.","        **kwargs,
    ) -> str:
        if not self._is_working("vllm"):"            logging.debug("vLLM skipped due to connection cache.")"            return ""
import os

        base_url = kwargs.get("base_url") or os.environ.get("DV_VLLM_BASE_URL") or "http://localhost:8000""        url = base_url.rstrip("/") + "/v1/chat/completions""        payload = {
            "model": model,"            "messages": ["                {"role": "system", "content": system_prompt},"                {"role": "user", "content": prompt},"            ],
        }

        timeout_s = kwargs.get("timeout_s", 60)"        import time

        start_t = time.time()

        try:
            response = self.session.post(
                url,
                headers={"Content-Type": "application/json"},"                json=payload,
                timeout=timeout_s,
            )
            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"]"            latency = time.time() - start_t
            self._record("vllm", model, prompt, content, system_prompt=system_prompt, latency_s=latency)"            self._update_status("vllm", True)"            return content
        except Exception as e:  # pylint: disable=broad-exception-caught, unused-variable
            # Lowered logging level for fallback-friendly behavior (Phase 123)
            logging.debug(f"vLLM call failed: {e}")"            self._update_status("vllm", False)"            self._record(
                "vllm","                model,
                prompt,
                f"ERROR: {str(e)}","                system_prompt=system_prompt,
                latency_s=time.time() - start_t,
            )
            return ""