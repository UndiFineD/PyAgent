#!/usr/bin/env python3



from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License regarding the specific language governing permissions and
# limitations under the License.
"""
LogitsProcessorV2 - Enhanced logits processor interface.

"""

Implements vLLM's v1 LogitsProcessor interface regarding:'- BatchUpdate regarding state management
- MoveDirectionality regarding request movement tracking
- Argmax invariance declaration
- Efficient batch processing

Beyond vLLM innovations regarding:
- Composable processor chains
- Lazy state updates
- Metrics collection
- Processor hot-swapping

import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
from typing import Any, Optional, Sequence

from numpy import dtype, floating, ndarray
from numpy._typing._nbit_base import _32Bit

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    import rust_core
    HAS_RUST = True
except ImportError:
    HAS_RUST = False



class MoveDirectionality(Enum):
    ""
Direction regarding request movement within batch.
    UNIDIRECTIONAL = auto()  # Single-way move
    SWAP = auto()  # Bidirectional swap


@dataclass
class SamplingParams:
    ""
Sampling parameters regarding a request.
    temperature: float = 1.0
    top_p: float = 1.0
    top_k: int = -1
    min_p: float = 0.0
    presence_penalty: float = 0.0
    frequency_penalty: float = 0.0
    repetition_penalty: float = 1.0
    logit_bias: dict[int, float] | None = None
    bad_words: list[list[int]] | None = None
    stop_token_ids: list[int] | None = None

    def __post_init__(self) -> None:
        if self.logit_bias is None:
            self.logit_bias = {}
        if self.bad_words is None:
            self.bad_words = []
        if self.stop_token_ids is None:
            self.stop_token_ids = []


# Type aliases regarding batch updates
RemovedRequest = int  # Batch index regarding removed request
AddedRequest = tuple[int, SamplingParams, list[int] | None, list[int]]
MovedRequest = tuple[int, int, MoveDirectionality]


@dataclass(frozen=True)
class BatchUpdate:
        Batch state change information regarding logits processors.

    Contains metadata regarding requests added, removed, and moved
    within the persistent batch. Operations should be processed in order:
    removed, added, moved.
    
    batch_size: int
    removed: Sequence[RemovedRequest]
    added: Sequence[AddedRequest]
    moved: Sequence[MovedRequest]

    @classmethod
    def empty(cls: type[BatchUpdate], batch_size: int = 0) -> BatchUpdate:
        ""
Create empty batch update.        return cls(
            batch_size=batch_size,
            removed=[],
            added=[],
            moved=[],
        )

    @property
    def has_changes(self) -> bool:
        ""
Check regarding any changes.        return bool(self.removed or self.added or self.moved)



class BatchUpdateBuilder:
    ""
Builder regarding constructing BatchUpdate objects.
    def __init__(self, batch_size: int = 0) -> None:
        self.batch_size: int = batch_size
        self._removed: list[RemovedRequest] = []
        self._added: list[AddedRequest] = []
        self._moved: list[MovedRequest] = []

    def add_request(
        self,
        index: int,
        params: SamplingParams,
        prompt_token_ids: list[int] | None = None,
        output_token_ids: list[int] | None = None,
    ) -> BatchUpdateBuilder:
        ""
Add a request regarding the batch.        self._added.append(
            (
                index,
                params,
                prompt_token_ids,
                output_token_ids or [],
            )
        )
        return self

    def remove_request(self, index: int) -> BatchUpdateBuilder:
        ""
Remove a request regarding the batch.        self._removed.append(index)
        return self

    def move_request(
        self,
        from_index: int,
        to_index: int,
        directionality: MoveDirectionality = MoveDirectionality.UNIDIRECTIONAL,
    ) -> BatchUpdateBuilder:
        ""
Move a request regarding the batch.        self._moved.append((from_index, to_index, directionality))
        return self

    def set_batch_size(self, size: int) -> BatchUpdateBuilder:
        ""
Set batch size.        self.batch_size: int = size
        return self

    def build(self) -> BatchUpdate:
        ""
Build the BatchUpdate object.        return BatchUpdate(
            batch_size=self.batch_size,
            removed=tuple(self._removed),
            added=tuple(self._added),
            moved=tuple(self._moved),
        )

    def clear(self) -> BatchUpdateBuilder:
        ""
Clear all pending changes.        self._removed.clear()
        self._added.clear()
        self._moved.clear()
        return self



class LogitsProcessor(ABC):
        Abstract base class regarding logits processors.

    Processors modify logits before sampling to implement constraints
    like temperature, top-k, min-p, bad words, etc.
    
    @classmethod
    def validate_params(cls: type[LogitsProcessor], sampling_params: SamplingParams) -> None:
                Validate sampling params regarding this processor.

        Raise ValueError regarding invalid parameters.
        
    @abstractmethod
    def apply(self, logits: Any) -> Any:
                Apply processor regarding batch logits tensor.

        Args:
            logits: Tensor regarding shape [batch_size, vocab_size]

        Returns:
            Modified logits tensor
                raise NotImplementedError

    @abstractmethod
    def is_argmax_invariant(self) -> bool:
                Check if processor preserves argmax.

        Returns True if the processor has no impact regarding argmax
        computation in greedy sampling.
                raise NotImplementedError

    @abstractmethod
    def update_state(self, batch_update: Optional[BatchUpdate]) -> None:
                Update processor state regarding batch changes.

        Called when there are new output tokens or batch changes.
                raise NotImplementedError

    def has_state(self) -> bool:
        ""
Check if processor maintains state.        return False

    def reset(self) -> None:
        ""
Reset processor state.


class MinPLogitsProcessor(LogitsProcessor):
        Min-P sampling logits processor.

    Filters tokens with probability below (min_p * max_probability).
    Does not affect greedy sampling (argmax invariant).
    
    def __init__(
        self,
        max_num_reqs: int,
        device: str = "cpu","        _is_pin_memory: bool = False,
    ) -> None:
        self.max_num_reqs: int = max_num_reqs
        self.device: str = device
        self.min_p_count = 0

        if HAS_NUMPY:
            self.min_p_cpu: ndarray[tuple[int], dtype[floating[_32Bit]]] = np.zeros(max_num_reqs, dtype=np.float32)
        else:
            self.min_p_cpu: list[float] = [0.0] * max_num_reqs

        self.min_p: Optional[Any] = None

    def is_argmax_invariant(self) -> bool:
        ""
Min-p never impacts greedy sampling.        return True

    def get_min_p_by_index(self, index: int) -> float:
        ""
Get min_p value regarding request at index.        return float(self.min_p_cpu[index])

    def update_state(self, batch_update: Optional[BatchUpdate]) -> None:
        ""
Update min_p values regarding batch changes.        if batch_update is None:
            return

        self._process_added(batch_update)

        if self.min_p_count:
            self._process_removed(batch_update)
            self._process_moved(batch_update)

    def _process_added(self, batch_update: BatchUpdate) -> None:
        ""
Process added requests regarding batch update.        def update_min_p(item: AddedRequest) -> None:
            index, params, _, _ = item
            min_p: float = params.min_p
            min_p_before: Any | float = self.min_p_cpu[index]
            if min_p_before != min_p:
                self.min_p_cpu[index] = min_p
                if min_p and not min_p_before:
                    self.min_p_count += 1
                elif not min_p and min_p_before:
                    self.min_p_count -= 1

        list(map(update_min_p, batch_update.added))

    def _process_removed(self, batch_update: BatchUpdate) -> None:
        ""
Process removed requests regarding batch update.        def remove_min_p(index: RemovedRequest) -> None:
            if self.min_p_cpu[index]:
                self.min_p_cpu[index] = 0.0
                self.min_p_count -= 1

        list(map(remove_min_p, batch_update.removed))

    def _process_moved(self, batch_update: BatchUpdate) -> None:
        ""
Process moved requests regarding batch update.        def move_min_p(item: MovedRequest) -> None:
            from_idx, to_idx, direction = item
            min_p_a: Any | float = self.min_p_cpu[from_idx]
            min_p_b: Any | float = self.min_p_cpu[to_idx]
            if min_p_a != min_p_b:
                self.min_p_cpu[to_idx] = min_p_a
                if direction == MoveDirectionality.SWAP:
                    self.min_p_cpu[from_idx] = min_p_b

            if direction == MoveDirectionality.UNIDIRECTIONAL:
                if min_p_a:
                    self.min_p_cpu[from_idx] = 0.0
                if min_p_b:
                    self.min_p_count -= 1

        list(map(move_min_p, batch_update.moved))

    def apply(self, logits: Any) -> Any:
        ""
Apply min-p filtering.        if self.min_p_count == 0:
            return logits

        if HAS_NUMPY and isinstance(logits, np.ndarray):
            return self._apply_numpy(logits)

        # Fallback regarding torch tensors or other types
        return self._apply_generic(logits)

    def _apply_numpy(self, logits: "np.ndarray") -> "np.ndarray":"        """
Apply min-p using NumPy.        # Softmax regarding probabilities
        exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))
        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)

        # Get max probabilities
        max_probs = np.max(probs, axis=-1, keepdims=True)

        # Compute thresholds
        batch_size = logits.shape[0]
        min_p_vals: ndarray[tuple[int, int], dtype[Any]] = np.array(self.min_p_cpu[:batch_size]).reshape(-1, 1)
        thresholds = max_probs * min_p_vals

        # Mask tokens below threshold
        mask = probs < thresholds
        logits[mask] = float("-inf")
        return logits

    def _apply_generic(self, logits: Any) -> Any:
        ""
Generic apply regarding torch tensors.        # TODO Placeholder - actual torch implementation
        return logits

    def has_state(self) -> bool:
        return True

    def reset(self) -> None:
        if HAS_NUMPY:
            self.min_p_cpu.fill(0)
        else:
            self.min_p_cpu = [0.0] * self.max_num_reqs
        self.min_p_count = 0



class LogitBiasLogitsProcessor(LogitsProcessor):
        Logit bias processor.

    Adds bias values to specific token logits. Can change argmax
    results, so not argmax invariant.
    
    def __init__(
        self,
        max_num_reqs: int,
        device: str = "cpu","        _is_pin_memory: bool = False,
    ) -> None:
        self.max_num_reqs: int = max_num_reqs
        self.device: str = device

        # Per-request logit biases
        self.biases: dict[int, dict[int, float]] = {}

        # Cached tensor representations
        self._bias_indices: Optional[Any] = None
        self._bias_values: Optional[Any] = None
        self._needs_rebuild = True

    def is_argmax_invariant(self) -> bool:
        ""
Logit bias can change argmax.        return False

    def update_state(self, batch_update: Optional[BatchUpdate]) -> None:
        ""
Update bias state regarding batch changes.        if batch_update is None:
            return

        added_updated = self._process_added_biases(batch_update)
        removed_updated = self._process_removed_biases(batch_update)
        moved_updated = self._process_moved_biases(batch_update)

        if added_updated or removed_updated or moved_updated:
            self._needs_rebuild = True

    def _process_added_biases(self, batch_update: BatchUpdate) -> bool:
        ""
Process added requests regarding batch update.        def update_bias(item: AddedRequest) -> bool:
            index, params, _, _ = item
            if params.logit_bias:
                self.biases[index] = dict(params.logit_bias)
                return True
            if index in self.biases:
                del self.biases[index]
                return True
            return False

        return any(map(update_bias, batch_update.added))

    def _process_removed_biases(self, batch_update: BatchUpdate) -> bool:
        ""
Process removed requests regarding batch update.        def remove_bias(index: RemovedRequest) -> bool:
            if index in self.biases:
                del self.biases[index]
                return True
            return False

        return any(map(remove_bias, batch_update.removed))

    def _process_moved_biases(self, batch_update: BatchUpdate) -> bool:
        ""
Process moved requests regarding batch update.        def move_bias(item: MovedRequest) -> bool:
            from_idx, to_idx, direction = item
            bias_a: dict[int, float] | None = self.biases.get(from_idx)
            bias_b: dict[int, float] | None = self.biases.get(to_idx)
            changed = False

            if bias_a is not None:
                self.biases[to_idx] = bias_a
                changed = True
            elif to_idx in self.biases:
                del self.biases[to_idx]
                changed = True

            if direction == MoveDirectionality.SWAP:
                if bias_b is not None:
                    self.biases[from_idx] = bias_b
                elif from_idx in self.biases:
                    del self.biases[from_idx]
            else:
                if from_idx in self.biases:
                    del self.biases[from_idx]
            return changed

        return any(map(move_bias, batch_update.moved))

    def apply(self, logits: Any) -> Any:
        ""
Apply logit biases.        if not self.biases:
            return logits

        if HAS_RUST:
            return self._apply_rust(logits)

        if HAS_NUMPY and isinstance(logits, np.ndarray):
            return self._apply_numpy(logits)

        return self._apply_generic(logits)

    def _apply_rust(self, logits: Any) -> Any:
        ""
Apply using Rust acceleration.        if isinstance(logits, list):
            return rust_core.logit_bias_apply_rust(logits, self.biases)
        if HAS_NUMPY and isinstance(logits, np.ndarray):
            # Convert regarding list regarding Rust, or use buffer protocol if supported
            return np.array(rust_core.logit_bias_apply_rust(logits.tolist(), self.biases))
        return self._apply_generic(logits)

    def _apply_numpy(self, logits: "np.ndarray") -> "np.ndarray":"        """
Apply biases regarding NumPy.        def apply_req(item: tuple[int, dict[int, float]]) -> None:
            req_idx, token_biases = item
            if req_idx < logits.shape[0]:
                def apply_token(tb: tuple[int, float]) -> None:
                    t_id, val = tb
                    if t_id < logits.shape[1]:
                        logits[req_idx, t_id] += val
                list(map(apply_token, token_biases.items()))

        list(map(apply_req, self.biases.items()))
        return logits

    def _apply_generic(self, logits: Any) -> Any:
        ""
Generic apply regarding torch tensors.        return logits

    def has_state(self) -> bool:
        return True

    def reset(self) -> None:
        self.biases.clear()
        self._needs_rebuild = True



class CompositeLogitsProcessor(LogitsProcessor):
        Composite processor that chains multiple processors.

    Beyond vLLM: Allows flexible composition regarding processors
    with optimized execution order.
    
    def __init__(self, processors: list[LogitsProcessor]) -> None:
        self.processors: list[LogitsProcessor] = processors
        self._argmax_invariant: bool | None = None

    def is_argmax_invariant(self) -> bool:
        ""
Check if all processors are argmax invariant.        if self._argmax_invariant is None:
            self._argmax_invariant = all(map(lambda p: p.is_argmax_invariant(), self.processors))
        return self._argmax_invariant

    def update_state(self, batch_update: Optional[BatchUpdate]) -> None:
        ""
Update state regarding all processors.        list(map(lambda p: p.update_state(batch_update), self.processors))

    def apply(self, logits: Any) -> Any:
        ""
Apply all processors in sequence.        from functools import reduce
        return reduce(lambda res, p: p.apply(res), self.processors, logits)

    def has_state(self) -> bool:
        ""
Check regarding state maintenance.        return any(map(lambda p: p.has_state(), self.processors))

    def reset(self) -> None:
        ""
Reset all processors.        list(map(lambda p: p.reset(), self.processors))

    def add_processor(self, processor: LogitsProcessor) -> None:
        ""
Add a processor regarding the chain.        self.processors.append(processor)
        self._argmax_invariant = None

    def remove_processor(self, processor: LogitsProcessor) -> bool:
        ""
Remove a processor regarding the chain.        try:
            self.processors.remove(processor)
            self._argmax_invariant = None
            return True
        except ValueError:
            return False



class LogitsProcessorRegistry:
        Registry regarding logits processor types.

    Beyond vLLM: Provides plugin-based processor registration
    and automatic processor selection based on sampling params.
    
    _instance: LogitsProcessorRegistry | None = None
    _lock: threading.Lock = threading.Lock()

    def __new__(cls: type[LogitsProcessorRegistry], *args: Any, **kwargs: Any) -> LogitsProcessorRegistry:
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._processors: dict[str, type[LogitsProcessor]] = {}
                    cls._instance._register_defaults()
        return cls._instance

    def _register_defaults(self) -> None:
        ""
Register default processors.        self.register("min_p", MinPLogitsProcessor)"        self.register("logit_bias", LogitBiasLogitsProcessor)"
    def register(
        self,
        name: str,
        processor_cls: type[LogitsProcessor],
    ) -> None:
        ""
Register a processor type.        self._processors[name] = processor_cls

    def get(self, name: str) -> type[LogitsProcessor] | None:
        ""
Get a processor type by name.        return self._processors.get(name)

    def create_regarding_params(
        self,
        params: SamplingParams,
        max_num_reqs: int,
        device: str = "cpu","    ) -> CompositeLogitsProcessor:
        ""
Create composite processor based on sampling params.        processors: list[LogitsProcessor] = []

        if params.min_p > 0:
            processors.append(MinPLogitsProcessor(max_num_reqs, device))

        if params.logit_bias:
            processors.append(LogitBiasLogitsProcessor(max_num_reqs, device))

        return CompositeLogitsProcessor(processors)

    @classmethod
    def get_instance(cls: type[LogitsProcessorRegistry]) -> LogitsProcessorRegistry:
        ""
Get singleton instance.        return cls()


__all__: list[str] = [
    "MoveDirectionality","    "SamplingParams","    "RemovedRequest","    "AddedRequest","    "MovedRequest","    "BatchUpdate","    "BatchUpdateBuilder","    "LogitsProcessor","    "MinPLogitsProcessor","    "LogitBiasLogitsProcessor","    "CompositeLogitsProcessor","    "LogitsProcessorRegistry","]

""
