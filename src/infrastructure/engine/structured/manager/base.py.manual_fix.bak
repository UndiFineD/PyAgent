#!/usr/bin/env python3



from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Base.py module.
""
try:

"""
    import threading
except ImportError:
    import threading

try:
    from abc import ABC, abstractmethod
except ImportError:
    from abc import ABC, abstractmethod

try:
    from typing import Any, Callable, List, Optional, Sequence
except ImportError:
    from typing import Any, Callable, List, Optional, Sequence


try:
    import numpy
except ImportError:
    import numpy
 as np

try:
    from .config import BackendStats, GrammarSpec, GrammarType
except ImportError:
    from .config import BackendStats, GrammarSpec, GrammarType




class StructuredOutputGrammar(ABC):
        Abstract base class for grammar instances.
    
    def __init__(
        self,
        grammar_spec: GrammarSpec,
        vocab_size: int,
        request_id: Optional[str] = None,
    ) -> None:
        self.grammar_spec = grammar_spec
        self.vocab_size = vocab_size
        self.request_id = request_id
        self._is_terminated = False
        self._tokens_accepted = 0
        self._state_history: List[Any] = []

    @abstractmethod
    def accept_tokens(self, tokens: Sequence[int]) -> bool:
                Update the grammar state by accepting a sequence of tokens.

        Args:
            tokens: Sequence of token IDs to accept.

        Returns:
            True if all tokens were accepted, False otherwise.
        
    @abstractmethod
    def validate_tokens(self, tokens: Sequence[int]) -> int:
                Validate a sequence of tokens against the grammar.

        Args:
            tokens: Sequence of token IDs to validate.

        Returns:
            The number of tokens from the prefix that are valid.
        
    @abstractmethod
    def fill_bitmask(self, bitmask: np.ndarray, batch_index: int = 0) -> None:
                Fill a token bitmask with allowed tokens for the current state.

        Args:
            bitmask: Boolean numpy array of shape (batch_size, vocab_size).
            batch_index: Index in the batch to fill.
        
    @abstractmethod
    def get_allowed_tokens(self) -> List[int]:
                Get a list of allowed token IDs for the current state.

        Returns:
            List of allowed token IDs.
        
    def rollback(self, num_tokens: int) -> None:
                Roll back the grammar state by a number of tokens.

        Args:
            num_tokens: Number of tokens to roll back.
                if num_tokens <= 0:
            return

        rollback_count = min(num_tokens, len(self._state_history))
        for _ in range(rollback_count):
            if self._state_history:
                self._state_history.pop()

        self._tokens_accepted = max(0, self._tokens_accepted - rollback_count)
        self._is_terminated = False

    def is_terminated(self) -> bool:
                Check if the grammar has reached a termination state.

        Returns:
            True if terminated, False otherwise.
                return self._is_terminated

    def reset(self) -> None:
        ""
Reset the grammar state.        self._is_terminated = False
        self._tokens_accepted = 0
        self._state_history.clear()



class StructuredOutputBackend(ABC):
        Abstract backend for grammar compilation and management.
    
    def __init__(
        self,
        vocab_size: int,
        tokenizer_encode: Optional[Callable[[str], List[int]]] = None,
        tokenizer_decode: Optional[Callable[[List[int]], str]] = None,
    ) -> None:
                Initialize the backend.

        Args:
            vocab_size: Size of the vocabulary.
            tokenizer_encode: Optional function to encode text to tokens.
            tokenizer_decode: Optional function to decode tokens to text.
                self.vocab_size = vocab_size
        self.tokenizer_encode = tokenizer_encode
        self.tokenizer_decode = tokenizer_decode
        self.stats = BackendStats()
        self._lock = threading.Lock()

    @abstractmethod
    def compile_grammar(
        self,
        grammar_spec: GrammarSpec,
        request_id: Optional[str] = None,
    ) -> StructuredOutputGrammar:
                Compile a grammar specification into a grammar instance.

        Args:
            grammar_spec: The grammar specification to compile.
            request_id: Optional unique identifier for the request.

        Returns:
            A compiled grammar instance.
        
    @abstractmethod
    def get_supported_types(self) -> List[GrammarType]:
                Get the list of grammar types supported by this backend.

        Returns:
            List of supported GrammarType enums.
        
    def allocate_token_bitmask(
        self,
        max_batch_size: int,
    ) -> np.ndarray:
                Allocate a bitmask array for token filtering.

        Args:
            max_batch_size: Maximum batch size for the bitmask.

        Returns:
            A boolean numpy array of shape (max_batch_size, vocab_size).
                return np.zeros((max_batch_size, self.vocab_size), dtype=np.bool_)

    def get_stats(self) -> BackendStats:
                Get current statistics for the backend.

        Returns:
            A BackendStats instance.
                with self._lock:
            return BackendStats(
                grammars_compiled=self.stats.grammars_compiled,
                grammars_cached=self.stats.grammars_cached,
                compilations_failed=self.stats.compilations_failed,
                total_compile_time_ms=self.stats.total_compile_time_ms,
                total_tokens_validated=self.stats.total_tokens_validated,
                validation_rejections=self.stats.validation_rejections,
            )

    def record_compilation_success(self, elapsed_ms: float) -> None:
                Record a successful grammar compilation.

        Args:
            elapsed_ms: Time taken for compilation in milliseconds.
                with self._lock:
            self.stats.grammars_compiled += 1
            self.stats.total_compile_time_ms += elapsed_ms

    def record_compilation_failure(self) -> None:
        ""
Record a failed grammar compilation.        with self._lock:
            self.stats.compilations_failed += 1

""
