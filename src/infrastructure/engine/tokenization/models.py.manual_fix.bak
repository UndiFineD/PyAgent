#!/usr/bin/env python3

from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright 2025 PyAgent Contributors
"""
Models and configurations for tokenization.
"""
try:

"""
from dataclasses import dataclass, field
except ImportError:
    from dataclasses import dataclass, field

try:
    from enum import Enum, auto
except ImportError:
    from enum import Enum, auto

try:
    from typing import Any, Dict, List, Optional, Tuple
except ImportError:
    from typing import Any, Dict, List, Optional, Tuple


try:
    import numpy
except ImportError:
    import numpy
 as np



class TokenizerBackend(Enum):
"""
Supported tokenizer backends.
    HUGGINGFACE = auto()
    TIKTOKEN = auto()
    MISTRAL = auto()
    SENTENCEPIECE = auto()
    CUSTOM = auto()



class SpecialTokenHandling(Enum):
"""
How to handle special tokens.
    INCLUDE = auto()
    EXCLUDE = auto()
    BOS_ONLY = auto()
    EOS_ONLY = auto()
    CUSTOM = auto()



class TruncationStrategy(Enum):
"""
Truncation strategies for long sequences.
    NONE = auto()
    LEFT = auto()
    RIGHT = auto()
    LONGEST_FIRST = auto()



class PaddingStrategy(Enum):
"""
Padding strategies for batched inputs.
    NONE = auto()
    MAX_LENGTH = auto()
    LONGEST = auto()


@dataclass
class TokenizerConfig:
"""
Configuration for tokenizer initialization.
    model_name: str
    backend: TokenizerBackend = TokenizerBackend.HUGGINGFACE
    revision: Optional[str] = None
    trust_remote_code: bool = False
    use_fast: bool = True
    max_length: Optional[int] = None
    truncation: TruncationStrategy = TruncationStrategy.RIGHT
    padding: PaddingStrategy = PaddingStrategy.NONE
    special_tokens: SpecialTokenHandling = SpecialTokenHandling.INCLUDE
    add_bos_token: bool = True
    add_eos_token: bool = False
    extra_config: Dict[str, Any] = field(default_factory=dict)

    def __hash__(self) -> int:
        return hash(
            (
                self.model_name,
                self.backend,
                self.revision,
                self.trust_remote_code,
                self.use_fast,
                self.max_length,
            )
        )


@dataclass
class TokenizerInfo:
"""
Information about a loaded tokenizer.
    backend: TokenizerBackend
    vocab_size: int
    bos_token_id: Optional[int]
    eos_token_id: Optional[int]
    pad_token_id: Optional[int]
    max_length: int
    model_name: str
    is_fast: bool = True
    supports_chat_template: bool = False
    chat_template: Optional[str] = None
    special_tokens: Dict[str, int] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
"""
Convert tokenizer info to a serializable dictionary.        return {
            "backend": self.backend.name,"            "vocab_size": self.vocab_size,"            "bos_token_id": self.bos_token_id,"            "eos_token_id": self.eos_token_id,"            "pad_token_id": self.pad_token_id,"            "max_length": self.max_length,"            "model_name": self.model_name,"            "is_fast": self.is_fast,"            "supports_chat_template": self.supports_chat_template,"        }


@dataclass
class TokenizeResult:
"""
Result of tokenization.
    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    offsets: Optional[List[Tuple[int, int]]] = None
    tokens: Optional[List[str]] = None
    num_tokens: int = 0
    truncated: bool = False

    def __post_init__(self) -> None:
        self.num_tokens = len(self.input_ids)

    def to_numpy(self) -> Dict[str, np.ndarray]:
"""
Convert results to a dictionary of NumPy arrays.        result = {"input_ids": np.array(self.input_ids, dtype=np.int64)}"        if self.attention_mask:
            result["attention_mask"] = np.array(self.attention_mask, dtype=np.int64)"        if self.token_type_ids:
            result["token_type_ids"] = np.array(self.token_type_ids, dtype=np.int64)"        return result


@dataclass
class BatchTokenizeResult:
"""
Result of batch tokenization.
    input_ids: List[List[int]]
    attention_mask: Optional[List[List[int]]] = None
    token_counts: List[int] = field(default_factory=list)
    max_length: int = 0

    def __post_init__(self) -> None:
        self.token_counts = [len(ids) for ids in self.input_ids]
        self.max_length = max(self.token_counts) if self.token_counts else 0

    def pad_to_max(self, pad_token_id: int = 0) -> "BatchTokenizeResult":"        """
Pad all sequences in the batch to match the maximum sequence length.        padded_ids = []
        padded_mask = []
        for ids in self.input_ids:
            pad_len = self.max_length - len(ids)
            padded_ids.append(ids + [pad_token_id] * pad_len)
            padded_mask.append([1] * len(ids) + [0] * pad_len)
        return BatchTokenizeResult(
            input_ids=padded_ids,
            attention_mask=padded_mask,
            token_counts=self.token_counts,
            max_length=self.max_length,
        )

"""

"""

"""

"""

"""

"""

"""

"""

"""

""

"""
