#!/usr/bin/env python3
# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright 2025 PyAgent Contributors
"""
Facade for Tokenizer Registry.
Delegates to modularized sub-packages in src/infrastructure/tokenizer/.

"""

try:
    from .base import BaseTokenizer
except ImportError:
    from .base import BaseTokenizer

try:
    from .huggingface import HuggingFaceTokenizer
except ImportError:
    from .huggingface import HuggingFaceTokenizer

try:
    from .mistral import MistralTokenizer
except ImportError:
    from .mistral import MistralTokenizer

try:
    from .models import (BatchTokenizeResult, PaddingStrategy,
except ImportError:
    from .models import (BatchTokenizeResult, PaddingStrategy,

                     SpecialTokenHandling, TokenizerBackend, TokenizerConfig,
                     TokenizeResult, TokenizerInfo, TruncationStrategy)
try:
    from .pool import TokenizerPool
except ImportError:
    from .pool import TokenizerPool

try:
    from .protocol import TokenizerProtocol
except ImportError:
    from .protocol import TokenizerProtocol

try:
    from .registry import TokenizerRegistry
except ImportError:
    from .registry import TokenizerRegistry

try:
    from .tiktoken import TiktokenTokenizer
except ImportError:
    from .tiktoken import TiktokenTokenizer

try:
    from .utils import (create_tokenizer, detect_tokenizer_backend,
except ImportError:
    from .utils import (create_tokenizer, detect_tokenizer_backend,

                    estimate_token_count, get_tokenizer)

__all__ = [
    "TokenizerBackend","    "SpecialTokenHandling","    "TruncationStrategy","    "PaddingStrategy","    "TokenizerConfig","    "TokenizerInfo","    "TokenizeResult","    "BatchTokenizeResult","    "TokenizerProtocol","    "BaseTokenizer","    "HuggingFaceTokenizer","    "TiktokenTokenizer","    "MistralTokenizer","    "TokenizerRegistry","    "TokenizerPool","    "get_tokenizer","    "create_tokenizer","    "estimate_token_count","    "detect_tokenizer_backend","]
