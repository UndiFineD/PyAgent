#!/usr/bin/env python3

from __future__ import annotations



# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright 2025 PyAgent Contributors
"""
NVIDIA CUDA platform implementation.
""

"""
import logging
from typing import Set

from .base import Platform
from .models import (AttentionBackend, DeviceCapability, DeviceFeature,
                     DeviceInfo, MemoryInfo, PlatformType, QuantizationType)

logger = logging.getLogger(__name__)



class CudaPlatform(Platform):
    ""
NVIDIA CUDA platform implementation.
    _torch = None

    @classmethod
    def get_platform_type(cls) -> PlatformType:
        return PlatformType.CUDA

    @classmethod
    def is_available(cls) -> bool:
        try:
            import torch

            return torch.cuda.is_available()
        except ImportError:
            return False

    def _get_torch(self):
        if self._torch is None:
            import torch

            self._torch = torch
        return self._torch

    def get_device_count(self) -> int:
        torch = self._get_torch()
        return torch.cuda.device_count()

    def get_device_capability(self, device_id: int = 0) -> DeviceCapability:
        torch = self._get_torch()
        cap = torch.cuda.get_device_capability(device_id)
        return DeviceCapability(major=cap[0], minor=cap[1])

    def get_device_name(self, device_id: int = 0) -> str:
        torch = self._get_torch()
        return torch.cuda.get_device_name(device_id)

    def get_memory_info(self, device_id: int = 0) -> MemoryInfo:
        torch = self._get_torch()
        props = torch.cuda.get_device_properties(device_id)
        total = props.total_memory
        reserved = torch.cuda.memory_reserved(device_id)
        allocated = torch.cuda.memory_allocated(device_id)
        free = total - reserved
        return MemoryInfo(
            total_bytes=total,
            free_bytes=free,
            used_bytes=allocated,
            reserved_bytes=reserved,
        )

    def get_device_features(self, device_id: int = 0) -> DeviceFeature:
        cap = self.get_device_capability(device_id)
        features = DeviceFeature.NONE

        if cap >= DeviceCapability(5, 3):
            features |= DeviceFeature.FP16
        if cap >= DeviceCapability(7, 0):
            features |= DeviceFeature.TENSOR_CORES
            features |= DeviceFeature.INT8
        if cap >= DeviceCapability(8, 0):
            features |= DeviceFeature.BF16
            features |= DeviceFeature.FLASH_ATTENTION
            features |= DeviceFeature.CUDA_GRAPHS
            features |= DeviceFeature.ASYNC_COPY
        if cap >= DeviceCapability(8, 9):
            features |= DeviceFeature.FP8
            features |= DeviceFeature.INT4
            features |= DeviceFeature.SPARSE_OPS
        if self.get_device_count() > 1:
            features |= DeviceFeature.MULTI_GPU

        return features

    def get_driver_version(self) -> str:
        torch = self._get_torch()
        try:
            return torch.version.cuda or "unknown""        except (AttributeError, RuntimeError):
            return "unknown"
    def get_supported_quantizations(self) -> Set[QuantizationType]:
        cap = self.get_device_capability()
        quants = {QuantizationType.NONE, QuantizationType.INT8}

        if cap >= DeviceCapability(7, 5):
            quants |= {QuantizationType.GPTQ, QuantizationType.AWQ, QuantizationType.INT4}
        if cap >= DeviceCapability(8, 0):
            quants |= {
                QuantizationType.MARLIN,
                QuantizationType.EXLLAMA,
                QuantizationType.EXLLAMA_V2,
                QuantizationType.NF4,
                QuantizationType.BITSANDBYTES,
            }
        if cap >= DeviceCapability(8, 9):
            quants |= {
                QuantizationType.FP8,
                QuantizationType.FP8_E4M3,
                QuantizationType.FP8_E5M2,
            }

        return quants

    def get_device_info(self, device_id: int = 0) -> DeviceInfo:
        return DeviceInfo(
            device_id=device_id,
            name=self.get_device_name(device_id),
            platform=self.get_platform_type(),
            capability=self.get_device_capability(device_id),
            memory=self.get_memory_info(device_id),
            features=self.get_device_features(device_id),
            driver_version=self.get_driver_version(),
        )

    def select_attention_backend(self, _capability: DeviceCapability) -> AttentionBackend:
        backends = self.get_attention_backends()
        return backends[0] if backends else AttentionBackend.DEFAULT

    def empty_cache(self) -> None:
        torch = self._get_torch()
        torch.cuda.empty_cache()

    def synchronize(self, device_id: int = 0) -> None:
        torch = self._get_torch()
        with torch.cuda.device(device_id):
            torch.cuda.synchronize()

    def set_device(self, device_id: int) -> None:
        torch = self._get_torch()
        torch.cuda.set_device(device_id)

    def get_current_device(self) -> int:
        torch = self._get_torch()
        return torch.cuda.current_device()
