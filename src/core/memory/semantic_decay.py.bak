#!/usr/bin/env python3
# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import annotations


"""
""""Module: semantic_decay
Implements advanced Synaptic Decay, Neural Context Pruning, and Semantic Cache Invalidation.
Phase 91-92: Sliding-window invalidation and attention-entropy maps for KV-cache landmarks.
"""

try:
    import time
except ImportError:
    import time

try:
    import logging
except ImportError:
    import logging

try:
    import math
except ImportError:
    import math

try:
    from typing import Any, Dict, List, Optional, Tuple, Set
except ImportError:
    from typing import Any, Dict, List, Optional, Tuple, Set

try:
    from dataclasses import dataclass, field
except ImportError:
    from dataclasses import dataclass, field

try:
    from collections import defaultdict
except ImportError:
    from collections import defaultdict


logger = logging.getLogger(__name__)


@dataclass
class MemoryBlock:
    """Represents a block of memory with metadata for pruning decisions."""key: str
    content: Any
    access_count: int = 0
    last_access: float = field(default_factory=time.time)
    creation_time: float = field(default_factory=time.time)
    relevance_score: float = 1.0
    attention_entropy: float = 0.0
    semantic_fingerprint: Optional[str] = None
    dependencies: Set[str] = field(default_factory=set)


@dataclass
class PruningDecision:
    """Result of pruning analysis."""block_key: str
    should_prune: bool
    reason: str
    confidence: float
    alternative_blocks: List[str] = field(default_factory=list)



class NeuralContextPruner:
    """Advanced neural context pruning using attention-entropy maps.
    
    Implements Phase 92: Using attention-entropy maps to identify and prune 
    KV-cache landmarks, enabling 1M+ token contexts.
    """
    def __init__(self, attention_threshold: float = 0.3, entropy_window: int = 1000):
        self.attention_threshold = attention_threshold
        self.entropy_window = entropy_window
        self.attention_history: Dict[str, List[float]] = defaultdict(list)
        
    def calculate_attention_entropy(self, block: MemoryBlock, context_window: List[MemoryBlock]) -> float:
        """Calculate attention entropy for a memory block within its context window.
        
        Higher entropy indicates the block is a semantic landmark that should be preserved.
        Lower entropy indicates redundant or low-value information that can be pruned.
        """if not context_window:
            return 0.0
            
        # Calculate attention distribution within the window
        attentions = []
        for ctx_block in context_window:
            # Simplified attention calculation based on semantic similarity and recency
            similarity = self._calculate_semantic_similarity(block, ctx_block)
            recency_weight = math.exp(-(time.time() - ctx_block.last_access) / 3600)  # 1 hour decay
            attention = similarity * recency_weight * ctx_block.access_count
            attentions.append(attention)
        
        # Normalize attentions
        total_attention = sum(attentions)
        if total_attention == 0:
            return 0.0
            
        normalized_attentions = [a / total_attention for a in attentions]
        
        # Calculate entropy
        entropy = 0.0
        for p in normalized_attentions:
            if p > 0:
                entropy -= p * math.log2(p)
                
        return entropy
    
    def _calculate_semantic_similarity(self, block1: MemoryBlock, block2: MemoryBlock) -> float:
        """Calculate semantic similarity between two memory blocks."""if block1.semantic_fingerprint and block2.semantic_fingerprint:
            # Simple fingerprint-based similarity (could be enhanced with embeddings)
            return 1.0 if block1.semantic_fingerprint == block2.semantic_fingerprint else 0.1
        else:
            # Fallback to content-based similarity
            return self._content_similarity(block1.content, block2.content)
    
    def _content_similarity(self, content1: Any, content2: Any) -> float:
        """Calculate content-based similarity."""if isinstance(content1, str) and isinstance(content2, str):
            # Simple Jaccard similarity for text content
            words1 = set(content1.lower().split())
            words2 = set(content2.lower().split())
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            return intersection / union if union > 0 else 0.0
        return 0.1  # Default low similarity
    
    def should_prune_block(self, block: MemoryBlock, context_window: List[MemoryBlock]) -> PruningDecision:
        """Determine if a memory block should be pruned based on neural context analysis.
        """entropy = self.calculate_attention_entropy(block, context_window)
        block.attention_entropy = entropy
        
        # Low entropy + low access + old age = candidate for pruning
        age_hours = (time.time() - block.creation_time) / 3600
        access_recency = (time.time() - block.last_access) / 3600
        
        # Pruning criteria
        low_attention = entropy < self.attention_threshold
        low_access = block.access_count < 3
        old_age = age_hours > 24  # Older than 1 day
        stale = access_recency > 168  # Not accessed in 1 week
        
        should_prune = low_attention and (low_access or old_age or stale)
        
        if should_prune:
            reasons = []
            if low_attention:
                reasons.append(f"low attention entropy ({entropy:.3f})")"            if low_access:
                reasons.append(f"low access count ({block.access_count})")"            if old_age:
                reasons.append(f"old age ({age_hours:.1f}h)")"            if stale:
                reasons.append(f"stale ({access_recency:.1f}h since last access)")"                
            reason = " + ".join(reasons)"            confidence = min(0.9, 0.5 + (self.attention_threshold - entropy) * 2)
        else:
            reason = f"high attention entropy ({entropy:.3f}) or recent/frequent access""            confidence = 0.8
            
        return PruningDecision(
            block_key=block.key,
            should_prune=should_prune,
            reason=reason,
            confidence=confidence
        )



class SemanticCacheInvalidator:
    """Implements Phase 91: Sliding-window invalidation of LSH buckets.
    
    Manages semantic cache invalidation to prevent context staleness in long-running conversations.
    """
    def __init__(self, window_size: int = 100, invalidation_threshold: float = 0.7):
        self.window_size = window_size
        self.invalidation_threshold = invalidation_threshold
        self.access_window: List[Tuple[str, float]] = []
        self.semantic_clusters: Dict[str, Set[str]] = defaultdict(set)
        
    def track_access(self, key: str, semantic_fingerprint: Optional[str] = None):
        """Track memory access for sliding window analysis."""current_time = time.time()
        self.access_window.append((key, current_time))
        
        # Maintain window size
        if len(self.access_window) > self.window_size:
            self.access_window.pop(0)
            
        # Update semantic clustering
        if semantic_fingerprint:
            self.semantic_clusters[semantic_fingerprint].add(key)
    
    def get_invalidated_keys(self, current_context: List[str]) -> Set[str]:
        """Identify keys that should be invalidated based on sliding window analysis.
        
        Returns keys that are no longer relevant in the current context window.
        """invalidated = set()
        
        # Find keys outside the current context that haven't been accessed recently'        context_set = set(current_context)
        window_keys = {key for key, _ in self.access_window}
        
        # Keys in window but not in current context may be stale
        stale_candidates = window_keys - context_set
        
        for key in stale_candidates:
            # Check if this key's semantic cluster is still relevant'            key_fingerprint: Optional[str] = self._get_fingerprint_for_key(key)
            if key_fingerprint is not None:
                cluster = self.semantic_clusters[key_fingerprint]
                cluster_relevance = len(cluster & context_set) / len(cluster) if cluster else 0
                
                if cluster_relevance < self.invalidation_threshold:
                    invalidated.add(key)
            else:
                # No semantic fingerprint, use recency-based invalidation
                key_age = self._get_key_age(key)
                if key_age > 3600:  # Older than 1 hour
                    invalidated.add(key)
                    
        return invalidated
    
    def _get_fingerprint_for_key(self, key: str) -> Optional[str]:
        """Get semantic fingerprint for a key (TODO Placeholder implementation)."""
# This would integrate with the actual memory storage system
        return None  # Explicitly document that this returns None in current implementation
        
    def _get_key_age(self, key: str) -> float:
        """Get age of key in seconds."""for k, timestamp in self.access_window:
            if k == key:
                return time.time() - timestamp
        return float('inf')'


class SynapticDecay:
    """Enhanced synaptic decay system with neural context pruning and semantic invalidation.
    
    Integrates multiple pruning strategies for optimal memory management.
    """
    def __init__(self, decay_rate: float = 0.05, relevance_threshold: float = 0.2):
        self.decay_rate = decay_rate
        self.relevance_threshold = relevance_threshold
        self.knowledge_scores: Dict[str, float] = {}
        self.last_access: Dict[str, float] = {}
        
        # Advanced components
        self.neural_pruner = NeuralContextPruner()
        self.semantic_invalidator = SemanticCacheInvalidator()
        self.memory_blocks: Dict[str, MemoryBlock] = {}

    def track_access(self, key_id: str, semantic_fingerprint: Optional[str] = None):
        """Update access timestamp and boost relevance score."""self.last_access[key_id] = time.time()
        current_score = self.knowledge_scores.get(key_id, 1.0)
        self.knowledge_scores[key_id] = min(1.0, current_score + 0.1)
        
        # Update memory block if it exists
        if key_id in self.memory_blocks:
            block = self.memory_blocks[key_id]
            block.access_count += 1
            block.last_access = time.time()
            block.relevance_score = self.knowledge_scores[key_id]
            
        # Track in semantic invalidator
        self.semantic_invalidator.track_access(key_id, semantic_fingerprint)

    def add_memory_block(self, key: str, content: Any, semantic_fingerprint: Optional[str] = None):
        """Add a new memory block to the system."""block = MemoryBlock(
            key=key,
            content=content,
            semantic_fingerprint=semantic_fingerprint
        )
        self.memory_blocks[key] = block
        self.knowledge_scores[key] = 1.0
        self.last_access[key] = time.time()

    def process_decay(self, keys: List[str]) -> List[str]:
        """Enhanced decay processing with neural context pruning.
        
        Returns keys that should be pruned based on multiple criteria.
        """to_prune = []
        current_time = time.time()
        
        # Get all memory blocks for context analysis
        all_blocks = list(self.memory_blocks.values())
        
        for key in keys:
            if key not in self.memory_blocks:
                # Fallback to simple decay for non-block keys
                if self._should_prune_simple(key, current_time):
                    to_prune.append(key)
                continue
                
            block = self.memory_blocks[key]
            
            # Neural context pruning
            pruning_decision = self.neural_pruner.should_prune_block(block, all_blocks)
            
            if pruning_decision.should_prune:
                logger.debug(f"Pruning {key}: {pruning_decision.reason} (confidence: {pruning_decision.confidence:.2f})")"                to_prune.append(key)
            else:
                # Update relevance score with exponential decay
                time_since_access = current_time - self.last_access.get(key, current_time)
                decay_factor = math.exp(-self.decay_rate * time_since_access / 86400)  # Daily decay
                self.knowledge_scores[key] = self.knowledge_scores.get(key, 1.0) * decay_factor
                
                # Final check against relevance threshold
                if self.knowledge_scores[key] < self.relevance_threshold:
                    to_prune.append(key)
        
        # Semantic cache invalidation
        current_context = [k for k in keys if k not in to_prune]
        invalidated_keys = self.semantic_invalidator.get_invalidated_keys(current_context)
        to_prune.extend(invalidated_keys)
        
        # Clean up pruned blocks
        for key in to_prune:
            if key in self.memory_blocks:
                del self.memory_blocks[key]
            if key in self.knowledge_scores:
                del self.knowledge_scores[key]
            if key in self.last_access:
                del self.last_access[key]
                
        return list(set(to_prune))  # Remove duplicates

    def _should_prune_simple(self, key: str, current_time: float) -> bool:
        """Simple pruning decision for keys without full block data."""time_since_access = current_time - self.last_access.get(key, current_time)
        decay_factor = math.exp(-self.decay_rate * time_since_access / 86400)
        relevance = self.knowledge_scores.get(key, 1.0) * decay_factor
        return relevance < self.relevance_threshold

    def get_memory_stats(self) -> Dict[str, Any]:
        """Get comprehensive memory statistics."""total_blocks = len(self.memory_blocks)
        avg_entropy = sum(b.attention_entropy for b in self.memory_blocks.values()) / total_blocks if total_blocks > 0 else 0
        avg_relevance = sum(b.relevance_score for b in self.memory_blocks.values()) / total_blocks if total_blocks > 0 else 0
        
        return {
            "total_memory_blocks": total_blocks,"            "average_attention_entropy": avg_entropy,"            "average_relevance_score": avg_relevance,"            "semantic_clusters": len(self.semantic_invalidator.semantic_clusters),"            "access_window_size": len(self.semantic_invalidator.access_window)"        }

    def prune_low_utility(self, cache_manager: Any):
        """Actively prunes low-utility items from a provided cache manager."""
# Concept: get all keys from cache_manager.kv_cache
        # keys = cache_manager.get_all_keys()
        # dead_keys = self.process_decay(keys)
        # for k in dead_keys: cache_manager.delete(k)
        pass
