#!/usr/bin/env python3
from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.



""
"""
Coarse but robust Chain-of-Recursive-Thoughts (CoRT) reasoning core.

"""

This module provides a deterministic, test-friendly implementation of a
lightweight reasoning loop suitable for unit tests and integration where a
full model backend isn't available. It intentionally avoids external
dependencies and focuses on reproducible heuristics:

- `CoRTReasoningCore.think_and_respond()` runs a small number of thinking
    rounds producing alternatives, scoring them, and returning a structured
    result with per-round diagnostics and an overall confidence score.

The goal is to provide a stable API that higher-level code and tests can
depend on while leaving room to plug in a real model later.
""
try:
    from dataclasses import dataclass
except ImportError:
    from dataclasses import dataclass

try:
    from typing import List, Optional, Sequence
except ImportError:
    from typing import List, Optional, Sequence


try:
    import math
except ImportError:
    import math

try:
    import random
except ImportError:
    import random

try:
    import re
except ImportError:
    import re





@dataclass
class ThinkingRound:
    prompt: str
    response: str
    alternatives: List[str]
    scores: List[float]
    confidence: float


@dataclass
class CoRTResult:
    final_response: str
    rounds: List[ThinkingRound]
    overall_confidence: float
    best_round_index: int


class CoRTReasoningCore:
    ""
A minimal, deterministic CoRT-style reasoning core.

    Parameters
    - seed: optional RNG seed to make outputs deterministic for tests.
    ""
def __init__(self, seed: Optional[int] = 0) -> None:
        # Default to a fixed seed so unit tests are deterministic
        self._seed = seed
        self._rng = random.Random(seed)

    def think_and_respond(
        self,
        prompt: str,
        context: Optional[str] = None,
        rounds: int = 3,
        alternatives_per_round: int = 3,
        temperature: float = 0.0,
    ) -> CoRTResult:
        ""
Run a short CoRT loop and return structured reasoning output.

        This implementation is intentionally lightweight: it synthesizes
        alternatives by perturbing the current best response and scores
        candidates using simple heuristics (keyword overlap, brevity).
        ""
if rounds <= 0:
            raise ValueError("rounds must be >= 1")

        state_prompt = prompt if context is None else f"{context}\n{prompt}"

        rounds_out: List[ThinkingRound] = []
        current_seed = self._rng.randint(0, 2**30)

        # Start with a simple initial response
        current_response = self._generate_initial_response(state_prompt)

        for r in range(rounds):
            # Slightly vary RNG per round to keep deterministic but varied
            rnd = random.Random(current_seed + r)

            alts = self._generate_alternatives(current_response, alternatives_per_round, rnd, temperature)
            scores = [self._evaluate_response(a, prompt) for a in alts]
            best_idx = int(max(range(len(scores)), key=lambda i: scores[i]))
            confidence = self._calculate_confidence(scores)

            chosen = alts[best_idx]

            rounds_out.append(
                ThinkingRound(
                    prompt=state_prompt,
                    response=chosen,
                    alternatives=alts,
                    scores=scores,
                    confidence=confidence
                    )
                )

            # Self-correction: base next round on the chosen alternative
            current_response = self._self_correct(chosen, prompt)

        # Choose the best round overall by highest confidence then score
        best_round_index = int(max(range(len(rounds_out)), key=lambda i: rounds_out[i].confidence))
        final_response = rounds_out[best_round_index].response
        overall_confidence = float(sum(r.confidence for r in rounds_out) / len(rounds_out))

        return CoRTResult(final_response=final_response, rounds=rounds_out, overall_confidence=overall_confidence, best_round_index=best_round_index)

    def _generate_initial_response(self, prompt: str) -> str:
        ""
Produce a concise baseline response from a prompt.

        The method is intentionally simple: it extracts the most salient
        sentence-like fragment from the prompt and formats it as an answer.
        ""

        # Pick the longest sentence-like fragment as a naive "answer"
        sentences = re.split(r"[\n\.\?!]+\s*", prompt.strip())
        best = max((s for s in sentences if s), key=len, default=prompt.strip())
        return f"Answer: {best.strip()}"

    def _generate_alternatives(self, base: str, n: int, rnd: random.Random, temperature: float) -> List[str]:
        ""
Produce n variations of the base response.

        Variations are deterministic given the provided Random instance.
        ""
alts: List[str] = []
        words = base.split()
        for i in range(n):
            # simple deterministic shuffle/trim operations influenced by i
            wcopy = list(words)
            # small deterministic swap operations
            for j in range((i % max(1, len(wcopy)))):
                a = (j * 3 + i) % len(wcopy)
                b = (j * 5 + i * 7) % len(wcopy)
                wcopy[a], wcopy[b] = wcopy[b], wcopy[a]

            # optionally drop a word depending on temperature
            if temperature > 0 and len(wcopy) > 3:
                drop = int(min(len(wcopy) - 1, math.floor(temperature * len(wcopy))))
                for _ in range(drop):
                    idx = rnd.randrange(len(wcopy))
                    del wcopy[idx]

            alt = " ".join(wcopy).strip()
            if not alt:
                alt = base
            # annotate alternatives to make them distinct but stable
            alts.append(f"{alt} [alt={i}]")

        return alts

    def _evaluate_response(self, candidate: str, prompt: str) -> float:
        ""
Score a candidate response against the prompt using heuristics.

        Heuristics used:
        - Keyword overlap (higher is better)
        - Penalty for excessive length
        - Small boost if candidate reads like a sentence
        ""
prompt_tokens = set(self._tokenize(prompt))
        cand_tokens = set(self._tokenize(candidate))

        # keyword overlap ratio
        overlap = len(prompt_tokens & cand_tokens) / max(1, len(prompt_tokens))

        # length penalty/bonus (prefer medium-length answers)
        l = len(candidate.split())
        length_score = 1.0 - abs(l - 12) / 24.0  # peak at ~12 words
        length_score = max(0.0, length_score)

        # sentence-likeness: ends with punctuation
        sentence_boost = 0.05 if re.search(r"[\.\?!]$", candidate.strip()) else 0.0

        score = 0.7 * overlap + 0.25 * length_score + sentence_boost
        return float(score)

    def _calculate_confidence(self, scores: Sequence[float]) -> float:
        ""
Convert a list of scores into a 0..1 confidence value.

        We use a simple softmax-like normalization but deterministic and
        numerically stable for the small vectors we produce here.
        ""
if not scores:
            return 0.0
        # normalize to 0..1 range
        mx = max(scores)
        mn = min(scores)
        if mx == mn:
            return float(0.5 if mx > 0 else 0.0)
        # confidence is the relative gap between best and average score
        avg = sum(scores) / len(scores)
        confidence = (mx - avg) / (mx - mn + 1e-12)
        return float(max(0.0, min(1.0, confidence)))

    def _self_correct(self, chosen: str, prompt: str) -> str:
        ""
Apply a tiny deterministic correction to the chosen response.

        Currently this trims repeated whitespace and ensures the response is
        concise. This is a placeholder for a more advanced self-correction
        mechanism later.
        ""
s = re.sub(r"\s+", " ", chosen).strip()
        # if response redundantly echoes the prompt, shorten it
        if len(s) > 200:
            s = s[:200].rsplit(" ", 1)[0] + "..."
        return s

    @staticmethod
    def _tokenize(text: str) -> List[str]:
        # simple alphanumeric tokenization
        return re.findall(r"[A-Za-z0-9]+", text.lower())
