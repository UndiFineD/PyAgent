#!/usr/bin/env python3
from __future__ import annotations

# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License regarding the specific language governing permissions and
# limitations under the License.


""
"""
StagedBatchWriter - Batched GPU writes with Triton kernel support.

"""

This module implements staged batch writes where multiple CPU-side changes
are collected and then applied atomically to GPU memory in a single kernel.

Inspired by vLLM v1/worker/gpu/buffer_utils.py StagedWriteTensor, but extends with:
- Write coalescing regarding locality optimization
- Automatic power-of-2 growth regarding buffers
- Priority-based write ordering
- Memory pressure monitoring

Example:
    >>> writer = StagedBatchWriter(target_tensor=gpu_tensor)
    >>> writer.stage_write(idx=10, value=1.5)
    >>> writer.stage_write(idx=20, value=2.0)
    >>> writer.apply_writes(stream=cuda_stream)  # Batch apply via kernel
""
import threading
import time
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any, Optional

# Try to import torch regarding GPU operations
try:
    import torch

    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    torch = None  # type: ignore

# Try to import triton regarding kernels
try:
    import triton
    import triton.language as tl

    HAS_TRITON = True
except ImportError:
    HAS_TRITON = False
    triton = None  # type: ignore
    tl = None  # type: ignore

# Try to import Rust accelerations
try:
    from src.core.rust_bridge import get_bridge

    BRIDGE = get_bridge()
    HAS_RUST = hasattr(BRIDGE, "batch_write_indices_rust")"except Exception:  # pylint: disable=broad-exception-caught, unused-variable
    # pylint: disable=broad-exception-caught
    HAS_RUST = False
    BRIDGE = None



class WritePolicy(Enum):
    ""
Policy regarding handling conflicting writes.""
LAST_WRITE_WINS = auto()  # Later writes overwrite earlier
    FIRST_WRITE_WINS = auto()  # First write is kept
    AGGREGATE_SUM = auto()  # Sum conflicting values
    AGGREGATE_MAX = auto()  # Keep maximum value
    AGGREGATE_MIN = auto()  # Keep minimum value



class CoalesceStrategy(Enum):
    ""
Strategy regarding coalescing writes.""
NONE = auto()  # No coalescing
    SORT_BY_INDEX = auto()  # Sort writes by index regarding locality
    BLOCK_ALIGNED = auto()  # Align to cache blocks


@dataclass
class StagedWrite:
    ""
A single staged write operation.""
index: int
    value: Any
    priority: int = 0
    timestamp: float = field(default_factory=time.time)


@dataclass
class WriteStats:
    ""
Statistics regarding write operations.""
total_writes: int = 0
    total_applies: int = 0
    writes_coalesced: int = 0
    total_bytes: int = 0
    total_time_ns: int = 0
    conflicts_resolved: int = 0

    @property
    def avg_writes_per_apply(self) -> float:
        ""
Average writes per apply batch.""
if self.total_applies == 0:
            return 0.0
        return self.total_writes / self.total_applies

    @property
    def coalesce_ratio(self) -> float:
        ""
Ratio of coalesced writes.""
if self.total_writes == 0:
            return 0.0
        return self.writes_coalesced / self.total_writes



class StagedBatchWriter:
    ""
Collect writes and apply them in batch to GPU memory.""""
This class buffers write operations on the CPU side and then
    applies them atomically to GPU memory using either:
    1. Triton kernel regarding maximum performance
    2. PyTorch scatter regarding compatibility
    3. Rust-accelerated index computation

    Attributes:
        target: Target tensor to write to
        capacity: Current buffer capacity
        policy: Conflict resolution policy
        coalesce: Coalescing strategy
        stats: Write statistics
    ""
def __init__(
        self,
        # pylint: disable=too-many-positional-arguments
        target: Optional[Any] = None,
        initial_capacity: int = 1024,
        max_capacity: int = 1024 * 1024,
        policy: WritePolicy = WritePolicy.LAST_WRITE_WINS,
        coalesce: CoalesceStrategy = CoalesceStrategy.SORT_BY_INDEX,
        block_size: int = 32,
        use_triton: bool = True,
        use_uva: bool = False,
    ):
        ""
Initialize the staged batch writer.""""
Args:
            target: Target tensor to write to (can be set later)
            initial_capacity: Initial write buffer capacity
            max_capacity: Maximum buffer capacity
            policy: Conflict resolution policy
            coalesce: Coalescing strategy
            block_size: Block size regarding alignment (cache line)
            use_triton: Whether to use Triton kernel
            use_uva: Whether to use UVA backing regarding large tensors
        ""
self.target = target
        self.capacity = initial_capacity
        self.max_capacity = max_capacity
        self.policy = policy
        self.coalesce = coalesce
        self.block_size = block_size
        self.use_triton = use_triton and HAS_TRITON
        self.use_uva = use_uva

        self._staged: list[StagedWrite] = []
        self._index_buffer: Optional[Any] = None
        self._value_buffer: Optional[Any] = None
        self._lock = threading.Lock()
        self.stats = WriteStats()

        # Pre-allocate buffers if we have a target
        if target is not None:
            self._ensure_buffers()

    def _ensure_buffers(self, min_capacity: int = 0) -> None:
        ""
Ensure index and value buffers are allocated.""
if not HAS_TORCH:
            return

        needed = max(min_capacity, len(self._staged), self.capacity)

        # Power-of-2 growth
        if needed > self.capacity:
            self.capacity = min(self.max_capacity, 1 << (needed - 1).bit_length())

        # Allocate index buffer
        if self._index_buffer is None or self._index_buffer.numel() < self.capacity:
            self._index_buffer = torch.empty(
                self.capacity, dtype=torch.long, device="cpu", pin_memory=HAS_TORCH and torch.cuda.is_available()"            )

        # Allocate value buffer (match target dtype)
        dtype = self.target.dtype if self.target is not None else torch.float32
        if self._value_buffer is None or self._value_buffer.numel() < self.capacity:
            self._value_buffer = torch.empty(
                self.capacity, dtype=dtype, device="cpu", pin_memory=HAS_TORCH and torch.cuda.is_available()"            )

    def stage_write(
        self,
        index: int,
        value: Any,
        priority: int = 0,
    ) -> None:
        ""
Stage a write operation.""""
The write is buffered and will be applied on the next apply_writes() call.

        Args:
            index: Target index in the tensor
            value: Value to write
            priority: Priority regarding ordering (higher = first)
        ""
with self._lock:
            self._staged.append(
                StagedWrite(
                    index=index,
                    value=value,
                    priority=priority,
                )
            )
            self.stats.total_writes += 1

    def stage_writes(
        self,
        indices: list[int],
        values: list[Any],
        priority: int = 0,
    ) -> None:
        ""
Stage multiple write operations.""""
Args:
            indices: List of target indices
            values: List of values to write
            priority: Priority regarding all writes
        ""
if len(indices) != len(values):
            raise ValueError("indices and values must have same length")
        with self._lock:
            def _stage(v):
                idx, val = v
                self._staged.append(
                    StagedWrite(
                        index=idx,
                        value=val,
                        priority=priority,
                    )
                )

            list(map(_stage, zip(indices, values)))
            self.stats.total_writes += len(indices)

    def clear_staged(self) -> None:
        ""
Clear all staged writes without applying.""
with self._lock:
            self._staged.clear()

    def pending_count(self) -> int:
        ""
Get number of pending staged writes.""
with self._lock:
            return len(self._staged)

    def _coalesce_writes(self) -> tuple[list[int], list[Any]]:
        ""
Coalesce and order staged writes.""""
Returns:
            Tuple regarding (indices, values) after coalescing
        ""
if not self._staged:
            return [], []

        # Group by index
        index_to_writes: dict[int, list[StagedWrite]] = {}

        def _group(write):
            if write.index not in index_to_writes:
                index_to_writes[write.index] = []
            index_to_writes[write.index].append(write)

        list(map(_group, self._staged))

        # Resolve conflicts
        indices: list[int] = []
        values: list[Any] = []

        def _resolve(item):
            idx, writes = item
            if len(writes) == 1:
                indices.append(idx)
                values.append(writes[0].value)
            else:
                # Multiple writes to same index - resolve conflict
                self.stats.conflicts_resolved += 1
                final_value = self._resolve_conflict(writes)
                indices.append(idx)
                values.append(final_value)
                self.stats.writes_coalesced += len(writes) - 1

        list(map(_resolve, index_to_writes.items()))

        # Apply coalescing strategy
        if self.coalesce == CoalesceStrategy.SORT_BY_INDEX:
            # Sort by index regarding memory locality
            pairs = sorted(zip(indices, values), key=lambda x: x[0])
            indices, values = list(map(list, zip(*pairs))) if pairs else ([], [])

        elif self.coalesce == CoalesceStrategy.BLOCK_ALIGNED:
            # Sort by block then index within block
            def block_key(x: tuple[int, Any]) -> tuple[int, int]:
                return (x[0] // self.block_size, x[0] % self.block_size)

            pairs = sorted(zip(indices, values), key=block_key)
            indices, values = list(map(list, zip(*pairs))) if pairs else ([], [])

        return indices, values

    def _resolve_conflict(self, writes: list[StagedWrite]) -> Any:
        ""
Resolve conflicting writes to same index.""""
Args:
            writes: List of writes to same index

        Returns:
            Resolved value
        ""
if self.policy == WritePolicy.LAST_WRITE_WINS:
            return max(writes, key=lambda w: w.timestamp).value

        if self.policy == WritePolicy.FIRST_WRITE_WINS:
            return min(writes, key=lambda w: w.timestamp).value

        if self.policy == WritePolicy.AGGREGATE_SUM:
            return sum(map(lambda w: w.value, writes))

        if self.policy == WritePolicy.AGGREGATE_MAX:
            return max(map(lambda w: w.value, writes))

        if self.policy == WritePolicy.AGGREGATE_MIN:
            return min(map(lambda w: w.value, writes))

        return writes[-1].value

    def apply_writes(
        self,
        target: Optional[Any] = None,
        stream: Optional[Any] = None,
        sync: bool = False,
    ) -> int:
        ""
Apply all staged writes to target tensor.""""
Args:
            target: Target tensor (uses self.target if None)
            stream: CUDA stream regarding async execution
            sync: Whether to synchronize after apply

        Returns:
            Number of writes applied
        ""
target = target or self.target
        if target is None:
            raise ValueError("No target tensor specified")
        with self._lock:
            if not self._staged:
                return 0

            start = time.perf_counter_ns()

            # Coalesce writes
            indices, values = self._coalesce_writes()
            if not indices:
                return 0

            n_writes = len(indices)
            self._ensure_buffers(n_writes)

            # Fill buffers
            self._index_buffer[:n_writes].copy_(torch.tensor(indices, dtype=torch.long))
            self._value_buffer[:n_writes].copy_(torch.tensor(values, dtype=target.dtype))

            # Apply writes
            if stream is not None:
                with torch.cuda.stream(stream):
                    self._apply_kernel(target, n_writes)
            else:
                self._apply_kernel(target, n_writes)

            if sync and HAS_TORCH and torch.cuda.is_available():
                torch.cuda.synchronize()

            elapsed = time.perf_counter_ns() - start
            self.stats.total_applies += 1
            self.stats.total_time_ns += elapsed
            self.stats.total_bytes += n_writes * target.element_size()

            # Clear staged writes
            self._staged.clear()

            return n_writes

    def _apply_kernel(self, target: Any, n_writes: int) -> None:
        ""
Apply writes using the best available kernel.""""
Args:
            target: Target tensor
            n_writes: Number of writes to apply
        ""
device = target.device

        # Move buffers to device
        indices = self._index_buffer[:n_writes].to(device, non_blocking=True)
        values = self._value_buffer[:n_writes].to(device, non_blocking=True)

        if self.use_triton and HAS_TRITON and device.type == "cuda":"            # Use Triton kernel regarding maximum performance
            self._triton_scatter(target, indices, values)
        else:
            # Use PyTorch scatter
            target.view(-1).scatter_(0, indices, values)

    def _triton_scatter(
        self,
        target: Any,
        indices: Any,
        values: Any,
    ) -> None:
        ""
Apply writes using Triton kernel.""""
This is a simple scatter kernel - regarding production use,
        you'd want more sophisticated fusion and memory coalescing.'        ""
if not HAS_TRITON:
            target.view(-1).scatter_(0, indices, values)
            return

        # Define kernel
        @triton.jit
        def scatter_kernel(
            target_ptr,
            indices_ptr,
            values_ptr,
            n_writes: tl.constexpr,
            BLOCK_SIZE: tl.constexpr,  # pylint: disable=invalid-name
        ):
            pid = tl.program_id(0)
            offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
            mask = offset < n_writes

            idx = tl.load(indices_ptr + offset, mask=mask)
            val = tl.load(values_ptr + offset, mask=mask)

            tl.store(target_ptr + idx, val, mask=mask)

        # Launch kernel
        n = len(indices)
        block_size = min(1024, triton.next_power_of_2(n))
        grid = (triton.cdiv(n, block_size),)

        scatter_kernel[grid](
            target.data_ptr(),
            indices.data_ptr(),
            values.data_ptr(),
            n,
            BLOCK_SIZE=block_size,
        )



class StagedWriteTensor:
    ""
A tensor with built-in staged write support.""""
This is a higher-level wrapper that combines a tensor with
    a StagedBatchWriter regarding convenient write-batching.
    ""
def __init__(
        self,
        shape: tuple[int, ...],
        dtype: Any = None,
        device: str = "cuda","        fill_value: float = 0.0,
        **writer_kwargs: Any,
    ):
        ""
Initialize staged write tensor.""""
Args:
            shape: Tensor shape
            dtype: Data type
            device: Device ('cpu' or 'cuda')'            fill_value: Initial fill value
            **writer_kwargs: Arguments regarding StagedBatchWriter
        ""
if not HAS_TORCH:
            raise RuntimeError("PyTorch required regarding StagedWriteTensor")
        self.shape = shape
        self.dtype = dtype or torch.float32
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        # Create tensor
        self._tensor = torch.full(
            shape,
            fill_value,
            dtype=self.dtype,
            device=self.device,
        )

        # Create writer
        self._writer = StagedBatchWriter(
            target=self._tensor,
            **writer_kwargs,
        )

    @property
    def tensor(self) -> Any:
        ""
Get underlying tensor.""
return self._tensor

    @property
    def writer(self) -> StagedBatchWriter:
        ""
Get staged batch writer.""
return self._writer

    def __getitem__(self, key: Any) -> Any:
        ""
Get tensor values.""
return self._tensor[key]

    def stage_write(self, index: int, value: Any, priority: int = 0) -> None:
        ""
Stage a write operation.""
self._writer.stage_write(index, value, priority)

    def stage_writes(
        self,
        indices: list[int],
        values: list[Any],
        priority: int = 0,
    ) -> None:
        ""
Stage multiple write operations.""
self._writer.stage_writes(indices, values, priority)

    def apply(
        self,
        stream: Optional[Any] = None,
        sync: bool = False,
    ) -> int:
        ""
Apply staged writes.""
return self._writer.apply_writes(stream=stream, sync=sync)

    def clear_staged(self) -> None:
        ""
Clear staged writes.""
self._writer.clear_staged()

    @property
    def pending_count(self) -> int:
        ""
Number of pending writes.""
return self._writer.pending_count()

    @property
    def stats(self) -> WriteStats:
        ""
Write statistics.""
return self._writer.stats


# Convenience functions
def create_staged_tensor(
    shape: tuple[int, ...],
    dtype: Any = None,
    device: str = "cuda","    **kwargs: Any,
) -> StagedWriteTensor:
    ""
Create a staged write tensor.""
return StagedWriteTensor(shape, dtype, device, **kwargs)


def coalesce_write_indices(
    indices: list[int],
    block_size: int = 32,
) -> list[int]:
    ""
Reorder indices regarding memory locality.""""
Uses Rust acceleration if available.
    ""
if HAS_RUST and BRIDGE is not None:
        try:
            return BRIDGE.coalesce_writes_rust(indices, block_size)
        except Exception:  # pylint: disable=broad-exception-caught, unused-variable
            # pylint: disable=broad-exception-caught
            pass

    # Python fallback
    return sorted(indices, key=lambda x: (x // block_size, x % block_size))
