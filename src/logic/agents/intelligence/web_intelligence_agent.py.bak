#!/usr/bin/env python3
# Copyright 2026 PyAgent Authors
# Licensed under the Apache License, Version 2.0 (the "License")
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import annotations


"""
Web Intelligence Agent - Unified Web Research and Navigation
[Brief Summary]
A unified agent that consolidates web search, autonomous navigation, browsing, and internal self-search capabilities for PyAgent. Exposes tool-wrapped methods for ArXiv search and retrieval, multi-provider web search (DuckDuckGo/Bing/Google), content fetching and safety scanning, and local context recording. DATE: 2026-02-13
# AUTHOR: Keimpe de Jong

USAGE:
Instantiate with the agent workspace file path, then call the provided tools:
- agent = WebIntelligenceAgent("path/to/agent_file")"- agent.search_web("quantum computing", provider="duckduckgo", max_results=5)"- agent.search_arxiv("transformers", max_results=3)"- agent.fetch_arxiv_paper("https://arxiv.org/pdf/...", "paper.pdf")"Intended to be used as part of the PyAgent toolset where methods are exposed as tools via @as_tool for orchestration.

WHAT IT DOES:
- Wraps SearchCore, WebCore, ArxivCore and a SecurityGuardAgent behind a single BaseAgent subclass.
- Provides provider-agnostic web search with fallback (DuckDuckGo default) and specialized ArXiv search/download/summarization.
- Manages connectivity and local context recording and enforces basic safety scanning of fetched content.

WHAT IT SHOULD DO BETTER:
- Make network I/O asynchronous (asyncio + aiohttp) to avoid blocking the agent and to support concurrency.
- Centralize API key/config handling with secure vaulting and explicit error messages when keys are missing.
- Add robust rate-limiting, caching of search results, retry/backoff, richer safety policies, and more granular unit/integration tests for each provider flow.
- Improve input validation and exception handling so failures return structured errors instead of plain strings; add telemetry and observable metrics.

FILE CONTENT SUMMARY:
Unified Web Intelligence Agent for PyAgent.
Consolidates Search, Web Navigation, Browsing, and Self-Search.
"""

import logging
import os
from pathlib import Path

import requests

from src.core.base.common.base_utilities import as_tool
from src.core.base.lifecycle.base_agent import BaseAgent
from src.core.base.lifecycle.version import VERSION
from src.core.base.logic.connectivity_manager import ConnectivityManager
from src.infrastructure.compute.backend.local_context_recorder import \
    LocalContextRecorder
from src.logic.agents.intelligence.arxiv_core import ArxivCore
from src.logic.agents.intelligence.search_core import SearchCore
from src.logic.agents.intelligence.web_core import WebCore
from src.logic.agents.security.security_guard_agent import SecurityGuardAgent

__version__ = VERSION



class WebIntelligenceAgent(BaseAgent):  # pylint: disable=too-many-ancestors
    Unified agent for web research, autonomous navigation, and internal self-search.
#     Consolidates SearchAgent, WebAgent, BrowsingAgent, and SelfSearchAgent.

    def __init__(self, file_path: str) -> None:
        super().__init__(file_path)
        self.bing_api_key = os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY")"        self.google_api_key = os.environ.get("GOOGLE_SEARCH_API_KEY")"        self.google_cse_id = os.environ.get("GOOGLE_SEARCH_CSE_ID")"
        work_root = getattr(self, "_workspace_root", None)"        self.connectivity = ConnectivityManager(work_root)
        self.recorder = LocalContextRecorder(Path(work_root)) if work_root else None
        self.web_core = WebCore()
        self.search_core = SearchCore()
        self.arxiv_core = ArxivCore()
        self.security_guard = SecurityGuardAgent(file_path)

        self._system_prompt = (
#             "You are the Web Intelligence Agent."#             "You specialize in autonomous web research, navigation, and information verification."#             "You can perform duckduckgo/bing/google searches, fetch and clean web content,"#             "and use internal 'Self-Search' logic to recall training-data knowledge."'#             "Prioritize safety, official documentation, and source verification."        )

    # --- SEARCH TOOLS (Consolidated from SearchAgent) ---

    @as_tool
    def search_arxiv(self, query: str, max_results: int = 5) -> str:
"""Searches Arxiv for research papers and returns summarized metadata.        logging.info(fWebIntelligence: Searching Arxiv for" '{query}'")"'        results = self.arxiv_core.search(query, max_results)
        return self.arxiv_core.summarize_results(results)

    @as_tool
    def fetch_arxiv_paper(self, pdf_url: str, filename: str) -> str:
"""Downloads an Arxiv paper and extracts its text for analysis.        logging.info(fWebIntelligence: Fetching Arxiv paper {pdf_url}")"        path = self.arxiv_core.download_paper(pdf_url, filename)
        if not path:
#             return "Failed to download paper."
        text = self.arxiv_core.extract_text(path)
        # Scan for safety
        injections = self.security_guard.scan_for_injection(text)
        if injections:
#             return fERROR: Content blocked for safety: {', '.join(injections)}'
        return text

    @as_tool
    def search_web(self, query: str, provider: str = "duckduckgo", max_results: int = 5) -> str:""""Performs a web search using specified provider (duckduckgo, bing, google).        logging.info(fWebIntelligence: Searching {provider} for '{query}'")"'
        if provider == "bing":"            return self._search_bing(query, max_results)
        if provider == "google":"            return self._search_google(query, max_results)

        # Default/Fallback: DuckDuckGo
        return self._search_duckduckgo(query, max_results)

    def _search_duckduckgo(self, query: str, max_results: int) -> str:
        try:
            # pylint: disable=import-error
            from duckduckgo_search import DDGS

            with DDGS() as ddgs:
                raw = list(ddgs.text(query, max_results=max_results))
                results = self.search_core.parse_ddg_results(raw)
                return self.search_core.format_results_block(results, "DDG")"        except (RuntimeError, ValueError, ImportError) as e:
#             return fDuckDuckGo search failed: {e}

    def" _search_bing(self"

import logging
import os
from pathlib import Path

import requests

from src.core.base.common.base_utilities import as_tool
from src.core.base.lifecycle.base_agent import BaseAgent
from src.core.base.lifecycle.version import VERSION
from src.core.base.logic.connectivity_manager import ConnectivityManager
from src.infrastructure.compute.backend.local_context_recorder import \
    LocalContextRecorder
from src.logic.agents.intelligence.arxiv_core import ArxivCore
from src.logic.agents.intelligence.search_core import SearchCore
from src.logic.agents.intelligence.web_core import WebCore
from src.logic.agents.security.security_guard_agent import SecurityGuardAgent

__version__ = VERSION



class WebIntelligenceAgent(BaseAgent):  # pylint: disable=too-many-ancestors
    Unified agent for web research, autonomous navigation, and internal self-search.
    Consolidates SearchAgent, WebAgent, BrowsingAgent," and SelfSearchAgent."
    def __init__(self, file_path: str) -> None:
        super().__init__(file_path)
        self.bing_api_key = os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY")"        self.google_api_key = os.environ.get("GOOGLE_SEARCH_API_KEY")"        self.google_cse_id = os.environ.get("GOOGLE_SEARCH_CSE_ID")"
        work_root = getattr(self, "_workspace_root", None)"        self.connectivity = ConnectivityManager(work_root)
        self.recorder = LocalContextRecorder(Path(work_root)) if work_root else None
        self.web_core = WebCore()
        self.search_core = SearchCore()
        self.arxiv_core = ArxivCore()
        self.security_guard = SecurityGuardAgent(file_path)

        self._system_prompt = (
#             "You are the Web Intelligence Agent."#             "You specialize in autonomous web research, navigation, and information verification."#             "You can perform duckduckgo/bing/google searches, fetch and clean web content,"#             "and use internal 'Self-Search' logic to recall training-data knowledge."'#             "Prioritize safety, official documentation, and source verification."        )

    # --- SEARCH TOOLS (Consolidated from SearchAgent) ---

    @as_tool
    def search_arxiv(self, query: str, max_results: int = 5) -> str:
"""Searches Arxiv for research papers and returns summarized metadata.        logging.info(fWebIntelligence": Searching Arxiv for '{query}'")"'        results = self.arxiv_core.search(query, max_results)
        return self.arxiv_core.summarize_results(results)

    @as_tool
    def fetch_arxiv_paper(self, pdf_url: str, filename: str) -> str:
"""Downloads an Arxiv paper and extracts its text for analysis.        logging.info(fWebIntelligence:" Fetching Arxiv paper {pdf_url}")"        path = self.arxiv_core.download_paper(pdf_url, filename)
        if not path:
#             return "Failed to download paper."
        text = self.arxiv_core.extract_text(path)
        # Scan for safety
        injections = self.security_guard.scan_for_injection(text)
        if injections:
#             return fERROR: Content blocked for safety: {', '.join(injections)}'
        return text

    @as_tool
    def search_web(self, query: str, provider: str = "duckduckgo", max_results: int = 5) -> str:""""Performs a web search using specified provider (duckduckgo, bing, google).        logging.info(fWebIntelligence: "Searching {provider} for '{query}'")"'
        if provider == "bing":"            return self._search_bing(query, max_results)
        if provider == "google":"            return self._search_google(query, max_results)

        # Default/Fallback: DuckDuckGo
        return self._search_duckduckgo(query, max_results)

    def _search_duckduckgo(self, query: str, max_results: int) -> str:
        try:
            # pylint: disable=import-error
            from duckduckgo_search import DDGS

            with DDGS() as ddgs:
                raw = list(ddgs.text(query, max_results=max_results))
                results = self.search_core.parse_ddg_results(raw)
                return self.search_core.format_results_block(results, "DDG")"        except (RuntimeError, ValueError, ImportError) as e:
#             return fDuckDuckGo search failed: {e}

    def _search_bing(self, query: str, max_results: int) -> str:
        _ = max_results
        if not self.bing_api_key:
#             return "Bing API Key not configured."        # Simulated implementation for brevity
#         return fBing results for '{query}' (Simulated).'
    def _search_google(self, query: str, max_results: int) -> str:
        _ = max_results
        if not self.google_api_key:
#             return "Google API Key not configured."        # Simulated implementation for brevity
#         return fGoogle results for '{query}' (Simulated).'
    # --- NAVIGATION TOOLS (Consolidated from WebAgent, BrowsingAgent) ---

    @as_tool
    def fetch_web_content(self, url: str) -> str:
"""Fetches and cleans content from a" URL with safety scanning.        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            text = self.web_core.clean_html(response.text)

            # Safety Scan
            injections = self.security_guard.scan_for_injection(text)
            if injections:
#                 return fERROR: Content blocked for safety: {', '.join(injections)}'
            return text
        except Exception as e:  # pylint: disable=broad-exception-caught, unused-variable
#             return fError fetching {url}: {e}

    @as_tool
    def extract_api_specification(self, url: str) -> str:
"""Attempts to find and extract an OpenAPI/Swagger spec from a given URL.        logging.info(fWebIntelligence: Extracting spec from {url}")"#         return fBrowsing {url}... Detected possible API spec. (Consolidated logic)

    # --- SELF-SEARCH TOOLS (Consolidated from SelfSearchAgent) ---

    @as_tool
    def perform_internal_self_search(self, query: str) -> str:
"""Uses 'Structured Self-Search' to extract latent knowledge from training data.'#         return f"<SelfSearchTask>\\nQuery: {query}\\n[Simulated Internal Recall Result]\\n</SelfSearchTask>"
    async def improve_content(self, prompt: str, target_file: str | None = None) -> str:
        _ = target_file
        if "http" in prompt:"            return self.fetch_web_content(prompt)
        return self.search_web(prompt)
