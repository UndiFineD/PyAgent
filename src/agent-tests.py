#!/usr / bin / env python3
# Copyright (c) 2025 DebVisor contributors
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org / licenses / LICENSE - 2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Tests Agent: Improves and updates code file test suites.

Reads a tests file (test_Codefile.py), uses Copilot to enhance the tests,
and updates the tests file with improvements.

# Description
This module provides a Tests Agent that reads existing code file test suites,
uses AI assistance to improve and complete them, ensuring each line of the codefile is tested,
and updates the tests files with enhanced test coverage.

# Changelog
- 1.0.0: Initial implementation
- 1.1.0: Added test prioritization, flakiness detection, coverage analysis

# Suggested Fixes
- Add validation for tests file format
- Improve prompt engineering for better test generation

# Improvements
- Better integration with other agents
- Enhanced diff reporting
"""

from __future__ import annotations
import ast
import hashlib
import json
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set, Tuple
from base_agent import BaseAgent, create_main_function


class TestPriority(Enum):
    """Test priority levels."""
    CRITICAL = 5
    HIGH = 4
    MEDIUM = 3
    LOW = 2
    SKIP = 1


class TestStatus(Enum):
    """Test execution status."""
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"
    FLAKY = "flaky"


class CoverageType(Enum):
    """Types of coverage to track."""
    LINE = "line"
    BRANCH = "branch"
    FUNCTION = "function"
    CLASS = "class"


# ========== Session 7 Enums ==========


class BrowserType(Enum):
    """Browser types for cross-browser testing."""
    CHROME = "chrome"
    FIREFOX = "firefox"
    SAFARI = "safari"
    EDGE = "edge"
    IE = "ie"


class TestSourceType(Enum):
    """Types of test result sources for aggregation."""
    PYTEST = "pytest"
    UNITTEST = "unittest"
    JEST = "jest"
    MOCHA = "mocha"
    JUNIT = "junit"


class MutationOperator(Enum):
    """Mutation operators for mutation testing."""
    ARITHMETIC = "arithmetic"
    RELATIONAL = "relational"
    LOGICAL = "logical"
    ASSIGNMENT = "assignment"
    RETURN_VALUE = "return_value"


class ExecutionMode(Enum):
    """Test execution replay modes."""
    STEP_BY_STEP = "step_by_step"
    FULL_REPLAY = "full_replay"
    BREAKPOINT = "breakpoint"


@dataclass
class TestCase:
    """Represents a single test case."""
    id: str
    name: str
    file_path: str
    line_number: int
    priority: TestPriority = TestPriority.MEDIUM
    status: TestStatus = TestStatus.PASSED
    duration_ms: float = 0.0
    flakiness_score: float = 0.0
    last_run: str = ""
    run_count: int = 0
    failure_count: int = 0
    tags: List[str] = field(default_factory=lambda: [])
    dependencies: List[str] = field(default_factory=lambda: [])


@dataclass
class TestRun:
    """A test execution run."""
    id: str
    timestamp: str
    total_tests: int = 0
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    errors: int = 0
    duration_ms: float = 0.0
    test_results: Dict[str, TestStatus] = field(default_factory=lambda: {})


@dataclass
class CoverageGap:
    """Represents a gap in test coverage."""
    file_path: str
    line_start: int
    line_end: int
    coverage_type: CoverageType
    suggestion: str = ""


@dataclass
class TestFactory:
    """A test data factory for generating test data."""
    name: str
    return_type: str
    parameters: Dict[str, str] = field(default_factory=lambda: {})
    generator: str = ""  # Code snippet or function name


# ========== Session 7 Dataclasses ==========


@dataclass
class VisualRegressionConfig:
    """Configuration for visual regression testing."""
    baseline_dir: str
    diff_threshold: float = 0.01
    browsers: List[BrowserType] = field(default_factory=lambda: [BrowserType.CHROME])
    viewport_sizes: List[Tuple[int, int]] = field(default_factory=lambda: [(1920, 1080)])
    ignore_regions: List[Tuple[int, int, int, int]] = field(default_factory=lambda: [])


@dataclass
class ContractTest:
    """A contract test for API boundaries."""
    consumer: str
    provider: str
    endpoint: str
    request_schema: Dict[str, Any] = field(default_factory=lambda: {})
    response_schema: Dict[str, Any] = field(default_factory=lambda: {})
    status_code: int = 200


@dataclass
class TestEnvironment:
    """Test environment configuration."""
    name: str
    base_url: str = ""
    variables: Dict[str, str] = field(default_factory=lambda: {})
    fixtures: List[str] = field(default_factory=lambda: [])
    setup_commands: List[str] = field(default_factory=lambda: [])
    teardown_commands: List[str] = field(default_factory=lambda: [])


@dataclass
class ExecutionTrace:
    """Test execution trace for replay."""
    test_id: str
    timestamp: str
    steps: List[Dict[str, Any]] = field(default_factory=lambda: [])
    variables: Dict[str, Any] = field(default_factory=lambda: {})
    stdout: str = ""
    stderr: str = ""


@dataclass
class TestDependency:
    """A dependency for test injection."""
    name: str
    dependency_type: str
    implementation: str = ""
    mock_behavior: str = ""


@dataclass
class CrossBrowserConfig:
    """Cross-browser testing configuration."""
    browsers: List[BrowserType]
    parallel: bool = True
    headless: bool = True
    timeout_seconds: int = 30
    retries: int = 1


@dataclass
class AggregatedResult:
    """Aggregated test result from multiple sources."""
    source: TestSourceType
    test_name: str
    status: TestStatus
    duration_ms: float
    timestamp: str
    metadata: Dict[str, Any] = field(default_factory=lambda: {})


@dataclass
class Mutation:
    """A code mutation for mutation testing."""
    id: str
    file_path: str
    line_number: int
    operator: MutationOperator
    original_code: str
    mutated_code: str
    killed: bool = False


@dataclass
class GeneratedTest:
    """A test generated from specification."""
    name: str
    specification: str
    generated_code: str
    confidence: float = 0.0
    validated: bool = False


@dataclass
class TestProfile:
    """Runtime profiling data for a test."""
    test_id: str
    cpu_time_ms: float
    memory_peak_mb: float
    io_operations: int
    function_calls: int
    timestamp: str = ""


@dataclass
class ScheduleSlot:
    """A scheduled time slot for test execution."""
    start_time: str
    end_time: str
    tests: List[str] = field(default_factory=lambda: [])
    workers: int = 1
    priority: TestPriority = TestPriority.MEDIUM


# ========== Session 7 Helper Classes ==========
class VisualRegressionTester:
    """Visual regression testing for UI components.

    Provides capabilities for visual comparison testing
    of UI components across browsers and viewport sizes.

    Attributes:
        config: Visual regression configuration.
        baselines: Stored baseline images.
        results: Test results.
    """

    def __init__(self, config: VisualRegressionConfig) -> None:
        """Initialize visual regression tester.

        Args:
            config: The configuration to use.
        """
        self.config = config
        self.baselines: Dict[str, str] = {}  # component_id -> hash
        self.results: List[Dict[str, Any]] = []
        self._diffs: Dict[str, float] = {}

    def capture_baseline(
        self,
        component_id: str,
        screenshot_path: str
    ) -> str:
        """Capture a baseline screenshot.

        Args:
            component_id: Unique identifier for the component.
            screenshot_path: Path to save the screenshot.

        Returns:
            Hash of the captured image.
        """
        # Simulated capture
        image_hash = hashlib.md5(
            f"{component_id}:{screenshot_path}".encode()
        ).hexdigest()
        self.baselines[component_id] = image_hash
        return image_hash

    def compare(
        self,
        component_id: str,
        current_screenshot: str
    ) -> Dict[str, Any]:
        """Compare current screenshot against baseline.

        Args:
            component_id: The component identifier.
            current_screenshot: Path to current screenshot.

        Returns:
            Comparison result with diff percentage.
        """
        baseline = self.baselines.get(component_id)
        if not baseline:
            return {"error": "No baseline found", "passed": False}  # type: ignore

        # Simulated comparison
        current_hash = hashlib.md5(current_screenshot.encode()).hexdigest()
        diff = 0.0 if current_hash == baseline else 0.05  # Simulated diff
        self._diffs[component_id] = diff
        passed = diff <= self.config.diff_threshold
        result: Dict[str, Any] = {
            "component_id": component_id,
            "diff_percentage": diff,
            "threshold": self.config.diff_threshold,
            "passed": passed
        }
        self.results.append(result)
        return result

    def generate_diff_report(self) -> str:
        """Generate visual diff report.

        Returns:
            Markdown report of visual differences.
        """
        report = ["# Visual Regression Report\n"]
        report.append(f"Threshold: {self.config.diff_threshold * 100}%\n")
        passed = [r for r in self.results if r.get("passed")]
        failed = [r for r in self.results if not r.get("passed")]
        report.append(f"## Summary: {len(passed)} passed, {len(failed)} failed\n")
        if failed:
            report.append("## Failed Components\n")
            for r in failed:
                report.append(
                    f"- **{r['component_id']}**: {r['diff_percentage'] * 100:.2f}% diff"
                )
        return "\n".join(report)

    def run_for_browsers(self, component_id: str) -> List[Dict[str, Any]]:
        """Run visual test across all configured browsers.

        Args:
            component_id: The component to test.

        Returns:
            Results for each browser.
        """
        results: List[Dict[str, Any]] = []
        for browser in self.config.browsers:
            result: Dict[str, Any] = {
                "browser": browser.value,
                "component_id": component_id,
                "passed": True  # Simulated
            }
            results.append(result)
        return results


class ContractTestRunner:
    """Contract testing for API boundaries.

    Provides contract testing capabilities for validating
    API boundaries between consumers and providers.

    Attributes:
        contracts: Registered contracts.
        results: Test results.
    """

    def __init__(self) -> None:
        """Initialize contract test runner."""
        self.contracts: Dict[str, ContractTest] = {}
        self.results: List[Dict[str, Any]] = []

    def add_contract(
        self,
        consumer: str,
        provider: str,
        endpoint: str,
        request_schema: Optional[Dict[str, Any]] = None,
        response_schema: Optional[Dict[str, Any]] = None,
        status_code: int = 200
    ) -> ContractTest:
        """Add a contract definition.

        Args:
            consumer: The consuming service name.
            provider: The providing service name.
            endpoint: The API endpoint.
            request_schema: Expected request schema.
            response_schema: Expected response schema.
            status_code: Expected status code.

        Returns:
            The created contract.
        """
        contract_id = f"{consumer}:{provider}:{endpoint}"
        contract = ContractTest(
            consumer=consumer,
            provider=provider,
            endpoint=endpoint,
            request_schema=request_schema or {},
            response_schema=response_schema or {},
            status_code=status_code
        )
        self.contracts[contract_id] = contract
        return contract

    def verify_consumer(
        self,
        contract_id: str,
        actual_request: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Verify consumer sends correct request.

        Args:
            contract_id: The contract identifier.
            actual_request: The actual request data.

        Returns:
            Verification result.
        """
        contract = self.contracts.get(contract_id)
        if not contract:
            return {"error": "Contract not found", "valid": False}  # type: ignore

        # Simple schema validation (in real impl use jsonschema)
        valid = all(
            k in actual_request
            for k in contract.request_schema.keys()
        )
        result: Dict[str, Any] = {
            "contract_id": contract_id,
            "side": "consumer",
            "valid": valid
        }
        self.results.append(result)
        return result

    def verify_provider(
        self,
        contract_id: str,
        actual_response: Dict[str, Any],
        actual_status: int
    ) -> Dict[str, Any]:
        """Verify provider sends correct response.

        Args:
            contract_id: The contract identifier.
            actual_response: The actual response data.
            actual_status: The actual status code.

        Returns:
            Verification result.
        """
        contract = self.contracts.get(contract_id)
        if not contract:
            return {"error": "Contract not found", "valid": False}  # type: ignore
        status_match = actual_status == contract.status_code
        schema_valid = all(
            k in actual_response
            for k in contract.response_schema.keys()
        )
        result: Dict[str, Any] = {
            "contract_id": contract_id,
            "side": "provider",
            "valid": status_match and schema_valid,
            "status_match": status_match
        }
        self.results.append(result)
        return result

    def get_contracts_for_consumer(self, consumer: str) -> List[ContractTest]:
        """Get all contracts for a consumer.

        Args:
            consumer: The consumer name.

        Returns:
            List of contracts.
        """
        return [c for c in self.contracts.values() if c.consumer == consumer]

    def export_pact(self, consumer: str) -> str:
        """Export contracts in Pact format.

        Args:
            consumer: The consumer to export for.

        Returns:
            JSON string in Pact format.
        """
        contracts = self.get_contracts_for_consumer(consumer)
        pact: Dict[str, Any] = {
            "consumer": {"name": consumer},
            "provider": {"name": contracts[0].provider if contracts else ""},
            "interactions": [{
                "request": {"path": c.endpoint},
                "response": {"status": c.status_code}
            } for c in contracts]
        }
        return json.dumps(pact, indent=2)


class TestSuiteOptimizer:
    """Optimize test suites by removing redundant tests.

    Analyzes test coverage and identifies tests that
    can be removed without reducing coverage.

    Attributes:
        tests: All tests in the suite.
        coverage_map: Coverage data per test.
    """

    def __init__(self) -> None:
        """Initialize test suite optimizer."""
        self.tests: List[TestCase] = []
        self.coverage_map: Dict[str, Set[str]] = {}  # test_id -> covered lines

    def add_test(self, test_id: str, covers: Set[str]) -> None:
        """Add a test with its coverage.

        Args:
            test_id: The test identifier.
            covers: Set of identifiers it covers.
        """
        self.coverage_map[test_id] = covers

    def load_tests(self, tests: List[TestCase]) -> None:
        """Load tests for optimization.

        Args:
            tests: List of tests to optimize.
        """
        self.tests = tests

    def add_coverage(self, test_id: str, covered_lines: Set[str]) -> None:
        """Add coverage data for a test.

        Args:
            test_id: The test identifier.
            covered_lines: Set of covered line identifiers.
        """
        self.coverage_map[test_id] = covered_lines

    def find_redundant_tests(self) -> List[str]:
        """Find tests whose coverage is fully covered by other tests.

        Returns:
            List of redundant test IDs.
        """
        redundant: List[str] = []
        for test_id, coverage in self.coverage_map.items():
            # Check if this test's coverage is subset of others combined
            other_coverage: Set[str] = set()
            for other_id, other_cov in self.coverage_map.items():
                if other_id != test_id:
                    other_coverage |= other_cov
            if coverage <= other_coverage:
                redundant.append(test_id)
        return redundant

    def find_overlapping_tests(self) -> List[Tuple[str, str, float]]:
        """Find tests with significant overlap.

        Returns:
            List of (test_a, test_b, overlap_percentage) tuples.
        """
        overlaps: List[Tuple[str, str, float]] = []
        test_ids = list(self.coverage_map.keys())
        for i, id_a in enumerate(test_ids):
            for id_b in test_ids[i + 1:]:
                cov_a = self.coverage_map[id_a]
                cov_b = self.coverage_map[id_b]
                if not cov_a or not cov_b:
                    continue
                intersection = cov_a & cov_b
                overlap = len(intersection) / min(len(cov_a), len(cov_b))
                if overlap > 0.8:  # 80% overlap threshold
                    overlaps.append((id_a, id_b, overlap))
        return overlaps

    def suggest_removals(self) -> List[Dict[str, Any]]:
        """Suggest tests that could be removed.

        Returns:
            List of removal suggestions with reasons.
        """
        suggestions: List[Dict[str, Any]] = []
        # Redundant tests
        for test_id in self.find_redundant_tests():
            suggestions.append({
                "test_id": test_id,
                "reason": "fully_redundant",
                "confidence": 0.9
            })
        # High overlap tests (keep the one with more coverage)
        for id_a, id_b, overlap in self.find_overlapping_tests():
            cov_a = len(self.coverage_map.get(id_a, set()))
            cov_b = len(self.coverage_map.get(id_b, set()))
            remove = id_a if cov_a < cov_b else id_b
            suggestions.append({
                "test_id": remove,
                "reason": f"overlaps {overlap * 100:.0f}% with {id_a if remove == id_b else id_b}",
                "confidence": 0.7
            })
        return suggestions


class EnvironmentProvisioner:
    """Provision test environments.

    Manages creation and teardown of test environments
    with proper isolation.

    Attributes:
        environments: Registered environments.
        active: Currently active environments.
    """

    def __init__(self) -> None:
        """Initialize environment provisioner."""
        self.environments: Dict[str, TestEnvironment] = {}
        self.active: Dict[str, bool] = {}
        self._setup_logs: Dict[str, List[str]] = {}

    def register_environment(
        self,
        name: str,
        base_url: str = "",
        variables: Optional[Dict[str, str]] = None,
        fixtures: Optional[List[str]] = None,
        setup_commands: Optional[List[str]] = None,
        teardown_commands: Optional[List[str]] = None
    ) -> TestEnvironment:
        """Register a test environment.

        Args:
            name: Environment name.
            base_url: Base URL for the environment.
            variables: Environment variables.
            fixtures: Required fixtures.
            setup_commands: Commands to run on setup.
            teardown_commands: Commands to run on teardown.

        Returns:
            The registered environment.
        """
        env = TestEnvironment(
            name=name,
            base_url=base_url,
            variables=variables or {},
            fixtures=fixtures or [],
            setup_commands=setup_commands or [],
            teardown_commands=teardown_commands or []
        )
        self.environments[name] = env
        self.active[name] = False
        self._setup_logs[name] = []
        return env

    def provision(self, name: Dict[str, Any] | str) -> Dict[str, Any]:
        """Provision an environment.

        Args:
            name: The environment name or config dict.

        Returns:
            Provisioning result.
        """
        # Convert dict to string key if needed
        if isinstance(name, dict):
            name_key = json.dumps(name, sort_keys=True)
        else:
            name_key = name
        env = self.environments.get(name_key)
        if not env:
            return {"error": "Environment not found", "success": False}
        if self.active.get(name_key):
            return {"warning": "Already active", "success": True}
        # Run setup commands (simulated)
        for cmd in env.setup_commands:
            self._setup_logs[name].append(f"Executed: {cmd}")
        self.active[name] = True
        return {
            "environment": name,
            "success": True,
            "variables": env.variables
        }

    def teardown(self, name: str) -> Dict[str, Any]:
        """Teardown an environment.

        Args:
            name: The environment name.

        Returns:
            Teardown result.
        """
        env = self.environments.get(name)
        if not env:
            return {"error": "Environment not found", "success": False}
        # Run teardown commands (simulated)
        for cmd in env.teardown_commands:
            self._setup_logs[name].append(f"Teardown: {cmd}")
        self.active[name] = False
        return {"environment": name, "success": True}

    def get_active_environments(self) -> List[str]:
        """Get list of active environments.

        Returns:
            List of active environment names.
        """
        return [name for name, active in self.active.items() if active]

    def get_logs(self, name: str) -> List[str]:
        """Get setup / teardown logs for an environment.

        Args:
            name: The environment name.

        Returns:
            List of log entries.
        """
        return self._setup_logs.get(name, [])


class ExecutionReplayer:
    """Replay test execution for debugging.

    Provides capabilities to record and replay test
    execution for debugging purposes.

    Attributes:
        traces: Recorded execution traces.
    """

    def __init__(self) -> None:
        """Initialize execution replayer."""
        self.traces: Dict[str, ExecutionTrace] = {}
        self._current_recording: Optional[str] = None
        self._step_index: Dict[str, int] = {}

    def start_recording(self, test_id: str) -> ExecutionTrace:
        """Start recording test execution.

        Args:
            test_id: The test identifier.

        Returns:
            The execution trace being recorded.
        """
        trace = ExecutionTrace(
            test_id=test_id,
            timestamp=datetime.now().isoformat()
        )
        self.traces[test_id] = trace
        self._current_recording = test_id
        return trace

    def record_step(
        self,
        action: str,
        data: Optional[Dict[str, Any]] = None
    ) -> None:
        """Record an execution step.

        Args:
            action: The action being performed.
            data: Additional data for the step.
        """
        if not self._current_recording:
            return

        trace = self.traces.get(self._current_recording)
        if trace:
            step = {
                "index": len(trace.steps),
                "timestamp": datetime.now().isoformat(),
                "action": action,
                "data": data or {}
            }
            trace.steps.append(step)

    def stop_recording(self) -> Optional[ExecutionTrace]:
        """Stop recording and return the trace.

        Returns:
            The completed execution trace.
        """
        if not self._current_recording:
            return None

        trace = self.traces.get(self._current_recording)
        self._current_recording = None
        return trace

    def replay(
        self,
        test_id: str,
        mode: ExecutionMode = ExecutionMode.FULL_REPLAY,
        breakpoint_step: int = -1
    ) -> List[Dict[str, Any]]:
        """Replay a recorded execution.

        Args:
            test_id: The test to replay.
            mode: The replay mode.
            breakpoint_step: Step to stop at if mode is BREAKPOINT.

        Returns:
            List of replayed steps.
        """
        trace = self.traces.get(test_id)
        if not trace:
            return []

        replayed = []
        for i, step in enumerate(trace.steps):
            if mode == ExecutionMode.BREAKPOINT and i == breakpoint_step:
                break

            replayed.append({
                "step": i,
                "action": step["action"],
                "replayed": True
            })

            if mode == ExecutionMode.STEP_BY_STEP:
                # In real impl, would pause for user input
                pass

        return replayed

    def get_step(self, test_id: str, step_index: int) -> Optional[Dict[str, Any]]:
        """Get a specific step from a trace.

        Args:
            test_id: The test identifier.
            step_index: The step index.

        Returns:
            The step data or None.
        """
        trace = self.traces.get(test_id)
        if trace and 0 <= step_index < len(trace.steps):
            return trace.steps[step_index]
        return None

    def export_trace(self, test_id: str) -> str:
        """Export a trace to JSON.

        Args:
            test_id: The test identifier.

        Returns:
            JSON string of the trace.
        """
        trace = self.traces.get(test_id)
        if not trace:
            return "{}"

        return json.dumps({
            "test_id": trace.test_id,
            "timestamp": trace.timestamp,
            "steps": trace.steps,
            "variables": trace.variables
        }, indent=2)


class DependencyInjector:
    """Test dependency injection framework.

    Provides dependency injection capabilities for
    test fixtures and mocks.

    Attributes:
        dependencies: Registered dependencies.
        overrides: Active dependency overrides.
    """

    def __init__(self) -> None:
        """Initialize dependency injector."""
        self.dependencies: Dict[str, TestDependency] = {}
        self.overrides: Dict[str, Any] = {}
        self._scopes: Dict[str, str] = {}  # dep_name -> scope

    def register(
        self,
        name: str,
        dependency_type: str,
        implementation: str = "",
        mock_behavior: str = "",
        scope: str = "function"
    ) -> TestDependency:
        """Register a dependency.

        Args:
            name: Dependency name.
            dependency_type: Type of the dependency.
            implementation: Implementation code / reference.
            mock_behavior: Mock behavior if mocking.
            scope: Scope (function, class, module, session).

        Returns:
            The registered dependency.
        """
        dep = TestDependency(
            name=name,
            dependency_type=dependency_type,
            implementation=implementation,
            mock_behavior=mock_behavior
        )
        self.dependencies[name] = dep
        self._scopes[name] = scope
        return dep

    def override(self, name: str, value: Any) -> None:
        """Override a dependency with a specific value.

        Args:
            name: The dependency name.
            value: The override value.
        """
        self.overrides[name] = value

    def clear_override(self, name: str) -> bool:
        """Clear a dependency override.

        Args:
            name: The dependency name.

        Returns:
            True if override was cleared.
        """
        if name in self.overrides:
            del self.overrides[name]
            return True
        return False

    def resolve(self, name: str) -> Optional[Any]:
        """Resolve a dependency.

        Args:
            name: The dependency name.

        Returns:
            The resolved value or None.
        """
        # Check overrides first
        if name in self.overrides:
            return self.overrides[name]

        dep = self.dependencies.get(name)
        if not dep:
            return None

        # Return implementation reference
        return dep.implementation

    def get_fixture_code(self, name: str) -> str:
        """Generate pytest fixture code for a dependency.

        Args:
            name: The dependency name.

        Returns:
            Fixture code string.
        """
        dep = self.dependencies.get(name)
        if not dep:
            return ""

        scope = self._scopes.get(name, "function")
        return (
            f"@pytest.fixture(scope='{scope}')\n"
            f"def {name}() -> {dep.dependency_type}:\n"
            f"    \"\"\"{name} fixture.\"\"\"\n"
            f"    {dep.implementation or 'pass'}\n"
        )

    def get_all_fixtures(self) -> str:
        """Generate all fixture code.

        Returns:
            All fixtures as code string.
        """
        fixtures = []
        for name in self.dependencies:
            fixtures.append(self.get_fixture_code(name))
        return "\n".join(fixtures)


class CrossBrowserRunner:
    """Cross-browser testing configuration and execution.

    Manages cross - browser test execution with
    parallel capabilities.

    Attributes:
        config: Cross - browser configuration.
        results: Test results per browser.
    """

    def __init__(self, config: CrossBrowserConfig) -> None:
        """Initialize cross-browser runner.

        Args:
            config: The configuration to use.
        """
        self.config = config
        self.results: Dict[BrowserType, List[Dict[str, Any]]] = {
            b: [] for b in config.browsers
        }
        self._drivers: Dict[BrowserType, bool] = {}

    def setup_driver(self, browser: BrowserType) -> bool:
        """Setup browser driver.

        Args:
            browser: The browser type.

        Returns:
            True if setup successful.
        """
        # Simulated driver setup
        self._drivers[browser] = True
        return True

    def teardown_driver(self, browser: BrowserType) -> None:
        """Teardown browser driver.

        Args:
            browser: The browser type.
        """
        self._drivers[browser] = False

    def run_test(
        self,
        test_name: str,
        test_code: Callable[[], bool]
    ) -> Dict[BrowserType, Dict[str, Any]]:
        """Run a test across all browsers.

        Args:
            test_name: The test name.
            test_code: The test function.

        Returns:
            Results for each browser.
        """
        results = {}

        for browser in self.config.browsers:
            self.setup_driver(browser)

            retries = 0
            passed = False

            while retries <= self.config.retries and not passed:
                try:
                    passed = test_code()
                except Exception:
                    retries += 1

            result = {
                "test": test_name,
                "passed": passed,
                "retries": retries,
                "headless": self.config.headless
            }
            results[browser] = result
            self.results[browser].append(result)

            self.teardown_driver(browser)

        return results

    def get_summary(self) -> Dict[str, Any]:
        """Get summary of all test runs.

        Returns:
            Summary statistics.
        """
        summary: Dict[str, Any] = {"browsers": {}}

        for browser, results in self.results.items():
            passed = sum(1 for r in results if r.get("passed"))
            summary["browsers"][browser.value] = {
                "total": len(results),
                "passed": passed,
                "failed": len(results) - passed
            }

        return summary


class ResultAggregator:
    """Aggregate test results from multiple sources.

    Collects and aggregates test results from different
    test frameworks and sources.

    Attributes:
        results: All aggregated results.
    """

    def __init__(self) -> None:
        """Initialize result aggregator."""
        self.results: List[AggregatedResult] = []
        self._by_source: Dict[TestSourceType, List[AggregatedResult]] = {}

    def add_result(
        self,
        source: TestSourceType,
        test_name: str,
        status: TestStatus,
        duration_ms: float,
        metadata: Optional[Dict[str, Any]] = None
    ) -> AggregatedResult:
        """Add a test result.

        Args:
            source: The source framework.
            test_name: The test name.
            status: The test status.
            duration_ms: Test duration in milliseconds.
            metadata: Additional metadata.

        Returns:
            The aggregated result.
        """
        result = AggregatedResult(
            source=source,
            test_name=test_name,
            status=status,
            duration_ms=duration_ms,
            timestamp=datetime.now().isoformat(),
            metadata=metadata or {}
        )
        self.results.append(result)

        if source not in self._by_source:
            self._by_source[source] = []
        self._by_source[source].append(result)

        return result

    def add_run(self, run_data: Dict[str, int]) -> None:
        """Add a test run with summary stats.

        Args:
            run_data: Dictionary with passed, failed, skipped counts.
        """
        # Create synthetic results from run summary
        for _ in range(run_data.get("passed", 0)):
            self.add_result(
                source=TestSourceType.PYTEST,
                test_name="synthetic_test",
                status=TestStatus.PASSED,
                duration_ms=1.0
            )
        for _ in range(run_data.get("failed", 0)):
            self.add_result(
                source=TestSourceType.PYTEST,
                test_name="synthetic_test",
                status=TestStatus.FAILED,
                duration_ms=1.0
            )
        for _ in range(run_data.get("skipped", 0)):
            self.add_result(
                source=TestSourceType.PYTEST,
                test_name="synthetic_test",
                status=TestStatus.SKIPPED,
                duration_ms=0.0
            )

    def import_pytest_results(self, json_report: str) -> int:
        """Import results from pytest JSON report.

        Args:
            json_report: JSON string of pytest report.

        Returns:
            Number of results imported.
        """
        try:
            data = json.loads(json_report)
            count = 0
            for test in data.get("tests", []):
                status_map = {
                    "passed": TestStatus.PASSED,
                    "failed": TestStatus.FAILED,
                    "skipped": TestStatus.SKIPPED
                }
                self.add_result(
                    source=TestSourceType.PYTEST,
                    test_name=test.get("nodeid", ""),
                    status=status_map.get(test.get("outcome", ""), TestStatus.ERROR),
                    duration_ms=test.get("duration", 0) * 1000
                )
                count += 1
            return count
        except json.JSONDecodeError:
            return 0

    def get_summary(self) -> Dict[str, Any]:
        """Get aggregated summary.

        Returns:
            Summary statistics.
        """
        total = len(self.results)
        passed = sum(1 for r in self.results if r.status == TestStatus.PASSED)
        failed = sum(1 for r in self.results if r.status == TestStatus.FAILED)
        total_duration = sum(r.duration_ms for r in self.results)

        return {
            "total_tests": total,
            "passed": passed,
            "failed": failed,
            "pass_rate": (passed / total * 100) if total > 0 else 0,
            "total_duration_ms": total_duration,
            "sources": [s.value for s in self._by_source.keys()]
        }

    def export_unified_report(self) -> str:
        """Export unified report across all sources.

        Returns:
            JSON report string.
        """
        return json.dumps({
            "summary": self.get_summary(),
            "results": [{
                "source": r.source.value,
                "test": r.test_name,
                "status": r.status.value,
                "duration_ms": r.duration_ms
            } for r in self.results]
        }, indent=2)

    def merge(self) -> Dict[str, Any]:
        """Merge all results into a single summary.

        Returns:
            Merged summary with totals.
        """
        summary = self.get_summary()
        failed = summary.get("failed", 0)
        passed = summary.get("passed", 0)

        return {
            "total_passed": passed,
            "total_failed": failed,
            "total_skipped": sum(1 for r in self.results if r.status == TestStatus.SKIPPED),
            "total_duration_ms": summary.get("total_duration_ms", 0)
        }

    def get_trend(self) -> Dict[str, Any]:
        """Analyze trend in test results.

        Returns:
            Trend analysis.
        """
        if len(self.results) < 2:
            return {"pass_rate_trend": "stable"}

        # Calculate pass rates over time
        mid_point = len(self.results) // 2
        earlier_results = self.results[:mid_point]
        later_results = self.results[mid_point:]

        earlier_rate = (
            sum(1 for r in earlier_results if r.status == TestStatus.PASSED) /
            len(earlier_results) if earlier_results else 0
        )
        later_rate = (
            sum(1 for r in later_results if r.status == TestStatus.PASSED) /
            len(later_results) if later_results else 0
        )

        if later_rate > earlier_rate:
            trend = "improving"
        elif later_rate < earlier_rate:
            trend = "declining"
        else:
            trend = "stable"

        return {"pass_rate_trend": trend}


class MutationTester:
    """Test mutation analysis.

    Provides mutation testing capabilities to evaluate
    test suite effectiveness.

    Attributes:
        mutations: Generated mutations.
        results: Mutation test results.
    """

    def __init__(self) -> None:
        """Initialize mutation tester."""
        self.mutations: List[Mutation] = []
        self.results: Dict[str, bool] = {}  # mutation_id -> killed

    def generate_mutations(
        self,
        source_code: str,
        file_path: str
    ) -> List[Mutation]:
        """Generate mutations for source code.

        Args:
            source_code: The source code to mutate.
            file_path: The file path.

        Returns:
            List of generated mutations.
        """
        mutations = []
        lines = source_code.split("\n")

        for i, line in enumerate(lines, 1):
            # Arithmetic mutations
            if "+" in line:
                mut_id = hashlib.md5(f"{file_path}:{i}:+->-".encode()).hexdigest()[:8]
                mutations.append(Mutation(
                    id=mut_id,
                    file_path=file_path,
                    line_number=i,
                    operator=MutationOperator.ARITHMETIC,
                    original_code=line,
                    mutated_code=line.replace("+", "-", 1)
                ))

            # Relational mutations
            if "==" in line:
                mut_id = hashlib.md5(f"{file_path}:{i}:==->!=".encode()).hexdigest()[:8]
                mutations.append(Mutation(
                    id=mut_id,
                    file_path=file_path,
                    line_number=i,
                    operator=MutationOperator.RELATIONAL,
                    original_code=line,
                    mutated_code=line.replace("==", "!=", 1)
                ))

            # Logical mutations
            if " and " in line:
                mut_id = hashlib.md5(f"{file_path}:{i}:and->or".encode()).hexdigest()[:8]
                mutations.append(Mutation(
                    id=mut_id,
                    file_path=file_path,
                    line_number=i,
                    operator=MutationOperator.LOGICAL,
                    original_code=line,
                    mutated_code=line.replace(" and ", " or ", 1)
                ))

        self.mutations.extend(mutations)
        return mutations

    def record_kill(self, mutation_id: str, killed: bool) -> None:
        """Record whether a mutation was killed.

        Args:
            mutation_id: The mutation identifier.
            killed: Whether the mutation was killed.
        """
        self.results[mutation_id] = killed
        for mut in self.mutations:
            if mut.id == mutation_id:
                mut.killed = killed
                break

    def get_mutation_score(self) -> float:
        """Calculate mutation score.

        Returns:
            Percentage of mutations killed.
        """
        if not self.mutations:
            return 0.0

        killed = sum(1 for m in self.mutations if m.killed)
        return (killed / len(self.mutations)) * 100

    def get_surviving_mutations(self) -> List[Mutation]:
        """Get mutations that survived (not killed).

        Returns:
            List of surviving mutations.
        """
        return [m for m in self.mutations if not m.killed]

    def generate_report(self) -> str:
        """Generate mutation testing report.

        Returns:
            Markdown report.
        """
        report = ["# Mutation Testing Report\n"]
        report.append(f"Total mutations: {len(self.mutations)}")
        report.append(f"Mutation score: {self.get_mutation_score():.1f}%\n")

        surviving = self.get_surviving_mutations()
        if surviving:
            report.append("## Surviving Mutations\n")
            for mut in surviving[:10]:  # Limit to first 10
                report.append(
                    f"- Line {mut.line_number}: {mut.operator.value} "
                    f"(`{mut.original_code.strip()}` -> `{mut.mutated_code.strip()}`)"
                )

        return "\n".join(report)


class MutationRunner:
    """Run mutation testing analysis.

    Provides mutation testing capabilities with a test-friendly interface.
    """

    def __init__(self) -> None:
        """Initialize mutation runner."""
        self.tester = MutationTester()
        self.mutation_counter = 0

    def generate_mutations(self, source_code: str) -> List[str]:
        """Generate mutations for source code.

        Args:
            source_code: The source code to mutate.

        Returns:
            List of mutated code strings.
        """
        mutations = self.tester.generate_mutations(source_code, "test.py")
        return [m.mutated_code for m in mutations]

    def add_result(self, mutation_id: str, killed: bool) -> None:
        """Record mutation test result.

        Args:
            mutation_id: The mutation identifier.
            killed: Whether the mutation was killed by tests.
        """
        # If mutation not in tester, create it
        if not any(m.id == mutation_id for m in self.tester.mutations):
            # Create a synthetic mutation
            self.mutation_counter += 1
            mut = Mutation(
                id=mutation_id,
                file_path="test.py",
                line_number=self.mutation_counter,
                operator=MutationOperator.ARITHMETIC,
                original_code="",
                mutated_code=""
            )
            mut.killed = killed
            self.tester.mutations.append(mut)

        # Record in results
        self.tester.record_kill(mutation_id, killed)

    def get_mutation_score(self) -> float:
        """Get mutation score.

        Returns:
            Mutation score as percentage (0-100).
        """
        return self.tester.get_mutation_score()


class TestGenerator:
    """Generate tests from specifications.

    Provides capabilities to generate test code from
    natural language or structured specifications.

    Attributes:
        generated: Generated tests.
    """

    def __init__(self) -> None:
        """Initialize test generator."""
        self.generated: List[GeneratedTest] = []
        self._templates: Dict[str, str] = {}

    def add_template(self, name: str, template: str) -> None:
        """Add a test template.

        Args:
            name: Template name.
            template: Template code with placeholders.
        """
        self._templates[name] = template

    def generate_from_spec(
        self,
        specification: str,
        function_name: str,
        input_type: str = "Any",
        output_type: str = "Any"
    ) -> GeneratedTest:
        """Generate test from specification.

        Args:
            specification: The test specification.
            function_name: The function being tested.
            input_type: Input type hint.
            output_type: Output type hint.

        Returns:
            The generated test.
        """
        test_name = f"test_{function_name}_{len(self.generated)}"

        # Simple template - based generation
        code = (
            f"def {test_name}():\n"
            f"    \"\"\"{specification}\"\"\"\n"
            f"    # TODO: Implement test for {function_name}\n"
            f"    # Input type: {input_type}\n"
            f"    # Output type: {output_type}\n"
            f"    pass\n"
        )

        generated = GeneratedTest(
            name=test_name,
            specification=specification,
            generated_code=code,
            confidence=0.6
        )
        self.generated.append(generated)
        return generated

    def generate_parametrized(
        self,
        function_name: str,
        test_cases: List[Tuple[Any, Any]]  # (input, expected)
    ) -> GeneratedTest:
        """Generate parametrized test.

        Args:
            function_name: The function being tested.
            test_cases: List of (input, expected) tuples.

        Returns:
            The generated test.
        """
        test_name = f"test_{function_name}_parametrized"

        params = ", ".join(str(tc) for tc in test_cases)
        code = (
            f"@pytest.mark.parametrize('input_val,expected', [\n"
            f"    {params}\n"
            f"])\n"
            f"def {test_name}(input_val, expected):\n"
            f"    result={function_name}(input_val)\n"
            f"    assert result == expected\n"
        )

        generated = GeneratedTest(
            name=test_name,
            specification=f"Parametrized test for {function_name}",
            generated_code=code,
            confidence=0.8
        )
        self.generated.append(generated)
        return generated

    def validate_generated(self, test_id: int) -> bool:
        """Validate a generated test has valid syntax.

        Args:
            test_id: Index of the generated test.

        Returns:
            True if syntax is valid.
        """
        if test_id < 0 or test_id >= len(self.generated):
            return False

        try:
            ast.parse(self.generated[test_id].generated_code)
            self.generated[test_id].validated = True
            return True
        except SyntaxError:
            return False

    def export_all(self) -> str:
        """Export all generated tests.

        Returns:
            Combined test code.
        """
        validated = [g for g in self.generated if g.validated]
        return "\n\n".join(g.generated_code for g in validated)


class TestCaseMinimizer:
    """Minimize test cases for debugging.

    Provides test case minimization to find minimal
    failing inputs.

    Attributes:
        history: Minimization history.
    """

    def __init__(self) -> None:
        """Initialize test case minimizer."""
        self.history: List[Dict[str, Any]] = []

    def minimize_string(
        self,
        input_str: str,
        test_fn: Callable[[str], bool]
    ) -> str:
        """Minimize a string input.

        Args:
            input_str: The failing input string.
            test_fn: Function returning True if test still fails.

        Returns:
            Minimized string that still causes failure.
        """
        # Delta debugging approach
        current = input_str

        while len(current) > 1:
            # Try removing half
            mid = len(current) // 2
            left = current[:mid]
            right = current[mid:]

            if test_fn(left):
                current = left
            elif test_fn(right):
                current = right
            else:
                # Can't reduce further at this granularity
                break

        self.history.append({
            "original": input_str,
            "minimized": current,
            "reduction": 1 - len(current) / len(input_str)
        })

        return current

    def minimize_list(
        self,
        input_list: List[Any],
        test_fn: Callable[[List[Any]], bool]
    ) -> List[Any]:
        """Minimize a list input.

        Args:
            input_list: The failing input list.
            test_fn: Function returning True if test still fails.

        Returns:
            Minimized list that still causes failure.
        """
        current = input_list.copy()

        # Try removing each element
        i = 0
        while i < len(current):
            candidate = current[:i] + current[i + 1:]
            if test_fn(candidate):
                current = candidate
            else:
                i += 1

        self.history.append({
            "original_length": len(input_list),
            "minimized_length": len(current)
        })

        return current

    def get_minimization_stats(self) -> Dict[str, Any]:
        """Get minimization statistics.

        Returns:
            Statistics about minimizations.
        """
        if not self.history:
            return {"total": 0}

        reductions = [h.get("reduction", 0) for h in self.history if "reduction" in h]
        avg_reduction = sum(reductions) / len(reductions) if reductions else 0

        return {
            "total_minimizations": len(self.history),
            "average_reduction": avg_reduction
        }


class TestProfiler:
    """Runtime profiling for tests.

    Provides runtime profiling capabilities to identify
    slow or resource - intensive tests.

    Attributes:
        profiles: Collected profiles.
    """

    def __init__(self) -> None:
        """Initialize test profiler."""
        self.profiles: Dict[str, TestProfile] = {}
        self._start_times: Dict[str, float] = {}

    def start_profiling(self, test_id: str) -> None:
        """Start profiling a test.

        Args:
            test_id: The test identifier.
        """
        self._start_times[test_id] = time.time()

    def stop_profiling(
        self,
        test_id: str,
        memory_peak_mb: float = 0.0,
        io_operations: int = 0,
        function_calls: int = 0
    ) -> TestProfile:
        """Stop profiling and record results.

        Args:
            test_id: The test identifier.
            memory_peak_mb: Peak memory usage.
            io_operations: Number of I / O operations.
            function_calls: Number of function calls.

        Returns:
            The test profile.
        """
        start = self._start_times.pop(test_id, time.time())
        cpu_time = (time.time() - start) * 1000  # ms

        profile = TestProfile(
            test_id=test_id,
            cpu_time_ms=cpu_time,
            memory_peak_mb=memory_peak_mb,
            io_operations=io_operations,
            function_calls=function_calls,
            timestamp=datetime.now().isoformat()
        )
        self.profiles[test_id] = profile
        return profile

    def get_slowest_tests(self, limit: int = 10) -> List[TestProfile]:
        """Get the slowest tests.

        Args:
            limit: Maximum number to return.

        Returns:
            List of slowest test profiles.
        """
        sorted_profiles = sorted(
            self.profiles.values(),
            key=lambda p: p.cpu_time_ms,
            reverse=True
        )
        return sorted_profiles[:limit]

    def get_memory_heavy_tests(self, limit: int = 10) -> List[TestProfile]:
        """Get tests with highest memory usage.

        Args:
            limit: Maximum number to return.

        Returns:
            List of memory - heavy test profiles.
        """
        sorted_profiles = sorted(
            self.profiles.values(),
            key=lambda p: p.memory_peak_mb,
            reverse=True
        )
        return sorted_profiles[:limit]

    def generate_report(self) -> str:
        """Generate profiling report.

        Returns:
            Markdown report.
        """
        report = ["# Test Profiling Report\n"]
        report.append(f"Total profiled: {len(self.profiles)}\n")

        report.append("## Slowest Tests\n")
        for profile in self.get_slowest_tests(5):
            report.append(
                f"- `{profile.test_id}`: {profile.cpu_time_ms:.2f}ms, "
                f"{profile.memory_peak_mb:.1f}MB"
            )

        return "\n".join(report)


class TestScheduler:
    """Test scheduling and load balancing.

    Manages test execution scheduling across workers
    with load balancing.

    Attributes:
        schedule: Scheduled test slots.
        workers: Available workers.
    """

    def __init__(self, num_workers: int = 4) -> None:
        """Initialize test scheduler.

        Args:
            num_workers: Number of available workers.
        """
        self.num_workers = num_workers
        self.schedule: List[ScheduleSlot] = []
        self._test_durations: Dict[str, float] = {}

    def add_duration_estimate(self, test_id: str, duration_ms: float) -> None:
        """Add estimated duration for a test.

        Args:
            test_id: The test identifier.
            duration_ms: Estimated duration in milliseconds.
        """
        self._test_durations[test_id] = duration_ms

    def create_schedule(
        self,
        tests: List[str],
        start_time: str,
        strategy: str = "load_balanced"
    ) -> List[ScheduleSlot]:
        """Create a test execution schedule.

        Args:
            tests: List of test IDs to schedule.
            start_time: Starting time for schedule.
            strategy: Scheduling strategy (load_balanced, priority, sequential).

        Returns:
            List of scheduled slots.
        """
        if strategy == "load_balanced":
            return self._schedule_load_balanced(tests, start_time)
        elif strategy == "sequential":
            return self._schedule_sequential(tests, start_time)
        else:
            return self._schedule_load_balanced(tests, start_time)

    def _schedule_load_balanced(
        self,
        tests: List[str],
        start_time: str
    ) -> List[ScheduleSlot]:
        """Create load-balanced schedule.

        Args:
            tests: Tests to schedule.
            start_time: Start time.

        Returns:
            Scheduled slots.
        """
        # Sort by duration (longest first for better balancing)
        sorted_tests = sorted(
            tests,
            key=lambda t: self._test_durations.get(t, 1000),
            reverse=True
        )

        # Distribute across workers
        worker_loads: List[List[str]] = [[] for _ in range(self.num_workers)]
        worker_times = [0.0] * self.num_workers

        for test in sorted_tests:
            # Find worker with least load
            min_worker = worker_times.index(min(worker_times))
            worker_loads[min_worker].append(test)
            worker_times[min_worker] += self._test_durations.get(test, 1000)

        # Create slots
        self.schedule = []
        for i, tests_for_worker in enumerate(worker_loads):
            if tests_for_worker:
                slot = ScheduleSlot(
                    start_time=start_time,
                    end_time="",  # Would calculate based on duration
                    tests=tests_for_worker,
                    workers=1
                )
                self.schedule.append(slot)

        return self.schedule

    def _schedule_sequential(
        self,
        tests: List[str],
        start_time: str
    ) -> List[ScheduleSlot]:
        """Create sequential schedule.

        Args:
            tests: Tests to schedule.
            start_time: Start time.

        Returns:
            Scheduled slots.
        """
        slot = ScheduleSlot(
            start_time=start_time,
            end_time="",
            tests=tests,
            workers=1
        )
        self.schedule = [slot]
        return self.schedule

    def estimate_total_duration(self) -> float:
        """Estimate total schedule duration.

        Returns:
            Estimated duration in milliseconds.
        """
        if not self.schedule:
            return 0.0

        max_duration = 0.0
        for slot in self.schedule:
            slot_duration = sum(
                self._test_durations.get(t, 1000) for t in slot.tests
            )
            max_duration = max(max_duration, slot_duration)

        return max_duration

    def get_worker_assignments(self) -> Dict[int, List[str]]:
        """Get test assignments per worker.

        Returns:
            Dictionary of worker index to test list.
        """
        assignments = {}
        for i, slot in enumerate(self.schedule):
            assignments[i] = slot.tests
        return assignments


class TestsAgent(BaseAgent):
    """Updates code file test suites using AI assistance.

    Invariants:
    - self.file_path must point to a test file (usually starting with 'test_').
    - The agent attempts to locate the corresponding source file to provide context.
    """

    def __init__(self, file_path: str) -> None:
        super().__init__(file_path)

        # Test management
        self._tests: List[TestCase] = []
        self._test_runs: List[TestRun] = []
        self._coverage_gaps: List[CoverageGap] = []
        self._factories: Dict[str, TestFactory] = {}

        # Configuration
        self._flakiness_threshold: float = 0.1  # 10% failure rate=flaky
        self._parallel_enabled: bool = False
        self._max_parallel: int = 4

    # ========== Test Management ==========

    def add_test(
        self,
        name: str,
        file_path: str,
        line_number: int,
        priority: TestPriority = TestPriority.MEDIUM,
        tags: Optional[List[str]] = None,
        dependencies: Optional[List[str]] = None
    ) -> TestCase:
        """Add a new test case."""
        test_id = hashlib.md5(f"{name}:{file_path}".encode()).hexdigest()[:8]
        test = TestCase(
            id=test_id,
            name=name,
            file_path=file_path,
            line_number=line_number,
            priority=priority,
            tags=tags or [],
            dependencies=dependencies or []
        )
        self._tests.append(test)
        return test

    def get_tests(self) -> List[TestCase]:
        """Get all tests."""
        return self._tests

    def get_test_by_id(self, test_id: str) -> Optional[TestCase]:
        """Get test by ID."""
        return next((t for t in self._tests if t.id == test_id), None)

    def get_test_by_name(self, name: str) -> Optional[TestCase]:
        """Get test by name."""
        return next((t for t in self._tests if t.name == name), None)

    def get_tests_by_priority(self, priority: TestPriority) -> List[TestCase]:
        """Get tests filtered by priority."""
        return [t for t in self._tests if t.priority == priority]

    def get_tests_by_tag(self, tag: str) -> List[TestCase]:
        """Get tests with a specific tag."""
        return [t for t in self._tests if tag in t.tags]

    # ========== Test Prioritization ==========

    def prioritize_tests(self) -> List[TestCase]:
        """Return tests sorted by priority (highest first)."""
        return sorted(
            self._tests,
            key=lambda t: (t.priority.value, t.failure_count),
            reverse=True
        )

    def calculate_priority_score(self, test: TestCase) -> float:
        """Calculate a priority score for a test."""
        score: float = test.priority.value * 20

        # Boost score for tests that fail often
        if test.run_count > 0:
            failure_rate = test.failure_count / test.run_count
            score += failure_rate * 30

        # Boost score for faster tests (they're cheaper to run)
        if test.duration_ms > 0 and test.duration_ms < 100:
            score += 10

        # Reduce score for flaky tests
        score -= test.flakiness_score * 20

        return max(0, min(100, score))

    def get_critical_tests(self) -> List[TestCase]:
        """Get tests marked as critical."""
        return [t for t in self._tests if t.priority == TestPriority.CRITICAL]

    # ========== Flakiness Detection ==========

    def calculate_flakiness(self, test: TestCase) -> float:
        """Calculate flakiness score for a test."""
        if test.run_count < 5:
            return 0.0  # Not enough data

        failure_rate = test.failure_count / test.run_count
        return failure_rate

    def detect_flaky_tests(self) -> List[TestCase]:
        """Detect tests that are flaky."""
        flaky = []
        for test in self._tests:
            score = self.calculate_flakiness(test)
            if score > self._flakiness_threshold:
                test.flakiness_score = score
                test.status = TestStatus.FLAKY
                flaky.append(test)
        return flaky

    def set_flakiness_threshold(self, threshold: float) -> None:
        """Set the flakiness threshold."""
        self._flakiness_threshold = max(0.0, min(1.0, threshold))

    def quarantine_flaky_test(self, test_id: str) -> bool:
        """Quarantine a flaky test by marking it for skip."""
        test = self.get_test_by_id(test_id)
        if test:
            test.priority = TestPriority.SKIP
            test.tags.append("quarantined")
            return True
        return False

    # ========== Coverage Gap Analysis ==========

    def add_coverage_gap(
        self,
        file_path: str,
        line_start: int,
        line_end: int,
        coverage_type: CoverageType = CoverageType.LINE,
        suggestion: str = ""
    ) -> CoverageGap:
        """Add a coverage gap."""
        gap = CoverageGap(
            file_path=file_path,
            line_start=line_start,
            line_end=line_end,
            coverage_type=coverage_type,
            suggestion=suggestion
        )
        self._coverage_gaps.append(gap)
        return gap

    def get_coverage_gaps(self) -> List[CoverageGap]:
        """Get all coverage gaps."""
        return self._coverage_gaps

    def get_coverage_gaps_by_file(self, file_path: str) -> List[CoverageGap]:
        """Get coverage gaps for a specific file."""
        return [g for g in self._coverage_gaps if g.file_path == file_path]

    def suggest_tests_for_gap(self, gap: CoverageGap) -> str:
        """Generate test suggestion for a coverage gap."""
        file_name = gap.file_path.replace('/', '_').replace('.py', '')
        return (
            f"# Suggested test for {gap.file_path} "
            f"lines {gap.line_start}-{gap.line_end}\n"
            f"def test_{file_name}_line{gap.line_start}():\n"
            f"    # TODO: Add test for {gap.coverage_type.value} "
            f"coverage\n"
            f"    {gap.suggestion or 'pass'}\n"
        )

    # ========== Test Data Factories ==========

    def add_factory(
        self,
        name: str,
        return_type: str,
        parameters: Optional[Dict[str, str]] = None,
        generator: str = ""
    ) -> TestFactory:
        """Add a test data factory."""
        factory = TestFactory(
            name=name,
            return_type=return_type,
            parameters=parameters or {},
            generator=generator
        )
        self._factories[name] = factory
        return factory

    def get_factory(self, name: str) -> Optional[TestFactory]:
        """Get a factory by name."""
        return self._factories.get(name)

    def get_factories(self) -> Dict[str, TestFactory]:
        """Get all factories."""
        return self._factories

    def generate_factory_code(self, factory: TestFactory) -> str:
        """Generate code for a factory function."""
        params = ", ".join(f"{k}: {v}" for k, v in factory.parameters.items())
        return (
            f"def {factory.name}({params}) -> {factory.return_type}:\n"
            f"    \"\"\"Factory for creating {factory.return_type} instances.\"\"\"\n"
            f"    {factory.generator or 'pass'}\n"
        )

    # ========== Test Execution Recording ==========

    def record_test_run(
        self,
        test_results: Dict[str, TestStatus],
        duration_ms: float = 0.0
    ) -> TestRun:
        """Record a test execution run."""
        run_id = hashlib.md5(
            f"{datetime.now().isoformat()}:{len(test_results)}".encode()
        ).hexdigest()[:8]

        passed = sum(1 for s in test_results.values() if s == TestStatus.PASSED)
        failed = sum(1 for s in test_results.values() if s == TestStatus.FAILED)
        skipped = sum(1 for s in test_results.values() if s == TestStatus.SKIPPED)
        errors = sum(1 for s in test_results.values() if s == TestStatus.ERROR)

        run = TestRun(
            id=run_id,
            timestamp=datetime.now().isoformat(),
            total_tests=len(test_results),
            passed=passed,
            failed=failed,
            skipped=skipped,
            errors=errors,
            duration_ms=duration_ms,
            test_results=test_results
        )
        self._test_runs.append(run)

        # Update individual test statistics
        for test_name, status in test_results.items():
            test = self.get_test_by_name(test_name)
            if test:
                test.run_count += 1
                test.last_run = run.timestamp
                if status == TestStatus.FAILED:
                    test.failure_count += 1

        return run

    def get_test_runs(self) -> List[TestRun]:
        """Get all test runs."""
        return self._test_runs

    def get_latest_run(self) -> Optional[TestRun]:
        """Get the most recent test run."""
        return self._test_runs[-1] if self._test_runs else None

    # ========== Parallel Execution ==========

    def enable_parallel(self, max_workers: int = 4) -> None:
        """Enable parallel test execution."""
        self._parallel_enabled = True
        self._max_parallel = max_workers

    def disable_parallel(self) -> None:
        """Disable parallel test execution."""
        self._parallel_enabled = False

    def is_parallel_enabled(self) -> bool:
        """Check if parallel execution is enabled."""
        return self._parallel_enabled

    def get_parallel_groups(self) -> List[List[TestCase]]:
        """Group tests for parallel execution."""
        if not self._parallel_enabled:
            return [self._tests]

        # Group by dependencies - tests with same deps can't run in parallel
        groups: List[List[TestCase]] = []
        assigned: Set[str] = set()

        for test in self._tests:
            if test.id in assigned:
                continue

            group = [test]
            assigned.add(test.id)

            # Find other tests that can run with this one
            for other in self._tests:
                if other.id in assigned:
                    continue
                # No dependency conflicts
                if not any(d in test.dependencies for d in other.dependencies):
                    if len(group) < self._max_parallel:
                        group.append(other)
                        assigned.add(other.id)

            groups.append(group)

        return groups

    # ========== Documentation Generation ==========

    def generate_test_documentation(self) -> str:
        """Generate documentation for all tests."""
        docs = ["# Test Documentation\n"]

        # Summary
        docs.append("## Summary\n")
        docs.append(f"- Total Tests: {len(self._tests)}")
        docs.append(f"- Critical: {len(self.get_tests_by_priority(TestPriority.CRITICAL))}")
        docs.append(f"- Flaky: {len(self.detect_flaky_tests())}")
        docs.append(f"- Coverage Gaps: {len(self._coverage_gaps)}\n")

        # Tests by priority
        docs.append("## Tests by Priority\n")
        for priority in TestPriority:
            tests = self.get_tests_by_priority(priority)
            if tests:
                docs.append(f"### {priority.name}\n")
                for test in tests:
                    status_icon = "" if test.status == TestStatus.PASSED else ""
                    docs.append(f"- [{status_icon}] `{test.name}` (line {test.line_number})")
                docs.append("")

        return '\n'.join(docs)

    def export_tests(self, format: str = "json") -> str:
        """Export tests to various formats."""
        if format == "json":
            data = [{
                "id": t.id,
                "name": t.name,
                "file": t.file_path,
                "line": t.line_number,
                "priority": t.priority.name,
                "status": t.status.value,
                "flakiness": t.flakiness_score,
                "tags": t.tags
            } for t in self._tests]
            return json.dumps(data, indent=2)
        return ""

    # ========== Statistics ==========

    def calculate_statistics(self) -> Dict[str, Any]:
        """Calculate test statistics."""
        total = len(self._tests)
        if total == 0:
            return {"total_tests": 0}

        by_status = {}
        for status in TestStatus:
            count = len([t for t in self._tests if t.status == status])
            by_status[status.name] = count

        by_priority = {}
        for priority in TestPriority:
            count = len([t for t in self._tests if t.priority == priority])
            by_priority[priority.name] = count

        avg_duration = sum(t.duration_ms for t in self._tests) / total if total > 0 else 0
        flaky_count = len([t for t in self._tests if t.flakiness_score > self._flakiness_threshold])

        return {
            "total_tests": total,
            "by_status": by_status,
            "by_priority": by_priority,
            "average_duration_ms": avg_duration,
            "flaky_tests": flaky_count,
            "coverage_gaps": len(self._coverage_gaps),
            "factories": len(self._factories),
            "test_runs": len(self._test_runs)
        }

    # ========== Original Methods ==========

    def _get_default_content(self) -> str:
        """Return default content for new test files."""
        return "# Tests\n\nimport pytest\n\n# Add tests here\n"

    def _get_fallback_response(self) -> str:
        """Return fallback response when Copilot is unavailable."""
        return ("# AI Improvement Unavailable\n# GitHub CLI not found. Install from "
                "https://cli.github.com/\n\n# Original test code preserved below:\n\n")

    def _find_source_file(self) -> Optional[Path]:
        """Locate source file for test file (test_foo.py -> foo.py)."""
        if not self.file_path.name.startswith('test_'):
            return None

        source_name = self.file_path.name[5:]  # Remove test_ prefix
        # Try to find source file in common locations
        # 1. Same directory
        source_path = self.file_path.parent / source_name
        if source_path.exists():
            return source_path

        # 2. Parent directory (if tests are in tests/)
        if self.file_path.parent.name == 'tests':
            source_path = self.file_path.parent.parent / source_name
            if source_path.exists():
                return source_path

        # 3. scripts / agent directory (specific to this project structure)
        agent_dir = self.file_path.parent.parent / 'scripts' / 'agent'
        source_path = agent_dir / source_name
        if source_path.exists():
            return source_path

        return None

    def _validate_syntax(self, content: str) -> bool:
        """Validate Python syntax using ast."""
        try:
            ast.parse(content)
            return True
        except SyntaxError as e:
            logging.error(f"Syntax error in generated tests: {e}")
            return False

    def _validate_test_structure(self, content: str) -> bool:
        """Validate pytest / unittest-specific patterns."""
        try:
            tree = ast.parse(content)
            issues = []

            # Check 1: All test functions follow naming convention
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    if not node.name.startswith('test_') and 'test' in node.name.lower():
                        # Just a warning, might be a helper
                        pass

            # Check 2: Tests contain assertions
            test_funcs = [n for n in ast.walk(tree) if isinstance(
                n, ast.FunctionDef) and n.name.startswith('test_')]
            for func in test_funcs:
                has_assert = any(isinstance(n, ast.Assert) for n in ast.walk(func))
                # Simple check for pytest.raises context manager
                has_raises = False
                for node in ast.walk(func):
                    if isinstance(node, ast.With):
                        for item in node.items:
                            if isinstance(item.context_expr, ast.Call):
                                if isinstance(item.context_expr.func, ast.Attribute):
                                    if item.context_expr.func.attr == 'raises':
                                        has_raises = True

                if not (has_assert or has_raises):
                    issues.append(f"Test '{func.name}' lacks assertions")

            if issues:
                logging.warning(f"Test structure issues: {', '.join(issues)}")
                # We don't fail validation for this yet, just warn

            return True
        except Exception as e:
            logging.warning(f"Failed to validate test structure: {e}")
            return True

    def improve_content(self, prompt: str) -> str:
        """Use AI to improve the test suites.

        When Copilot CLI is unavailable, BaseAgent keeps the existing content
        unchanged (avoids injecting duplicated placeholder markdown blocks).
        """
        logging.info(f"Improving tests for {self.file_path}")
        # Enhance prompt with source code context if available
        source_path = self._find_source_file()
        enhanced_prompt = prompt
        if source_path and source_path.exists():
            logging.debug(f"Using source file context: {source_path}")
            try:
                source_content = source_path.read_text(encoding='utf-8')
                # Truncate source content if it's too large to avoid context window issues
                # Assuming ~4 chars per token, 8000 tokens ~ 32000 chars.
                # Leave room for prompt and response.
                max_source_chars = 20000
                if len(source_content) > max_source_chars:
                    source_content = source_content[:max_source_chars] + "\n# ... (truncated)"

                enhanced_prompt = (
                    f"{prompt}\n\n"
                    f"# Source Code being tested ({source_path.name}):\n"
                    f"```python\n{source_content}\n```\n\n"
                    "Ensure tests cover the public API and edge cases of the source code."
                )
            except Exception as e:
                logging.warning(f"Failed to read source file context: {e}")

        new_content = super().improve_content(enhanced_prompt)

        # Validate syntax
        if not self._validate_syntax(new_content):
            logging.error("Generated tests failed syntax validation. Reverting.")
            self.current_content = self.previous_content
            return self.previous_content

        logging.debug("Syntax validation passed")

        # Validate structure
        self._validate_test_structure(new_content)

        return new_content

    def update_file(self) -> None:
        """Write the improved content back to the file (no markdown fixing for test files)."""
        self.file_path.write_text(self.current_content, encoding='utf-8')


class TestMetricsCollector:
    """Collect test execution metrics.

    Tracks execution times, flakiness, and other metrics.
    """

    def __init__(self) -> None:
        """Initialize metrics collector."""
        self.executions: Dict[str, List[float]] = {}  # test_name -> durations
        self.flaky_tests: Dict[str, int] = {}  # test_name -> occurrence count

    def record_execution(self, test_name: str, duration_ms: float) -> None:
        """Record test execution time.

        Args:
            test_name: The test name.
            duration_ms: Execution duration in milliseconds.
        """
        if test_name not in self.executions:
            self.executions[test_name] = []
        self.executions[test_name].append(duration_ms)

    def record_flaky(self, test_name: str, occurrences: int = 1) -> None:
        """Record flaky test occurrence.

        Args:
            test_name: The test name.
            occurrences: Number of flaky occurrences.
        """
        self.flaky_tests[test_name] = occurrences

    def get_metrics(self) -> Dict[str, float]:
        """Get aggregated metrics.

        Returns:
            Dict with total_duration_ms and average_duration_ms.
        """
        total_duration = sum(sum(durations) for durations in self.executions.values())
        total_tests = sum(len(durations) for durations in self.executions.values())
        avg_duration = total_duration / total_tests if total_tests > 0 else 0

        return {
            "total_duration_ms": total_duration,
            "average_duration_ms": avg_duration
        }

    def get_flaky_tests(self) -> Dict[str, int]:
        """Get flaky tests.

        Returns:
            Dict of test_name -> occurrence count.
        """
        return self.flaky_tests.copy()


class BaselineComparisonResult:
    """Result of comparing output to baseline.

    Attributes:
        matches: Whether output matches baseline.
        differences: List of differences found.
    """

    def __init__(self, matches: bool, differences: Optional[List[str]] = None) -> None:
        """Initialize comparison result."""
        self.matches = matches
        self.differences = differences or []


class BaselineManager:
    """Manage test baselines.

    Saves and compares test output to baselines.
    """

    def __init__(self, baseline_dir: Optional[Path] = None) -> None:
        """Initialize baseline manager.

        Args:
            baseline_dir: Directory to store baselines.
        """
        self.baseline_dir = Path(baseline_dir) if baseline_dir else Path("./baselines")
        self.baseline_dir.mkdir(exist_ok=True)

    def save_baseline(self, name: str, data: Dict[str, Any]) -> None:
        """Save a baseline.

        Args:
            name: Baseline name.
            data: Data to save.
        """
        baseline_path = self.baseline_dir / f"{name}.json"
        with open(baseline_path, "w") as f:
            json.dump(data, f, indent=2)

    def load_baseline(self, name: str) -> Dict[str, Any]:
        """Load a baseline.

        Args:
            name: Baseline name.

        Returns:
            The baseline data.
        """
        baseline_path = self.baseline_dir / f"{name}.json"
        if baseline_path.exists():
            with open(baseline_path, "r") as f:
                return json.load(f)
        return {}

    def compare(self, name: str, current: Dict[str, Any]) -> BaselineComparisonResult:
        """Compare current data to baseline.

        Args:
            name: Baseline name.
            current: Current data.

        Returns:
            Comparison result.
        """
        baseline = self.load_baseline(name)
        if baseline == current:
            return BaselineComparisonResult(matches=True)

        differences = []
        for key in set(list(baseline.keys()) + list(current.keys())):
            baseline_val = baseline.get(key)
            current_val = current.get(key)
            if baseline_val != current_val:
                differences.append(f"{key}: {baseline_val} -> {current_val}")

        return BaselineComparisonResult(matches=False, differences=differences)

    def update_baseline(self, name: str, data: Dict[str, Any]) -> None:
        """Update a baseline.

        Args:
            name: Baseline name.
            data: New data.
        """
        self.save_baseline(name, data)


class DIContainer:
    """Dependency injection container.

    Manages dependency registration and resolution
    with override support for testing.
    """

    def __init__(self) -> None:
        """Initialize DI container."""
        self._dependencies: Dict[str, Callable[[], Any]] = {}
        self._overrides: Dict[str, Callable[[], Any]] = {}

    def register(self, name: str, factory: Callable[[], Any]) -> None:
        """Register a dependency.

        Args:
            name: Dependency name.
            factory: Callable that creates the dependency.
        """
        self._dependencies[name] = factory

    def has(self, name: str) -> bool:
        """Check if dependency is registered.

        Args:
            name: Dependency name.

        Returns:
            True if registered.
        """
        return name in self._dependencies

    def resolve(self, name: str) -> Any:
        """Resolve a dependency.

        Args:
            name: Dependency name.

        Returns:
            The resolved dependency.
        """
        # Check for override first
        if name in self._overrides:
            return self._overrides[name]()

        if name not in self._dependencies:
            raise ValueError(f"Dependency not registered: {name}")

        return self._dependencies[name]()

    def override(self, name: str, factory: Callable[[], Any]) -> Any:
        """Context manager for dependency override.

        Args:
            name: Dependency name.
            factory: Override factory.

        Returns:
            Context manager.
        """
        from contextlib import contextmanager

        @contextmanager
        def override_context():
            old_override = self._overrides.get(name)
            self._overrides[name] = factory
            try:
                yield
            finally:
                if old_override is None:
                    self._overrides.pop(name, None)
                else:
                    self._overrides[name] = old_override

        return override_context()


# Create main function using the helper

class TestPrioritizer:
    """Prioritizes tests based on various strategies."""

    def __init__(self) -> None:
        """Initialize test prioritizer."""
        self.tests: Dict[str, Dict[str, Any]] = {}

    def add_test(self, name: str, recent_changes: int = 0, failure_rate: float = 0.0) -> None:
        """Add test for prioritization."""
        self.tests[name] = {"recent_changes": recent_changes, "failure_rate": failure_rate}

    def prioritize_by_recent_changes(self) -> List[str]:
        """Prioritize by recent changes."""
        return sorted(self.tests.keys(), key=lambda t: self.tests[t]["recent_changes"], reverse=True)

    def prioritize_by_failure_history(self) -> List[str]:
        """Prioritize by failure history."""
        return sorted(self.tests.keys(), key=lambda t: self.tests[t]["failure_rate"], reverse=True)

    def prioritize_combined(self) -> List[str]:
        """Prioritize with combined strategy."""
        scores = {}
        for test, data in self.tests.items():
            scores[test] = data["recent_changes"] + data["failure_rate"] * 100
        return sorted(scores.keys(), key=lambda t: scores[t], reverse=True)


class FlakinessDetector:
    """Detects flaky tests."""

    def __init__(self) -> None:
        """Initialize flakiness detector."""
        self.test_runs: Dict[str, List[bool]] = {}

    def add_run(self, test_name: str, passed: bool) -> None:
        """Add a test run result."""
        if test_name not in self.test_runs:
            self.test_runs[test_name] = []
        self.test_runs[test_name].append(passed)

    def is_flaky(self, test_name: str) -> bool:
        """Detect if test is flaky."""
        if test_name not in self.test_runs or len(self.test_runs[test_name]) < 2:
            return False
        results = self.test_runs[test_name]
        passes = sum(results)
        fails = len(results) - passes
        return passes > 0 and fails > 0


class QuarantineManager:
    """Manages quarantined flaky tests."""

    def __init__(self) -> None:
        """Initialize quarantine manager."""
        self.quarantined: Set[str] = set()

    def quarantine(self, test_name: str) -> None:
        """Quarantine a test."""
        self.quarantined.add(test_name)

    def release(self, test_name: str) -> None:
        """Release a quarantined test."""
        self.quarantined.discard(test_name)

    def is_quarantined(self, test_name: str) -> bool:
        """Check if test is quarantined."""
        return test_name in self.quarantined


class ImpactAnalyzer:
    """Analyzes test impact of code changes."""

    def __init__(self) -> None:
        """Initialize impact analyzer."""
        self.dependencies: Dict[str, Set[str]] = {}

    def add_dependency(self, test: str, depends_on: str) -> None:
        """Add dependency between test and code."""
        if test not in self.dependencies:
            self.dependencies[test] = set()
        self.dependencies[test].add(depends_on)

    def get_impacted_tests(self, changed_files: List[str]) -> Set[str]:
        """Get tests impacted by file changes."""
        impacted = set()
        for test, deps in self.dependencies.items():
            if any(f in deps for f in changed_files):
                impacted.add(test)
        return impacted

    def build_dependency_graph(self) -> Dict[str, Set[str]]:
        """Build dependency graph."""
        return dict(self.dependencies)


class DataFactory:
    """Factory for creating test data."""

    def __init__(self) -> None:
        """Initialize data factory."""
        self.defaults: Dict[str, Any] = {}

    def set_default(self, key: str, value: Any) -> None:
        """Set default value."""
        self.defaults[key] = value

    def create(self, **overrides: Any) -> Dict[str, Any]:
        """Create data with defaults and overrides."""
        result = dict(self.defaults)
        result.update(overrides)
        return result

    def create_batch(self, count: int, **overrides: Any) -> List[Dict[str, Any]]:
        """Create batch of data."""
        return [self.create(**overrides) for _ in range(count)]


class ParallelizationStrategy:
    """Strategy for parallel test execution."""

    def __init__(self, strategy_type: str = "round_robin") -> None:
        """Initialize strategy."""
        self.strategy_type = strategy_type

    def distribute(self, tests: List[str], workers: int) -> List[List[str]]:
        """Distribute tests across workers."""
        if self.strategy_type == "round_robin":
            result: List[List[str]] = [[] for _ in range(workers)]
            for i, test in enumerate(tests):
                result[i % workers].append(test)
            return result
        else:
            # Load balanced: distribute by count
            result = [[] for _ in range(workers)]
            for i, test in enumerate(sorted(tests, key=len, reverse=True)):
                min_idx = min(range(workers), key=lambda i: len(result[i]))
                result[min_idx].append(test)
            return result


class CoverageGapAnalyzer:
    """Analyzes coverage gaps."""

    def __init__(self) -> None:
        """Initialize analyzer."""
        self.covered: Set[str] = set()
        self.total: Set[str] = set()

    def add_covered(self, item: str) -> None:
        """Mark item as covered."""
        self.covered.add(item)
        self.total.add(item)

    def add_uncovered(self, item: str) -> None:
        """Mark item as uncovered."""
        self.total.add(item)

    def get_coverage_percentage(self) -> float:
        """Get coverage percentage."""
        if not self.total:
            return 0.0
        return (len(self.covered) / len(self.total)) * 100

    def find_uncovered(self) -> List[str]:
        """Find uncovered items."""
        return list(self.total - self.covered)


class ContractValidator:
    """Validates test contracts."""

    def validate(self, contract: Dict[str, Any]) -> bool:
        """Validate contract specification."""
        required = {"name", "preconditions", "postconditions"}
        return all(k in contract for k in required)


class TestRecorder:
    """Records test execution."""

    def record(self, test_name: str, result: bool) -> None:
        """Record test execution."""
        pass


class TestReplayer:
    """Replays recorded tests."""

    def replay(self, recording: Dict[str, Any]) -> bool:
        """Replay recorded test."""
        return True


class TestDocGenerator:
    """Generates documentation from tests."""

    def __init__(self) -> None:
        """Initialize doc generator."""
        self.tests: List[Dict[str, Any]] = []

    def add_test(self, name: str, module: str) -> None:
        """Add test for documentation."""
        self.tests.append({"name": name, "module": module})

    def generate_grouped(self) -> Dict[str, List[Dict[str, Any]]]:
        """Generate documentation grouped by module."""
        return self.group_by_module(self.tests)

    def extract_examples(self, test_code: str) -> List[Dict[str, str]]:
        """Extract examples from test code."""
        return [{"example": "example_code"}]

    def group_by_module(self, tests: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Group tests by module."""
        result: Dict[str, List[Dict[str, Any]]] = {}
        for test in tests:
            module = test.get("module", "unknown")
            if module not in result:
                result[module] = []
            result[module].append(test)
        return result


main = create_main_function(
    TestsAgent,
    'Tests Agent: Updates code file test suites',
    'Path to the tests file (e.g., test_file.py)'
)

if __name__ == '__main__':
    main()
