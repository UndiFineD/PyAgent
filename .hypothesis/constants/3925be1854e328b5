# file: C:\Users\Keimpe\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\LocalCache\local-packages\Python313\site-packages\transformers\trainer.py
# hypothesis_version: 6.150.1

[-10000.0, 0.0, 1e-06, 0.001, 0.1, 0.25, 0.5, 0.98, 0.9999, 1.0, 2.0, 5.0, -100, 128, 200, '(?:^|\\.)norm(?:$|\\.)', '*.sagemaker-uploaded', ',', ', checkpoint', '-', '-inf', '.', '.gitignore', '.weight', '/', '0.11.0', '0.27.0', '0.28.0', '0.30.0', '0.4.0', '0.41.1', '0.44.0', '0.7.0', '1.1.0', '1.10', '1.10.1', '1.3.0', '1.4.0', '2.0.0', '2.4', '2.6', '5.0.0', '8bit', '=', 'CUDA', 'DeepSpeed', 'End of training', 'Evaluation', 'FSDP', 'FULL_STATE_DICT', 'False', 'HPU', 'Invalid optimizer', 'MLU', 'MUSA', 'Model save', 'NO', 'NPU', 'Prediction', 'README.md', 'SHARDED_STATE_DICT', 'TrainerControl', 'Training', '_', '_*', '_eval_dataloaders', '_no_split_modules', '_norm(?:$|\\.)', '_orig_mod', '_original_forward', '_past', '_smp_is_partial', '_tp_size', 'accelerate', 'accepts_loss_kwargs', 'active_adapter', 'active_adapters', 'ada', 'adam', 'adapter_only', 'ademamix', 'all-linear', 'alpha', 'alpha_init', 'apex', 'args', 'attention_mask', 'attributes', 'auto', 'autotp_size', 'base_model', 'batch_size', 'beta1', 'beta2', 'beta3', 'betas', 'bfloat16', 'bias', 'bitsandbytes', 'bos_token_id', 'broadcast_buffers', 'bucket_cap_mb', 'channel', 'collate_batch', 'collate_fn', 'compute_result', 'config', 'cpu', 'cpu_amp', 'cuda', 'dataloader_config', 'dataset', 'datasets.Dataset', 'decouple_lr', 'deepspeed_plugin', 'device', 'disk', 'dispatch_batches', 'drop_last', 'dtype', 'eos_token_id', 'epoch', 'eps', 'eval', 'eval_', 'eval_inputs_ids', 'eval_label_ids', 'eval_losses', 'eval_preds', 'eval_steps', 'even_batches', 'float32', 'floating_point_ops', 'format', 'format_kwargs', 'fp16', 'fp8', 'fsdp', 'fsdp_plugin', 'fsdp_version', 'fused', 'gamma', 'generation_config', 'get_base_model', 'grad_norm', 'gradient_clipping', 'hf_deepspeed_config', 'hf_device_map', 'hf_quantizer', 'hpu', 'inf', 'input_ids', 'inputs', 'is_loaded_in_8bit', 'is_paged', 'is_parallelizable', 'is_qat_trainable', 'is_quantized', 'item', 'jit_compilation_time', 'kahan_sum', 'label', 'label_ids', 'labels', 'lamb', 'last-checkpoint', 'layernorm', 'layerwise', 'learning_rate', 'library_name', 'limit_all_gathers', 'lion', 'load_adapter', 'logging_steps', 'loss', 'losses', 'lr', 'lr_scheduler', 'main_input_name', 'max_lr', 'mems', 'min_num_params', 'minimize', 'mlu', 'model', 'model_name', 'model_parallel', 'model_path', 'model_tags', 'momentum_dtype', 'musa', 'neftune_hook_handle', 'nested_gather', 'no', 'non_blocking', 'non_padding', 'npu', 'num_examples', 'num_items_in_batch', 'num_steps', 'num_workers', 'numpy', 'objective', 'optim_bits', 'optimizer', 'optimizer.bin', 'optimizer.pt', 'optimizer_dict', 'optuna.Trial', 'pad_token_id', 'paged', 'parallelism_config', 'params', 'past_key_values', 'peft', 'persistent_workers', 'pin_memory', 'position_ids', 'prefetch_factor', 'problem_type', 'processing_class', 'proj', 'proj_type', 'pt', 'push_in_progress', 'python', 'pytorch_model_fsdp', 'quantization_method', 'r', 'random', 'rank', 'relative_step', 'return_loss', 'rmsnorm', 'rmsprop', 'rng_state.pth', 'sampler', 'save_steps', 'saving_checkpoint', 'saving_scaler_state', 'scale', 'scale_parameter', 'scale_type', 'scale_wrt_gas', 'scaler.pt', 'scheduler.pt', 'sdpa', 'set_epoch', 'shard_metadata', 'shift_labels', 'split_batches', 'std', 'step', 'str', 'study', 't_alpha', 't_beta3', 'tags', 'tensor', 'tensor_parallel', 'test', 'tie_weights', 'tmp_trainer', 'tokenizer', 'torch', 'torch_tp_plugin', 'torchao', 'total_flos', 'tp_size', 'train', 'train_loss', 'trainer_state.json', 'training_args.bin', 'type', 'update_proj_gap', 'use_gather_object', 'use_kahan_summation', 'use_seedable_sampler', 'user_content.pt', 'variance_dtype', 'w', 'wandb', 'warmup_steps', 'weight', 'weight_decay', 'weight_lr_power', 'weight_map', 'worker_init_fn', 'xla', 'xla_fsdp_grad_ckpt', 'xla_fsdp_v2']