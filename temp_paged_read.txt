            self.key_cache[block_idx, offset] = key[i]
            self.value_cache[block_idx, offset] = value[i]
    
    def read_blocks(
        self,
        block_table: list[int],
        seq_len: int,
    ) -> tuple[NDArray, NDArray]:
        """Read key/value tensors for a sequence.
        
        Args:
            block_table: Physical block indices
            seq_len: Number of tokens to read
            
        Returns:
            Tuple of (keys, values) [seq_len, num_kv_heads, head_size]
        """
        keys = np.zeros((seq_len, self.num_kv_heads, self.head_size), dtype=self.dtype)
        values = np.zeros((seq_len, self.num_kv_heads, self.head_size), dtype=self.dtype)
        
        for i in range(seq_len):
            block_idx = i // self.block_size
            offset = i % self.block_size
            if block_idx < len(block_table):
                phys_block = block_table[block_idx]
                keys[i] = self.key_cache[phys_block, offset]
                values[i] = self.value_cache[phys_block, offset]
        
        return keys, values
    
    def get_memory_usage(self) -> int:
        """Get memory usage in bytes."""
        if self.key_cache is None or self.value_cache is None:
            return 0
        return self.key_cache.nbytes + self.value_cache.nbytes


@dataclass
class AttentionMetadata:
    """Metadata for batched attention computation.
    
    Attributes:
        seq_lens: Length of each sequence [num_seqs]
        query_start_loc: Cumulative query positions [num_seqs + 1]
        max_query_len: Maximum query length in batch
        max_seq_len: Maximum total sequence length
        block_tables: Block tables [num_seqs, max_blocks]
        slot_mapping: Slot mapping [total_tokens]
        num_prefill_tokens: Number of prefill tokens
        num_decode_tokens: Number of decode tokens
    """
    seq_lens: NDArray[np.int32]
    query_start_loc: NDArray[np.int32]
    max_query_len: int
    max_seq_len: int
    block_tables: NDArray[np.int32]
    slot_mapping: NDArray[np.int64]
    num_prefill_tokens: int = 0
    num_decode_tokens: int = 0
    
    @property
    def num_seqs(self) -> int:
        """Number of sequences in batch."""
        return len(self.seq_lens)
    
    @property
    def total_tokens(self) -> int:
        """Total number of tokens in batch."""
        return int(np.sum(self.seq_lens))
    
    @classmethod
    def from_seq_lens(
        cls,
        seq_lens: Sequence[int],
        block_tables: list[list[int]],
        block_size: int,
        max_blocks_per_seq: int,
    ) -> "AttentionMetadata":
        """Create metadata from sequence lengths.
        
        Args:
            seq_lens: List of sequence lengths
            block_tables: Block tables for each sequence
            block_size: Tokens per block
            max_blocks_per_seq: Maximum blocks per sequence
            
        Returns:
            AttentionMetadata instance
        """
        seq_lens_arr = np.array(seq_lens, dtype=np.int32)
        query_start_loc = np.zeros(len(seq_lens) + 1, dtype=np.int32)
        query_start_loc[1:] = np.cumsum(seq_lens_arr)
        
        # Pad block tables
        block_tables_arr = np.full(
            (len(seq_lens), max_blocks_per_seq),
            -1,
            dtype=np.int32
        )
        for i, table in enumerate(block_tables):
            for j, block in enumerate(table[:max_blocks_per_seq]):
                block_tables_arr[i, j] = block
        
        # Compute slot mapping
        total_tokens = int(np.sum(seq_lens_arr))
        slot_mapping = np.zeros(total_tokens, dtype=np.int64)
        slot_mapper = SlotMapping(block_size)
        
        token_idx = 0
        for seq_idx, seq_len in enumerate(seq_lens):
            seq_slots = slot_mapper.map_sequence_slots(
                block_tables[seq_idx], seq_len
            )
            slot_mapping[token_idx:token_idx + seq_len] = seq_slots
            token_idx += seq_len
        
        return cls(
            seq_lens=seq_lens_arr,
            query_start_loc=query_start_loc,
            max_query_len=max(seq_lens) if seq_lens else 0,
            max_seq_len=max(seq_lens) if seq_lens else 0,
            block_tables=block_tables_arr,
            slot_mapping=slot_mapping,
        )


class PagedAttentionOps:
    """Pure NumPy implementation of paged attention operations.
    
    Provides reference implementations matching vLLM's paged attention
    kernels for testing and CPU fallback.
    """
    
    @staticmethod
    def scaled_dot_product_attention(
        query: NDArray,
        key: NDArray,
        value: NDArray,
        scale: float = 1.0,
        causal: bool = True,
        sliding_window: int | None = None,
    ) -> NDArray:
        """Compute scaled dot-product attention.
        
        Args:
            query: Query tensor [batch, num_heads, seq_len_q, head_size]
            key: Key tensor [batch, num_heads, seq_len_k, head_size]
            value: Value tensor [batch, num_heads, seq_len_k, head_size]
            scale: Attention scaling factor
            causal: Whether to apply causal masking
            sliding_window: Sliding window size (None for full attention)
            
        Returns:
            Output tensor [batch, num_heads, seq_len_q, head_size]
        """
        # Compute attention scores
        scores = np.einsum("bhqd,bhkd->bhqk", query, key) * scale
        
        seq_len_q = query.shape[2]
        seq_len_k = key.shape[2]
        
        # Apply causal mask
        if causal:
            mask = np.triu(np.ones((seq_len_q, seq_len_k), dtype=bool), k=1)
            scores = np.where(mask, float("-inf"), scores)
        
        # Apply sliding window mask
        if sliding_window is not None:
            for i in range(seq_len_q):
                for j in range(seq_len_k):
                    if j < i - sliding_window:
                        scores[:, :, i, j] = float("-inf")
        
        # Softmax
        scores_max = np.max(scores, axis=-1, keepdims=True)
        scores_exp = np.exp(scores - scores_max)
        scores_sum = np.sum(scores_exp, axis=-1, keepdims=True)
        attn_weights = scores_exp / (scores_sum + 1e-9)
        
        # Weighted sum
        output = np.einsum("bhqk,bhkd->bhqd", attn_weights, value)
        return output
    
    @staticmethod
    def paged_attention_v1(
        query: NDArray,
        key_cache: PagedKVCache,
        block_tables: NDArray[np.int32],
        seq_lens: NDArray[np.int32],
        config: AttentionConfig,
    ) -> NDArray:
        """Paged attention v1 (no partitioning).
        
        Args:
            query: Query tensor [num_seqs, num_heads, head_size]
            key_cache: Paged KV cache
            block_tables: Block tables [num_seqs, max_blocks]
            seq_lens: Sequence lengths [num_seqs]
            config: Attention configuration
            
        Returns:
            Output tensor [num_seqs, num_heads, head_size]
        """
        num_seqs = query.shape[0]
        num_heads = config.num_heads
        head_size = config.head_size
        output = np.zeros((num_seqs, num_heads, head_size), dtype=query.dtype)
        
        for seq_idx in range(num_seqs):
            seq_len = seq_lens[seq_idx]
            if seq_len == 0:
                continue
            
            # Get block table for this sequence
            block_table = block_tables[seq_idx]
            valid_blocks = [b for b in block_table if b >= 0]
            
            # Read K, V from cache
            keys, values = key_cache.read_blocks(valid_blocks, seq_len)
            
            # Expand KV for GQA
            if config.is_gqa:
                keys = PagedAttentionOps.expand_kv_for_gqa(
                    keys, config.num_queries_per_kv
                )
                values = PagedAttentionOps.expand_kv_for_gqa(
                    values, config.num_queries_per_kv
                )
            
            # Compute attention for this sequence
            q = query[seq_idx:seq_idx + 1].reshape(1, num_heads, 1, head_size)
            k = keys.reshape(1, num_heads, seq_len, head_size)
            v = values.reshape(1, num_heads, seq_len, head_size)
            
            out = PagedAttentionOps.scaled_dot_product_attention(
                q, k, v,
                scale=config.scale,
                causal=True,
                sliding_window=config.sliding_window,
            )
            output[seq_idx] = out.reshape(num_heads, head_size)
        
        return output
    
    @staticmethod
    def paged_attention_v2(
        query: NDArray,
        key_cache: PagedKVCache,
        block_tables: NDArray[np.int32],
        seq_lens: NDArray[np.int32],
        config: AttentionConfig,
        partition_size: int = 512,
    ) -> NDArray:
        """Paged attention v2 (with partitioning for long sequences).
        
        Partitions the KV cache into chunks for numerically stable
        computation on very long sequences.
        
        Args:
            query: Query tensor [num_seqs, num_heads, head_size]
            key_cache: Paged KV cache
            block_tables: Block tables [num_seqs, max_blocks]
            seq_lens: Sequence lengths [num_seqs]
            config: Attention configuration
            partition_size: Size of each partition
            
        Returns:
            Output tensor [num_seqs, num_heads, head_size]
        """
        num_seqs = query.shape[0]
        num_heads = config.num_heads
        head_size = config.head_size
        output = np.zeros((num_seqs, num_heads, head_size), dtype=query.dtype)
        
        for seq_idx in range(num_seqs):
            seq_len = seq_lens[seq_idx]
            if seq_len == 0:
                continue
            
            block_table = block_tables[seq_idx]
            valid_blocks = [b for b in block_table if b >= 0]
            keys, values = key_cache.read_blocks(valid_blocks, seq_len)
            
            if config.is_gqa:
                keys = PagedAttentionOps.expand_kv_for_gqa(keys, config.num_queries_per_kv)
                values = PagedAttentionOps.expand_kv_for_gqa(values, config.num_queries_per_kv)
            
            num_partitions = (seq_len + partition_size - 1) // partition_size
            
            # Accumulate across partitions
            exp_sums = np.zeros((num_heads,), dtype=np.float64)
            max_logits = np.full((num_heads,), float("-inf"), dtype=np.float64)
            partial_output = np.zeros((num_heads, head_size), dtype=np.float64)
            
            q = query[seq_idx]  # [num_heads, head_size]
            
            for part_idx in range(num_partitions):
                start = part_idx * partition_size
                end = min(start + partition_size, seq_len)
                
                k_part = keys[start:end]  # [part_len, num_heads, head_size]
                v_part = values[start:end]
                
                # Compute scores for this partition
                scores = np.einsum("hd,phd->hp", q, k_part) * config.scale
                
                # Online softmax update
                part_max = np.max(scores, axis=1)  # [num_heads]
                
                # Update global max
                new_max = np.maximum(max_logits, part_max)
                
                # Rescale previous accumulator
                old_scale = np.exp(max_logits - new_max)
                exp_sums = exp_sums * old_scale
                partial_output = partial_output * old_scale[:, None]
                
                # Add this partition's contribution
                scores_shifted = scores - new_max[:, None]
                exp_scores = np.exp(scores_shifted)
                exp_sums += np.sum(exp_scores, axis=1)
                partial_output += np.einsum("hp,phd->hd", exp_scores, v_part)
                
                max_logits = new_max
            
            # Normalize
            output[seq_idx] = partial_output / (exp_sums[:, None] + 1e-9)
        
        return output.astype(query.dtype)
    
    @staticmethod
    def expand_kv_for_gqa(
        kv: NDArray,
        num_queries_per_kv: int,
    ) -> NDArray:
        """Expand key/value heads for Grouped Query Attention.
        
        Args:
            kv: Key or value tensor [seq_len, num_kv_heads, head_size]
            num_queries_per_kv: Number of query heads per KV head
            
        Returns:
            Expanded tensor [seq_len, num_heads, head_size]
        """
        if num_queries_per_kv == 1:
            return kv
        # Repeat each KV head for the corresponding query heads
        return np.repeat(kv, num_queries_per_kv, axis=1)
    
    @staticmethod
    def flash_attention_chunked(
        query: NDArray,
        key: NDArray,
        value: NDArray,
        scale: float = 1.0,
        chunk_size: int = 128,
    ) -> NDArray:
        """Flash-style chunked attention for memory efficiency.
        
        Computes attention in chunks to reduce peak memory usage.
        
        Args:
            query: Query tensor [batch, num_heads, seq_len_q, head_size]
            key: Key tensor [batch, num_heads, seq_len_k, head_size]
            value: Value tensor [batch, num_heads, seq_len_k, head_size]
            scale: Attention scaling factor
            chunk_size: Size of each chunk
            
        Returns:
            Output tensor [batch, num_heads, seq_len_q, head_size]
        """
        batch_size, num_heads, seq_len_q, head_size = query.shape
        seq_len_k = key.shape[2]
        
        output = np.zeros_like(query)
        
        for q_start in range(0, seq_len_q, chunk_size):
            q_end = min(q_start + chunk_size, seq_len_q)
            q_chunk = query[:, :, q_start:q_end]
            
            # Accumulate attention for this query chunk
            exp_sums = np.zeros((batch_size, num_heads, q_end - q_start, 1), dtype=np.float64)
            max_logits = np.full((batch_size, num_heads, q_end - q_start, 1), float("-inf"))
            out_chunk = np.zeros_like(q_chunk, dtype=np.float64)
            
            for k_start in range(0, seq_len_k, chunk_size):
                k_end = min(k_start + chunk_size, seq_len_k)
                k_chunk = key[:, :, k_start:k_end]
                v_chunk = value[:, :, k_start:k_end]
                
                scores = np.einsum("bhqd,bhkd->bhqk", q_chunk, k_chunk) * scale
                
                # Apply causal mask for relevant positions
                for qi in range(q_end - q_start):
                    for ki in range(k_end - k_start):
                        global_q = q_start + qi
                        global_k = k_start + ki
                        if global_k > global_q:
                            scores[:, :, qi, ki] = float("-inf")
                
                # Online softmax
                chunk_max = np.max(scores, axis=-1, keepdims=True)
                new_max = np.maximum(max_logits, chunk_max)
                
                old_scale = np.exp(max_logits - new_max)
                exp_sums = exp_sums * old_scale
                out_chunk = out_chunk * old_scale
                
                exp_scores = np.exp(scores - new_max)
                exp_sums += np.sum(exp_scores, axis=-1, keepdims=True)
                out_chunk += np.einsum("bhqk,bhkd->bhqd", exp_scores, v_chunk)
                
                max_logits = new_max
            
            output[:, :, q_start:q_end] = (out_chunk / (exp_sums + 1e-9)).astype(query.dtype)
        
        return output


class PagedAttentionEngine:
    """High-level engine for paged attention operations.
    
    Manages block allocation, KV cache, and attention computation.
    """
    
    def __init__(
        self,
        config: AttentionConfig,
        num_blocks: int = 1024,
    ):
        """Initialize the paged attention engine.
        
        Args:
            config: Attention configuration
            num_blocks: Number of physical blocks to allocate
        """
        self.config = config
        self.block_table = BlockTable(num_blocks, config.block_size)
        
        # Determine cache dtype
        if config.kv_cache_dtype == KVCacheDtype.FP16:
            dtype = np.float16
        elif config.kv_cache_dtype == KVCacheDtype.FP32:
            dtype = np.float32
        else:
            dtype = np.float32  # Default
        
        self.kv_cache = PagedKVCache(
            num_blocks=num_blocks,
            block_size=config.block_size,
            num_kv_heads=config.num_kv_heads,
            head_size=config.head_size,
            dtype=dtype,
        )
        self.slot_mapper = SlotMapping(config.block_size)
        self._seq_positions: dict[int, int] = {}  # seq_id -> current position
    
    def allocate_sequence(self, seq_id: int, initial_len: int = 0) -> None:
        """Allocate blocks for a new sequence.
        
        Args:
            seq_id: Sequence identifier
            initial_len: Initial sequence length
        """
        if seq_id in self.block_table.block_tables:
            return  # Already allocated
        
        num_blocks_needed = (initial_len + self.config.block_size - 1) // self.config.block_size
        for _ in range(max(1, num_blocks_needed)):
            self.block_table.allocate_block(seq_id)
        
        self._seq_positions[seq_id] = initial_len
    
    def append_kv(
        self,
        seq_id: int,
        key: NDArray,
        value: NDArray,
    ) -> None:
        """Append key/value to a sequence's cache.
        
        Args:
            seq_id: Sequence identifier
            key: Key tensor [num_tokens, num_kv_heads, head_size]
            value: Value tensor [num_tokens, num_kv_heads, head_size]
        """
        num_tokens = key.shape[0]
        current_pos = self._seq_positions.get(seq_id, 0)
        
        # Check if we need more blocks
        required_blocks = (current_pos + num_tokens + self.config.block_size - 1) // self.config.block_size
        current_blocks = self.block_table.num_allocated_blocks(seq_id)
        
        while current_blocks < required_blocks:
            self.block_table.allocate_block(seq_id)
            current_blocks += 1
        
        # Compute slot mapping for new tokens
        block_table = self.block_table.get_block_table(seq_id)
        slots = np.zeros(num_tokens, dtype=np.int64)
        
        for i in range(num_tokens):
            token_pos = current_pos + i
            block_idx = token_pos // self.config.block_size
            offset = token_pos % self.config.block_size
            slots[i] = self.slot_mapper.compute_slot(block_table[block_idx], offset)
        
        # Write to cache
        self.kv_cache.write(key, value, slots)
        self._seq_positions[seq_id] = current_pos + num_tokens
    
    def forward(
        self,
        query: NDArray,
        seq_ids: list[int],
        use_v2: bool = True,
    ) -> NDArray:
        """Compute paged attention.
        
        Args:
            query: Query tensor [num_seqs, num_heads, head_size]
            seq_ids: Sequence identifiers (one per query)
            use_v2: Whether to use v2 (partitioned) attention
            
        Returns:
            Output tensor [num_seqs, num_heads, head_size]
        """
        # Build metadata
        seq_lens = np.array(
            [self._seq_positions.get(sid, 0) for sid in seq_ids],
            dtype=np.int32
        )
        
        max_blocks = max(
            self.block_table.num_allocated_blocks(sid) for sid in seq_ids
        ) if seq_ids else 1
        
        block_tables = np.full((len(seq_ids), max_blocks), -1, dtype=np.int32)
        for i, sid in enumerate(seq_ids):
            table = self.block_table.get_block_table(sid)
            for j, block in enumerate(table[:max_blocks]):
                block_tables[i, j] = block
        
        if use_v2:
            return PagedAttentionOps.paged_attention_v2(
                query, self.kv_cache, block_tables, seq_lens, self.config
            )
        else:
            return PagedAttentionOps.paged_attention_v1(
                query, self.kv_cache, block_tables, seq_lens, self.config
            )
    
    def free_sequence(self, seq_id: int) -> None:
        """Free all resources for a sequence."""
        self.block_table.free_sequence(seq_id)
        self._seq_positions.pop(seq_id, None)
    
    def get_stats(self) -> dict:
        """Get engine statistics."""
        return {
            "num_sequences": len(self._seq_positions),
            "num_allocated_blocks": self.block_table.num_blocks - self.block_table.num_free_blocks,
            "num_free_blocks": self.block_table.num_free_blocks,
            "kv_cache_memory_mb": self.kv_cache.get_memory_usage() / (1024 * 1024),
        }


# Convenience functions
def create_attention_engine(
    head_size: int = 64,
    num_heads: int = 32,
    num_kv_heads: int = 8,
    block_size: int = 16,
    num_blocks: int = 1024,
) -> PagedAttentionEngine:
    """Create a paged attention engine with common defaults.
    
    Args:
        head_size: Size of each attention head
        num_heads: Number of query heads
        num_kv_heads: Number of KV heads (for GQA)
        block_size: Tokens per block
        num_blocks: Total number of blocks
        
    Returns:
        Configured PagedAttentionEngine
    """
    config = AttentionConfig(
        head_size=head_size,
        num_heads=num_heads,
        num_kv_heads=num_kv_heads,
        block_size=block_size,
    )
    return PagedAttentionEngine(config, num_blocks)
