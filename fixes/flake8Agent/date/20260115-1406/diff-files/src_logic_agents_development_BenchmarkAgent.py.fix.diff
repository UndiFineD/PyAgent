diff --git a/src/logic/agents/development/BenchmarkAgent.py b/src/logic/agents/development/BenchmarkAgent.py
index dfc03d5..b01ebc5 100644
--- a/src/logic/agents/development/BenchmarkAgent.py
+++ b/src/logic/agents/development/BenchmarkAgent.py
@@ -26,18 +26,28 @@ from __future__ import annotations
 from src.core.base.version import VERSION
 import time
 import logging
-from typing import Dict, List, Any, Optional
+from typing import Any
 from src.core.base.BaseAgent import BaseAgent
 from src.core.base.utilities import as_tool
 from src.logic.agents.development.core.BenchmarkCore import BenchmarkCore, BenchmarkResult
 
 __version__ = VERSION
 
+
+
+
+
+
+
+
+
+
+
 class BenchmarkAgent(BaseAgent):
     """Benchmarks the performance of the agent fleet.
     Integrated with BenchmarkCore for regression testing and baseline tracking.
     """
-    
+
     def __init__(self, file_path: str) -> None:
         super().__init__(file_path)
         self.core = BenchmarkCore()
@@ -56,7 +66,7 @@ class BenchmarkAgent(BaseAgent):
     def run_sgi_benchmark(self, agent_name: str, scientific_task: str) -> dict[str, Any]:
         """Runs an SGI-Bench scientific inquiry evaluation on an agent."""
         logging.info(f"BENCHMARK: Running SGI inquiry for {agent_name}")
-        
+
         # In a real system, we'd inspect the agent's internal DCAP metadata
         scores = {
             "deliberation_score": 0.85, # Mock
@@ -65,7 +75,7 @@ class BenchmarkAgent(BaseAgent):
             "perception_score": 0.88,
             "sgi_index": 0.86
         }
-        
+
         result = {
             "agent": agent_name,
             "task": scientific_task,
@@ -111,15 +121,15 @@ class BenchmarkAgent(BaseAgent):
         """Runs a task against an agent and measures performance."""
         # Note: In a real system, this would call the FleetManager
         start_time = time.time()
-        
+
         # Simulated run (for the skeleton)
         logging.info(f"BENCHMARK: Running task '{task}' on agent '{agent_name}'")
-        
+
         # We would actually trigger the agent here
         # captured_output = fleet.agents[agent_name].improve_content(task)
-        
+
         duration = time.time() - start_time
-        
+
         result = {
             "agent": agent_name,
             "task": task,
@@ -127,7 +137,7 @@ class BenchmarkAgent(BaseAgent):
             "success": True # Mock
         }
         self.results.append(result)
-        
+
         return f"Benchmark completed for {agent_name}. Latency: {duration:.2f}s"
 
     @as_tool
@@ -135,12 +145,12 @@ class BenchmarkAgent(BaseAgent):
         """Checks if an agent's current performance has regressed vs the fleet baseline."""
         baseline = self.core.calculate_baseline(self.benchmark_results)
         regression = self.core.check_regression(current_latency, baseline)
-        
+
         if regression["regression"]:
             msg = f"REGRESSION DETECTED: {agent_id} is {regression['delta_percentage']:.1f}% slower than baseline."
             logging.error(msg)
             return msg
-            
+
         return f"SUCCESS: {agent_id} is within performance limits."
 
     @as_tool
@@ -148,14 +158,24 @@ class BenchmarkAgent(BaseAgent):
         """Generates a summary report of all benchmark runs."""
         if not self.results:
             return "No benchmark data available."
-        
+
         report = ["## Benchmark Summary Report"]
+
+
+
+
+
         for r in self.results:
             report.append(f"- **Agent**: {r['agent']} | **Task**: {r['task']} | **Latency**: {r['latency']:.2f}s")
-        
+
         return "\n".join(report)
 
+
+
+
+
+
 if __name__ == "__main__":
     from src.core.base.utilities import create_main_function
     main = create_main_function(BenchmarkAgent, "Benchmark Agent", "Benchmark history path")
-    main()
\ No newline at end of file
+    main()
