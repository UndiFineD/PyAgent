diff --git a/src/logic/agents/cognitive/KnowledgeAgent.py b/src/logic/agents/cognitive/KnowledgeAgent.py
index d3621f8..d35ee2d 100644
--- a/src/logic/agents/cognitive/KnowledgeAgent.py
+++ b/src/logic/agents/cognitive/KnowledgeAgent.py
@@ -33,7 +33,7 @@ import json
 import re
 from datetime import datetime
 from pathlib import Path
-from typing import Dict, List, Any, Optional
+from typing import Any
 
 __version__ = VERSION
 
@@ -43,9 +43,14 @@ try:
 except ImportError:
     HAS_CHROMADB = False
 
+
+
+
+
+
 class KnowledgeAgent(BaseAgent):
     """Agent that scans the workspace to provide deep context using MIRIX 6-tier memory."""
-    
+
     def __init__(self, file_path: str | None = None, fleet: Any | None = None) -> None:
         # Phase 123: Robust initialization for dynamic discovery
         if file_path is None:
@@ -53,7 +58,7 @@ class KnowledgeAgent(BaseAgent):
                  file_path = str(fleet.workspace_root)
              else:
                  file_path = "."
-        
+
         super().__init__(file_path)
         workspace_root = self.file_path if self.file_path.is_dir() else self.file_path.parent
         self.index_file = workspace_root / ".agent_knowledge_index.json"
@@ -65,7 +70,7 @@ class KnowledgeAgent(BaseAgent):
         self.memory_engine = MemoryEngine(str(workspace_root))
         self.compressor = ContextCompressor(str(workspace_root))
         self.knowledge_core = KnowledgeCore()
-        
+
         self._system_prompt = (
             "You are the Knowledge Agent (MIRIX Memory Orchestrator). "
             "You manage 6 memory tiers: Core, Episodic, Semantic, Procedural, Resource, and Knowledge. "
@@ -77,7 +82,7 @@ class KnowledgeAgent(BaseAgent):
         """Initialize ChromaDB client and multiple collections for tiered memory."""
         if not HAS_CHROMADB:
             return False
-        
+
         try:
             if self._chroma_client is None:
                 self._chroma_client = chromadb.PersistentClient(path=str(self.db_path))
@@ -96,9 +101,17 @@ class KnowledgeAgent(BaseAgent):
             ".py": r"(?:class|def)\s+([a-zA-Z_][a-zA-Z0-9_]*)"
         }
         index = self.knowledge_core.build_symbol_map(root, patterns)
-                
-        with open(self.index_file, "w") as f:
-            json.dump(index, f, indent=4)
+
+        idx_path = Path(self.index_file)
+        temp_path = idx_path.with_suffix(".tmp")
+        try:
+            with open(temp_path, "w", encoding="utf-8") as f:
+                json.dump(index, f, indent=4)
+            temp_path.replace(idx_path)
+        except Exception:
+            if temp_path.exists(): temp_path.unlink()
+            raise
+
         logging.info(f"Knowledge index built at {self.index_file}")
 
     def record_tier_memory(self, tier: str, content: str, metadata: dict[str, Any] | None = None) -> str:
@@ -107,15 +120,15 @@ class KnowledgeAgent(BaseAgent):
         """
         if not self._init_chroma():
             return
-        
+
         valid_tiers = ["core", "episodic", "semantic", "procedural", "resource", "knowledge"]
         if tier.lower() not in valid_tiers:
             logging.warning(f"Invalid MIRIX tier: {tier}")
-        
+
         meta = metadata or {}
         meta["tier"] = tier.lower()
         meta["timestamp"] = datetime.now().isoformat()
-        
+
         try:
             self._mirix_collection.add(
                 documents=[content],
@@ -129,14 +142,14 @@ class KnowledgeAgent(BaseAgent):
         """Queries a specific tier of memory for context."""
         if not self._init_chroma():
             return ""
-        
+
         try:
             results = self._mirix_collection.query(
                 query_texts=[query],
                 n_results=limit,
                 where={"tier": tier.lower()}
             )
-            
+
             output = [f"### [MIRIX Tier: {tier.upper()}] Results for '{query}'"]
             for i, doc in enumerate(results.get("documents", [[]])[0]):
                 output.append(f"> [!NOTE] Memory {i+1}\n> {doc}\n")
@@ -162,12 +175,12 @@ class KnowledgeAgent(BaseAgent):
                 continue
             if any(part in str(p) for part in ["__pycache__", "venv", ".git", "data/db/.agent_chroma_db"]):
                 continue
-            
+
             try:
                 content = p.read_text(encoding="utf-8")
                 if not content.strip():
                     continue
-                
+
                 # Chunking: for now simple line-based or whole file
                 # To keep it simple for a "quick implementation", we'll do file-level with some overlap if large
                 # But let's just do file-level for now.
@@ -196,19 +209,19 @@ class KnowledgeAgent(BaseAgent):
                 query_texts=[query],
                 n_results=n_results
             )
-            
+
             snippets = []
             for i in range(len(results['documents'][0])):
                 doc = results['documents'][0][i]
                 meta = results['metadatas'][0][i]
                 path = meta['path']
-                
+
                 # Truncate doc if too long
                 if len(doc) > 1000:
                     doc = doc[:1000] + "\n... (truncated)"
-                
+
                 snippets.append(f"> [!ABSTRACT] File: {path} (Semantic Match)\n> ```\n" + "\n".join([f"> {sl}" for sl in doc.splitlines()[:20]]) + "\n> ```\n")
-            
+
             return "\n".join(snippets)
         except Exception as e:
             logging.error(f"Semantic search error: {e}")
@@ -218,11 +231,11 @@ class KnowledgeAgent(BaseAgent):
         """Searches the workspace using index, graph, and vector search."""
         if not self.index_file.exists():
             self.build_index()
-        
+
         # Build vector index if it doesn't exist
         if HAS_CHROMADB and not self.db_path.exists():
             self.build_vector_index()
-            
+
         try:
             with open(self.index_file) as f:
                 index = json.load(f)
@@ -231,7 +244,7 @@ class KnowledgeAgent(BaseAgent):
 
         root = self.file_path.parent
         context_snippets = []
-        
+
         # 1. Check Graph & Symbols (Hybrid logic)
         # Check if query is a symbol in the graph
         impacted_files = self.graph_engine.get_impact_radius(query)
@@ -297,26 +310,36 @@ class KnowledgeAgent(BaseAgent):
                     pass
                 if len(context_snippets) > 10:
                     break
-                
+
         if not context_snippets:
             return f"No relevant context found for '{query}' in {root}."
-            
+
         return "## Gathered Context\n\n" + "\n".join(context_snippets)
 
     def find_backlinks(self, file_name: str) -> list[str]:
         """Finds all notes that link to the specified file/note name."""
         if not self.index_file.exists():
             self.build_index()
-            
+
         try:
             with open(self.index_file) as f:
                 index = json.load(f)
         except Exception:
             index = {}
-            
+
         # Strip extension for note name
         note_name = Path(file_name).stem
-        return index.get(f"link:{note_name}", [])
+        entries = index.get(note_name, [])
+
+        # Extract paths from the index entries (which are dicts)
+        paths = []
+        for entry in entries:
+            if isinstance(entry, dict) and "path" in entry:
+                paths.append(entry["path"])
+            elif isinstance(entry, str):
+                paths.append(entry)
+
+        return paths
 
     def auto_update_backlinks(self, directory: str | None = None) -> int:
         """Updates all .md files in the directory with a Backlinks section."""
@@ -324,39 +347,39 @@ class KnowledgeAgent(BaseAgent):
         if not root.is_dir():
             logging.error(f"Cannot update backlinks: {root} is not a directory")
             return 0
-            
+
         updated_count = 0
         for p in root.rglob("*.md"):
             backlinks = self.find_backlinks(p.name)
             if not backlinks:
                 continue
-                
+
             try:
                 content = p.read_text(encoding="utf-8")
                 links_str = "\n".join([f"- [[{Path(b).stem}]]" for b in backlinks])
                 backlink_section = f"\n\n## Backlinks\n\n{links_str}\n"
-                
+
                 if "## Backlinks" in content:
                     # Replace existing section
                     new_content = re.sub(r"## Backlinks\n.*?(?=\n\n##|\Z)", backlink_section.strip(), content, flags=re.DOTALL)
                 else:
                     # Append to end
                     new_content = content.rstrip() + backlink_section
-                
+
                 if new_content != content:
                     p.write_text(new_content, encoding="utf-8")
                     updated_count += 1
                     logging.info(f"Updated backlinks for {p.name}")
             except Exception as e:
                 logging.error(f"Failed to update backlinks for {p}: {e}")
-                
+
         return updated_count
 
     def get_graph_mermaid(self) -> str:
         """Generates a Mermaid graph of the workspace note relationships."""
         if not self.index_file.exists():
             self.build_index()
-            
+
         try:
             with open(self.index_file) as f:
                 index = json.load(f)
@@ -365,19 +388,29 @@ class KnowledgeAgent(BaseAgent):
 
         nodes = set()
         edges = []
-        
-        for key, paths in index.items():
-            if key.startswith("link:"):
-                target = key.replace("link:", "")
-                nodes.add(target)
-                for path in paths:
-                    source = Path(path).stem
-                    nodes.add(source)
-                    edges.append(f"  {source} --> {target}")
-        
+
+        for key, entries in index.items():
+            target = key
+            # Skip if key looks like an absolute path or non-symbol (heuristic)
+            if "/" in target or "\\" in target:
+                continue
+
+            nodes.add(target)
+
+            for entry in entries:
+                path_str = entry
+                if isinstance(entry, dict) and "path" in entry:
+                    path_str = entry["path"]
+                elif not isinstance(entry, str):
+                    continue
+
+                source = Path(path_str).stem
+                nodes.add(source)
+                edges.append(f"  {source} --> {target}")
+
         if not edges:
             return "graph TD\n  Start[No Links Detected]"
-            
+
         return "graph TD\n" + "\n".join(edges)
 
     def get_compressed_briefing(self, file_paths: list[str]) -> str:
@@ -398,7 +431,7 @@ class KnowledgeAgent(BaseAgent):
             "related_nodes": [],
             "context_summary": ""
         }
-        
+
         # 1. Semantic Search
         try:
             # We use MemoryEngine's chroma client or the internal one
@@ -412,7 +445,7 @@ class KnowledgeAgent(BaseAgent):
         for match in results["semantic_matches"]:
             match_text = match.get("content", "")
             symbols = re.findall(r"(?:class|def)\s+([a-zA-Z_][a-zA-Z0-9_]*)", match_text)
-            
+
             for symbol in symbols:
                 if symbol in seen_nodes:
                     continue
@@ -431,7 +464,7 @@ class KnowledgeAgent(BaseAgent):
             file_path = match.get("metadata", {}).get("file_path")
             if file_path:
                 files_to_compress.add(file_path)
-        
+
         if files_to_compress:
             compressed_bits = []
             for f in list(files_to_compress)[:3]:
@@ -446,16 +479,16 @@ class KnowledgeAgent(BaseAgent):
     def query_knowledge(self, query: str) -> str:
         """User-facing knowledge query tool."""
         search_data = self.hybrid_search(query)
-        
+
         report = [f"# Hybrid Search Results: '{query}'\n"]
-        
+
         if search_data["semantic_matches"]:
             report.append("## ðŸ” Semantic Matches")
             for m in search_data["semantic_matches"]:
                 score = m.get("score", 0)
                 path = m.get("metadata", {}).get("file_path", "unknown")
                 report.append(f"- **{path}** (Score: {score:.4f})")
-                
+
         if search_data["related_nodes"]:
             report.append("\n## ðŸ•¸ï¸ Graph Relationships")
             for node in search_data["related_nodes"]:
@@ -464,11 +497,11 @@ class KnowledgeAgent(BaseAgent):
                     report.append(f"- **Depends on**: {', '.join(node['depends_on'])}")
                 if node["depended_on_by"]:
                     report.append(f"- **Depended on by**: {', '.join(node['depended_on_by'])}")
-                    
+
         if search_data["context_summary"]:
             report.append("\n## ðŸ“„ Context Signatures")
             report.append(search_data["context_summary"])
-            
+
         return "\n".join(report)
 
     def improve_content(self, prompt: str) -> str:
@@ -480,6 +513,11 @@ class KnowledgeAgent(BaseAgent):
         )
         return super().improve_content(synthesis_prompt)
 
+
+
+
+
+
 if __name__ == "__main__":
     main = create_main_function(KnowledgeAgent, "Knowledge Agent", "Topic/Symbol to find context for")
-    main()
\ No newline at end of file
+    main()
