diff --git a/src/infrastructure/backend/execution_engine.py b/src/infrastructure/backend/execution_engine.py
index 434218d..0eea0f4 100644
--- a/src/infrastructure/backend/execution_engine.py
+++ b/src/infrastructure/backend/execution_engine.py
@@ -24,7 +24,7 @@ from __future__ import annotations
 from src.core.base.version import VERSION
 import sys
 from pathlib import Path
-from typing import Any, Dict, List, Optional
+from typing import Any
 import requests
 from src.infrastructure.backend.SubagentRunner import SubagentRunner
 
@@ -46,46 +46,171 @@ _runner = SubagentRunner()
 _response_cache = _runner._response_cache
 _metrics = _runner._metrics
 
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 def _resolve_repo_root() -> Path:
     """Legacy helper."""
     return _runner._resolve_repo_root()
 
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 def _command_available(command: str) -> bool:
     """Legacy helper."""
+
+
+
+
+
     return _runner._command_available(command)
 
+
+
+
+
+
+
+
+
+
+
 def _get_cache_key(prompt: str, model: str) -> str:
+
+
+
+
+
     """Legacy helper."""
     return _runner._get_cache_key(prompt, model)
 
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 def clear_response_cache() -> None:
     """Clear the response cache."""
     _runner.clear_response_cache()
 
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 def get_metrics() -> dict[str, Any]:
     """Get current metrics snapshot."""
     return _runner.get_metrics()
 
+
+
+
+
+
 def reset_metrics() -> None:
     """Reset metrics."""
     _runner.reset_metrics()
 
+
+
+
+
+
 def validate_response_content(response: str, content_types: list[str] | None = None) -> bool:
     """Validate AI response content."""
     return _runner.validate_response_content(response, content_types)
 
+
+
+
+
+
+
+
+
+
+
 def estimate_tokens(text: str) -> int:
     """Estimate token count."""
     return _runner.estimate_tokens(text)
 
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 def estimate_cost(tokens: int, model: str = "gpt-4", rate_per_1k_input: float = 0.03) -> float:
     """Estimate cost."""
     return _runner.estimate_cost(tokens, model, rate_per_1k_input)
 
+
+
+
+
+
 def configure_timeout_per_backend(backend: str, timeout_s: int) -> None:
     """Configure timeout."""
     _runner.configure_timeout_per_backend(backend, timeout_s)
 
+
+
+
+
+
 def llm_chat_via_github_models(
     prompt: str,
     model: str,
@@ -114,14 +239,29 @@ def llm_chat_via_github_models(
         validate_content=validate_content,
     )
 
+
+
+
+
+
 def run_subagent(description: str, prompt: str, original_content: str = "") -> str | None:
     """Run a subagent."""
     return _runner.run_subagent(description, prompt, original_content)
 
+
+
+
+
+
 def get_backend_status() -> dict[str, Any]:
     """Return diagnostic snapshot."""
     return _runner.get_backend_status()
 
+
+
+
+
+
 def describe_backends() -> str:
     """Return human-readable diagnostics."""
-    return _runner.describe_backends()
\ No newline at end of file
+    return _runner.describe_backends()
