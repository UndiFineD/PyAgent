diff --git a/src/infrastructure/backend/VllmNativeEngine.py b/src/infrastructure/backend/VllmNativeEngine.py
index 119a429..947d94e 100644
--- a/src/infrastructure/backend/VllmNativeEngine.py
+++ b/src/infrastructure/backend/VllmNativeEngine.py
@@ -26,7 +26,7 @@ Optimized for local inference and future trillion-parameter context handling.
 from __future__ import annotations
 from src.core.base.version import VERSION
 import logging
-from typing import Optional, Any
+from typing import Any
 import os
 
 __version__ = VERSION
@@ -37,6 +37,16 @@ try:
 except ImportError:
     HAS_VLLM = False
 
+
+
+
+
+
+
+
+
+
+
 class VllmNativeEngine:
     """
     Manages a local vLLM instance using the library directly.
@@ -45,14 +55,14 @@ class VllmNativeEngine:
     _instance: VllmNativeEngine | None = None
     _llm: Any | None = None
 
-    def __init__(self, model_name: str = "meta-llama/Llama-3-8B-Instruct", 
+    def __init__(self, model_name: str = "meta-llama/Llama-3-8B-Instruct",
                  gpu_memory_utilization: float = 0.8,
                  tensor_parallel_size: int = 1) -> None:
         self.model_name = model_name
         self.gpu_memory_utilization = gpu_memory_utilization
         self.tensor_parallel_size = tensor_parallel_size
         self.enabled = HAS_VLLM
-        
+
     @classmethod
     def get_instance(cls, **kwargs) -> VllmNativeEngine:
         if cls._instance is None:
@@ -63,7 +73,7 @@ class VllmNativeEngine:
         """Lazily initialize the vLLM engine to save VRAM until needed."""
         if not self.enabled:
             return False
-        
+
         if self._llm is None:
             try:
                 import torch
@@ -76,27 +86,27 @@ class VllmNativeEngine:
                     else:
                         os.environ["VLLM_TARGET_DEVICE"] = "cpu"
                         logging.warning("vLLM: No CUDA detected. Using CPU mode (Lower performance).")
-                
+
                 logging.info(f"Initializing Native vLLM: {self.model_name} (Device: {os.environ.get('VLLM_TARGET_DEVICE', 'auto')})...")
-                
+
                 import torch
                 # Only check CUDA if we aren't explicitly targeting CPU
                 if os.environ.get("VLLM_TARGET_DEVICE") != "cpu" and not torch.cuda.is_available():
                     logging.warning("vLLM: No CUDA detected. Falling back to CPU mode.")
                     os.environ["VLLM_TARGET_DEVICE"] = "cpu"
-                
+
                 # Configure for CPU if applicable
                 kwargs = {
                     "model": self.model_name,
                     "trust_remote_code": True
                 }
-                
+
                 if os.environ.get("VLLM_TARGET_DEVICE") == "cpu":
                     kwargs["device"] = "cpu"
                 else:
                     kwargs["gpu_memory_utilization"] = self.gpu_memory_utilization
                     kwargs["tensor_parallel_size"] = self.tensor_parallel_size
-                
+
                 self._llm = LLM(**kwargs)
                 logging.info("Native vLLM Engine started successfully.")
             except Exception as e:
@@ -105,7 +115,7 @@ class VllmNativeEngine:
                 return False
         return True
 
-    def generate(self, prompt: str, system_prompt: str = "", 
+    def generate(self, prompt: str, system_prompt: str = "",
                  temperature: float = 0.7, max_tokens: int = 1024) -> str:
         """Generates text from the local model."""
         if not self._init_llm():
@@ -114,15 +124,15 @@ class VllmNativeEngine:
         try:
             # Format according to chat templates if possible, or simple concat
             full_prompt = f"{system_prompt}\n\nUser: {prompt}\n\nAssistant:" if system_prompt else prompt
-            
+
             sampling_params = SamplingParams(
                 temperature=temperature,
                 max_tokens=max_tokens,
                 top_p=0.95
             )
-            
+
             outputs = self._llm.generate([full_prompt], sampling_params)
-            
+
             if outputs:
                 return outputs[0].outputs[0].text
             return ""
@@ -133,7 +143,7 @@ class VllmNativeEngine:
     def shutdown(self) -> None:
         """Clears the vLLM instance and frees VRAM (Phase 108)."""
         if self._llm:
-            # vLLM doesn't have a simple 'off' but we can delete reference 
+            # vLLM doesn't have a simple 'off' but we can delete reference
             # and try to trigger GC or rely on process exit.
             import gc
             import torch
@@ -142,4 +152,4 @@ class VllmNativeEngine:
             gc.collect()
             if torch.cuda.is_available():
                 torch.cuda.empty_cache()
-            logging.info("Native vLLM Engine shut down and VRAM cleared.")
\ No newline at end of file
+            logging.info("Native vLLM Engine shut down and VRAM cleared.")
