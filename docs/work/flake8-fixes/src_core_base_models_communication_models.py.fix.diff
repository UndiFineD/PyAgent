diff --git a/src/core/base/models/communication_models.py b/src/core/base/models/communication_models.py
index 0d944e1..8d37f7c 100644
--- a/src/core/base/models/communication_models.py
+++ b/src/core/base/models/communication_models.py
@@ -20,12 +20,11 @@ import uuid
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
-from typing import Any, Dict, List, Optional
+from typing import Any
 from collections.abc import Callable
 from .enums import (
-    MessageRole, 
-    AgentEvent, 
-    FilePriority, 
+    MessageRole,
+    FilePriority,
     InputType
 )
 from .base_models import (
@@ -34,6 +33,16 @@ from .base_models import (
     _empty_dict_str_any
 )
 
+
+
+
+
+
+
+
+
+
+
 @dataclass(slots=True)
 class CascadeContext:
     """
@@ -46,7 +55,7 @@ class CascadeContext:
     parent_agent_id: str | None = None
     cascade_depth: int = 0
     max_depth: int = 10
-    
+
     def next_level(self, agent_id: str) -> CascadeContext:
         """Create a child context for the next level of delegation."""
         return CascadeContext(
@@ -58,14 +67,29 @@ class CascadeContext:
             max_depth=self.max_depth
         )
 
+
+
+
+
+
     def is_bursting(self) -> bool:
         """Check if recursion depth limit reached."""
         return self.cascade_depth >= self.max_depth
 
+
+
+
+
+
 @dataclass(slots=True)
 class PromptTemplate:
     """ reusable prompt template. """
     name: str
+
+
+
+
+
     template: str
     variables: list[str] = field(default_factory=_empty_list_str)
     id: str = ""
@@ -73,9 +97,19 @@ class PromptTemplate:
     version: str = "1.0"
     tags: list[str] = field(default_factory=_empty_list_str)
 
+
+
+
+
+
     def render(self, **kwargs: Any) -> str:
         return self.template.format(**kwargs)
 
+
+
+
+
+
 @dataclass(slots=True)
 class ConversationMessage:
     """A message in conversation history."""
@@ -83,6 +117,16 @@ class ConversationMessage:
     content: str
     timestamp: float = field(default_factory=time.time)
 
+
+
+
+
+
+
+
+
+
+
 class ConversationHistory:
     """Manages a conversation history with message storage and retrieval."""
 
@@ -96,15 +140,30 @@ class ConversationHistory:
         if len(self.messages) > self.max_messages:
             self.messages = self.messages[-self.max_messages:]
 
+
+
+
+
+
     def get_context(self) -> list[ConversationMessage]:
         return self.messages.copy()
 
     def clear(self) -> None:
         self.messages.clear()
 
+
+
+
+
+
 class PromptTemplateManager:
     """Manages a collection of prompt templates."""
 
+
+
+
+
+
     def __init__(self) -> None:
         self.templates: dict[str, PromptTemplate] = {}
 
@@ -115,6 +174,11 @@ class PromptTemplateManager:
         template = self.templates[template_name]
         return template.render(**kwargs)
 
+
+
+
+
+
 class ResponsePostProcessor:
     """Manages post-processing hooks for agent responses."""
 
@@ -130,16 +194,26 @@ class ResponsePostProcessor:
             text = hook(text)
         return text
 
+
+
+
+
+
 @dataclass(slots=True)
 class PromptVersion:
     """Versioned prompt for A/B testing."""
     version: str
     content: str
     description: str = ""
+
+
+
+
+
     active: bool = True
     created_at: datetime = field(default_factory=datetime.now)
     metrics: dict[str, float] = field(default_factory=_empty_dict_str_any)
-    
+
     # Old API compatibility fields (initialized in __init__)
     version_id: str = ""
     template_id: str = ""
@@ -166,11 +240,26 @@ class PromptVersion:
         self.created_at = datetime.now()
         self.metrics = {}
         self.version_id = self.version
+
+
+
+
+
         self.template_id = template_id or ""
         self.variant = variant or ""
         self.prompt_text = self.content
         self.weight = weight
 
+
+
+
+
+
+
+
+
+
+
 class BatchRequest:
     """Request in a batch processing queue."""
 
@@ -178,6 +267,11 @@ class BatchRequest:
         self,
         file_path: Path | None = None,
         prompt: str | None = None,
+
+
+
+
+
         priority: FilePriority = FilePriority.NORMAL,
         callback: Callable[[str], None] | None = None,
         max_size: int | None = None
@@ -201,6 +295,16 @@ class BatchRequest:
     def execute(self, processor: Callable[[list[Any]], list[Any]]) -> list[Any]:
         return processor(self.items)
 
+
+
+
+
+
+
+
+
+
+
 @dataclass(slots=True)
 class BatchResult:
     """Result of a batch processing request."""
@@ -210,7 +314,17 @@ class BatchResult:
     error: str = ""
     processing_time: float = 0.0
 
+
+
+
+
+
 @dataclass(slots=True)
+
+
+
+
+
 class MultimodalInput:
     """Multimodal input for agents."""
     input_type: InputType
@@ -218,6 +332,16 @@ class MultimodalInput:
     mime_type: str = ""
     metadata: dict[str, Any] = field(default_factory=_empty_dict_str_any)
 
+
+
+
+
+
+
+
+
+
+
 @dataclass(slots=True)
 class ContextWindow:
     """Manages token-based context window."""
@@ -229,6 +353,11 @@ class ContextWindow:
     def used_tokens(self) -> int:
         return sum(self.token_counts)
 
+
+
+
+
+
     @property
     def available_tokens(self) -> int:
         return max(0, self.max_tokens - self.used_tokens)
@@ -244,6 +373,11 @@ class ContextWindow:
         self.messages.clear()
         self.token_counts.clear()
 
+
+
+
+
+
 @dataclass(slots=True)
 class MultimodalBuilder:
     """Builds multimodal input sets."""
@@ -253,14 +387,19 @@ class MultimodalBuilder:
         self.inputs.append(MultimodalInput(content=content, input_type=input_type))
 
     def add_text(self, content: str) -> None:
-        self.inputs.append(MultimodalInput(content=content, input_type=InputType.TEXT))     
+        self.inputs.append(MultimodalInput(content=content, input_type=InputType.TEXT))
 
     def add_image(self, content: str) -> None:
-        self.inputs.append(MultimodalInput(content=content, input_type=InputType.IMAGE))    
+        self.inputs.append(MultimodalInput(content=content, input_type=InputType.IMAGE))
 
     def build(self) -> list[MultimodalInput]:
         return self.inputs
 
+
+
+
+
+
 @dataclass(slots=True)
 class CachedResult:
     """A cached agent result."""
@@ -271,6 +410,11 @@ class CachedResult:
     timestamp: float = field(default_factory=time.time)
     ttl_seconds: int = 3600
 
+
+
+
+
+
 @dataclass(slots=True)
 class TelemetrySpan:
     """A telemetry span for tracing."""
@@ -283,6 +427,11 @@ class TelemetrySpan:
     attributes: dict[str, Any] = field(default_factory=_empty_dict_str_any)
     events: list[dict[str, Any]] = field(default_factory=_empty_list_dict_str_any)
 
+
+
+
+
+
 class SpanContext:
     """Context for a telemetry span."""
 
@@ -292,7 +441,7 @@ class SpanContext:
     def set_attribute(self, key: str, value: Any) -> None:
         self._span.attributes[key] = value
 
-    def add_event(self, name: str, attributes: dict[str, Any] | None = None) -> None:    
+    def add_event(self, name: str, attributes: dict[str, Any] | None = None) -> None:
         self._span.events.append({
             "name": name,
             "timestamp": time.time(),
