diff --git a/src/infrastructure/backend/llm_backends/VllmBackend.py b/src/infrastructure/backend/llm_backends/VllmBackend.py
index aeecfe6..3cd0878 100644
--- a/src/infrastructure/backend/llm_backends/VllmBackend.py
+++ b/src/infrastructure/backend/llm_backends/VllmBackend.py
@@ -25,6 +25,16 @@ from .LLMBackend import LLMBackend
 
 __version__ = VERSION
 
+
+
+
+
+
+
+
+
+
+
 class VllmBackend(LLMBackend):
     """vLLM (OpenAI-compatible) LLM Backend."""
 
@@ -43,9 +53,9 @@ class VllmBackend(LLMBackend):
                 {"role": "user", "content": prompt}
             ]
         }
-        
+
         timeout_s = kwargs.get("timeout_s", 60)
-        
+
         try:
             response = self.session.post(url, headers={"Content-Type": "application/json"}, json=payload, timeout=timeout_s)
             response.raise_for_status()
@@ -58,4 +68,4 @@ class VllmBackend(LLMBackend):
             logging.debug(f"vLLM call failed: {e}")
             self._update_status("vllm", False)
             self._record("vllm", model, prompt, f"ERROR: {str(e)}", system_prompt=system_prompt)
-            return ""
\ No newline at end of file
+            return ""
