diff --git a/src/infrastructure/orchestration/RLSelector.py b/src/infrastructure/orchestration/RLSelector.py
index 53261e8..f25ecde 100644
--- a/src/infrastructure/orchestration/RLSelector.py
+++ b/src/infrastructure/orchestration/RLSelector.py
@@ -26,13 +26,26 @@ from __future__ import annotations
 from src.core.base.version import VERSION
 import logging
 import random
-from typing import Dict, List, Any
+from typing import Any
+from src.observability.StructuredLogger import StructuredLogger
 
 __version__ = VERSION
 
+logger = StructuredLogger(__name__)
+
+
+
+
+
+
+
+
+
+
+
 class RLSelector:
     """Uses Bayesian Thompson Sampling to optimize tool selection under uncertainty."""
-    
+
     def __init__(self) -> None:
         # Bayesian parameters for Beta distribution: alpha (successes), beta (failures)
         # schema: {tool_name: {"alpha": float, "beta": float, "total_calls": int}}
@@ -43,14 +56,14 @@ class RLSelector:
         if tool_name not in self.tool_stats:
             # Prior: Beta(1, 1) is a flat uniform prior
             self.tool_stats[tool_name] = {"alpha": 1.0, "beta": 1.0, "total_calls": 0}
-            
+
         stats = self.tool_stats[tool_name]
         stats["total_calls"] += 1
         if success:
             stats["alpha"] += 1.0
         else:
             stats["beta"] += 1.0
-            
+
         weight = stats["alpha"] / (stats["alpha"] + stats["beta"])
         logging.info(f"RL-SELECTOR: Updated Bayesian posterior for {tool_name} (Expected Success: {weight:.2f})")
 
@@ -61,40 +74,49 @@ class RLSelector:
         """
         if not candidate_tools:
             raise ValueError("No candidate tools provided.")
-            
+
         best_tool = candidate_tools[0]
         max_sample = -1.0
-        
+
         for tool in candidate_tools:
             if tool not in self.tool_stats:
                 self.tool_stats[tool] = {"alpha": 1.0, "beta": 1.0, "total_calls": 0}
-            
+
             stats = self.tool_stats[tool]
             # Thompson Sampling: Sample from Beta(alpha, beta)
             sample = random.betavariate(stats["alpha"], stats["beta"])
-            
+
             if sample > max_sample:
                 max_sample = sample
                 best_tool = tool
-                
+
         logging.info(f"RL-SELECTOR: Thompson Sampling selected '{best_tool}' (Sample value: {max_sample:.2f})")
         return best_tool
 
     def get_policy_summary(self) -> str:
         """Returns a summary of the current selection policy."""
         summary = ["## Tool Selection Policy (Bayesian Thompson Sampling)"]
+
+
+
+
+
         for tool, stats in self.tool_stats.items():
             expected = stats["alpha"] / (stats["alpha"] + stats["beta"])
             summary.append(f"- **{tool}**: Expected Success Rate {expected*100:.1f}% ({stats['total_calls']} calls)")
         return "\n".join(summary) if len(summary) > 1 else "No policy data yet."
 
+
+
+
+
+
 if __name__ == "__main__":
     # Internal test for Bayesian Thompson Sampling
-    logging.basicConfig(level=logging.INFO)
     rl = RLSelector()
     rl.update_stats("tool_a", True)
     rl.update_stats("tool_a", True)
     rl.update_stats("tool_a", False)
     rl.update_stats("tool_b", False)
-    print(rl.get_policy_summary())
-    print(f"Sampled winner: {rl.select_best_tool(['tool_a', 'tool_b'])}")
\ No newline at end of file
+    logger.info(rl.get_policy_summary())
+    logger.info(f"Sampled winner: {rl.select_best_tool(['tool_a', 'tool_b'])}")
