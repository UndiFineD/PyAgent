# Artificial General Intelligence (AGI)

## What is AGI?
Artificial General Intelligence (AGI), sometimes called "Strong AI," refers to a theoretical type of AI that possesses the ability to understand, learn, and apply knowledge across a wide variety of tasks, much like a human being.

Unlike **Narrow AI** (ANI), which is designed for specific tasks (like playing Chess or recognizing faces), an AGI could theoretically:
*   Reason about uncertain situations.
*   Plan for the future.
*   Learn new skills without forgetting old ones (Catastrophic Forgetting).
*   Communicate in natural language with full nuance.
*   Demonstrate creativity and emotional intelligence.

## Definitions and Benchmarks
There is no single agreed-upon definition, but several tests have been proposed:
*   **The Turing Test**: Can a machine fool a human into thinking it is human? (Largely considered obsolete/passed).
*   **The Coffee Test (Wozniak)**: Can a robot enter an average American home, figure out how to make coffee (find the kitchen, find the machine, find the beans/water/mug), and serve it?
*   **The Employment Test**: Can an AI economically replace a median human worker at most cognitive tasks?

## Approaches to AGI
1.  **Symbolic AI**: Trying to encode all human knowledge and logic rules (failed in the 80s).
2.  **Connectionism (Deep Learning)**: Scaling up neural networks (LLMs) in the hope that general intelligence "emerges" from scale.
3.  **Hybrid Models (Neuro-Symbolic)**: Combining neural networks for perception with symbolic logic for reasoning.
4.  **Whole Brain Emulation**: Scanning and simulating a biological brain neuron-by-neuron.

## Timelines
Predictions for when AGI will be achieved vary wildly:
*   **Optimists (e.g., Sam Altman, Ray Kurzweil)**: By 2029-2032.
*   **Skeptics (e.g., Yann LeCun)**: Decades away, requiring new architectures beyond LLMs.

## Risks (The Alignment Problem)
If an AGI is created, how do we ensure its goals align with human values?
*   **Instrumental Convergence**: An AI might destroy humanity not out of malice, but because we are in the way of its goal (e.g., "Make as many paperclips as possible").
