# Time-to-Think (Inference-Time Compute)

"Time-to-Think" refers to the paradigm shift where we allow models to spend more computation *during inference* to achieve better results, rather than just scaling up training. This is often framed as **System 2** thinking (slow, deliberative) vs. **System 1** (fast, intuitive).

## 1. Chain of Thought (CoT)

The most basic form of time-to-think. Instead of outputting the answer immediately, the model outputs a series of reasoning steps.
*   **Mechanism**: The intermediate tokens generated by the model serve as a "scratchpad." They attend to previous reasoning steps, allowing the model to break down complex problems.
*   **Scaling Law**: Performance improves with the length of the chain of thought (up to a point).

## 2. Tree of Thoughts (ToT) & Search

Instead of a single linear chain, the model explores a tree of possibilities.
*   **Generation**: The model generates multiple candidate "next steps."
*   **Evaluation**: The model (or a separate verifier) scores each candidate.
*   **Search**: Algorithms like BFS (Breadth-First Search) or DFS (Depth-First Search) are used to navigate the tree.
*   This allows the model to backtrack if it realizes it made a mistake, something standard autoregressive generation cannot do.

## 3. Self-Consistency

Sampling multiple reasoning paths (e.g., 10 different CoT generations) and taking the majority vote for the final answer.
*   **Intuition**: If multiple different reasoning paths lead to the same answer, that answer is likely correct. If they diverge, the model is hallucinating.

## 4. Process Reward Models (PRMs)

In standard RLHF, we reward the *final answer* (Outcome Reward Model).
In **Process Supervision**, we reward *each step* of the reasoning.
*   A "Verifier" model checks every step of the CoT.
*   This guides the search process much more effectively, preventing the model from going down a wrong path early on.
*   Famous example: OpenAI's "Let's Verify Step by Step" paper.

## 5. The "O1" Paradigm

Recent models (like OpenAI o1) are trained specifically to utilize inference-time compute.
*   They generate hidden "thought tokens" that the user doesn't see.
*   They use Reinforcement Learning to learn *how* to think effectively (e.g., when to backtrack, when to verify).
*   This effectively trades **Latency** for **Intelligence**.
