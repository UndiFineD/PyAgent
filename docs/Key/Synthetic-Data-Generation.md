# Synthetic Data Generation

We are running out of high-quality human text on the internet. To continue scaling, AI models must learn from data generated by other AI models. **Synthetic Data** is now a critical component of the training pipeline for state-of-the-art models.

## 1. Why Synthetic Data?

*   **Scarcity**: High-quality code, math proofs, and reasoning chains are rare on the web.
*   **Privacy**: Medical and financial data cannot be used for training due to GDPR/HIPAA. Synthetic data can mimic the statistical properties without exposing real individuals.
*   **Control**: You can force the model to practice specific skills (e.g., "Write a Python function that uses a recursive algorithm") by generating targeted datasets.

## 2. Generation Techniques

### A. Self-Instruct
The model is given a few human-written examples of tasks and asked to generate more unique tasks and solutions.
*   *Prompt*: "Here are 3 examples of coding challenges. Generate 100 new, diverse coding challenges."

### B. Evol-Instruct (WizardLM)
Iteratively making simple instructions more complex.
1.  Start: "Write a calculator."
2.  Evolve (Add Constraints): "Write a calculator that only uses bitwise operations."
3.  Evolve (Deepen): "Write a calculator using bitwise operations and handle floating point errors."
This creates a dataset of highly complex problems that don't exist in the wild.

### C. Back-Translation
Used in translation. Train a model to translate English -> French. Then take millions of French sentences, translate them back to English, and use the (Synthetic English, Real French) pairs to improve the model.

## 3. Quality Filtering

The danger of synthetic data is "Garbage In, Garbage Out."
*   **LLM-as-a-Judge**: Use a strong model (GPT-4) to score the synthetic samples. Discard the low-quality ones.
*   **Execution Feedback**: For code generation, run the code. If it crashes or fails unit tests, discard it. This guarantees 100% correctness for the training data.

## 4. Textbooks Are All You Need

Microsoft's **Phi** models proved that a tiny model (1.3B) trained on "textbook quality" synthetic data can outperform massive models trained on raw web scrapings. Data quality matters more than quantity.
